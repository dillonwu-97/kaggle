{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset  \n",
    "The goal of this notebook is to learn pytorch  \n",
    "Link: https://www.kaggle.com/c/digit-recognizer/data  \n",
    "Useful links used:  \n",
    "1) https://www.kaggle.com/abhinand05/mnist-introduction-to-computervision-with-pytorch  \n",
    "2) https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers  \n",
    "Guiding questions for this:\n",
    "1) What is the format of input data?\n",
    "2) How is the data processed and what aspects of a neural network should be tweaked?\n",
    "3) Which aspects are the most important to tweak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original copies\n",
    "train_orig = pd.read_csv(\"./train.csv\",dtype = np.float32) # Need to do this otherwise there will be a double/float conversion error \n",
    "test_orig = pd.read_csv(\"./test.csv\", dtype = np.float32)\n",
    "sample_sub = pd.read_csv(\"./sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dataset\n",
    "def make():\n",
    "    x_train = train_orig.copy()\n",
    "    y_train = x_train.label\n",
    "    x_train.drop(columns=['label'], inplace=True, axis=1)\n",
    "    test = test_orig.copy()\n",
    "    return x_train, y_train, test\n",
    "\n",
    "# Visualizing a sample in the dataset\n",
    "def visualize (data, index, shape=28):\n",
    "    plt.imshow(data[index].reshape(shape,shape))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Sample image of index \" + str(index))\n",
    "    plt.show()\n",
    "\n",
    "# Normalization of dataset\n",
    "def clean_data(data):\n",
    "    data /= 255\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning About the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (42000,), (28000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes of dataset\n",
    "train_x, train_y, test = make()\n",
    "train_x.shape, train_y.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max value of x_train and test is 255\n",
    "train_x.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide x_train by 255 (the max value)\n",
    "train_x = clean_data(train_x)\n",
    "test = clean_data(test)\n",
    "train_x.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert pandas to np arrays\n",
    "train_x = train_x.values\n",
    "train_y = train_y.values\n",
    "test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert np array to tensors\n",
    "train_x = torch.from_numpy(train_x) # need to do this otherwise there will be a type error later one\n",
    "train_y = torch.from_numpy(train_y).type(torch.LongTensor) \n",
    "test = torch.from_numpy(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size = .2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to tensor dataset and using dataloader\n",
    "batch_size = 256\n",
    "trainSet = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "testSet = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainSet, batch_size = batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testSet, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter is:  132\n"
     ]
    }
   ],
   "source": [
    "# Understanding what is in train loader\n",
    "counter = 0\n",
    "for images, labels in train_loader:\n",
    "    counter +=1\n",
    "#     print('images: ', images)\n",
    "#     print('labels: ', labels)\n",
    "print('counter is: ', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(train_x, 100,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is:  tensor([ 4., 16.], grad_fn=<PowBackward0>)\n",
      "o is:  tensor(10., grad_fn=<MulBackward0>)\n",
      "gradient is:  tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Testing out a simple forward / backward prop tensor\n",
    "a = [2,4]\n",
    "t = torch.Tensor(a)\n",
    "x = Variable(t, requires_grad=True)\n",
    "y = x**2\n",
    "print('y is: ', y)\n",
    "o = (1/2)*sum(y)\n",
    "print('o is: ', o)\n",
    "o.backward()\n",
    "print('gradient is: ', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters are:  <generator object Module.parameters at 0x7fbeb90a0dd0>\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Linear Regression Model first\n",
    "# 1. Creating the class object / model\n",
    "class linear_model(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        # super(linear_model, self).__init__() is the same as above\n",
    "        self.lin = nn.Linear(in_size, out_size)\n",
    "    def forward(self, x):\n",
    "        out = self.lin(x)\n",
    "        return out\n",
    "\n",
    "# 2. Instantiating model class with input and output dimensions \n",
    "linm = linear_model(28*28, 10)\n",
    "\n",
    "# 3. Calculate error using cross-entropy loss\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. SGD Optimizer\n",
    "lr = .001\n",
    "print('model parameters are: ', linm.parameters())\n",
    "optimizer = torch.optim.SGD(linm.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "# Learning more about linm:\n",
    "# Observations: linm.parameters() has a size of 2. linm.parameters[0] consists of initialized weights of dim 10 * 784\n",
    "# linm.parameters[1] consists of initialized weights of size 1 * 10\n",
    "temp = []\n",
    "for i in linm.parameters():\n",
    "    temp.append(i)\n",
    "print(len(temp))\n",
    "print(temp[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256, 784])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([9, 9, 9, 0, 9, 3, 8, 8, 6, 8, 9, 3, 0, 5, 2, 3, 1, 4, 3, 6, 4, 6, 5, 5,\n",
      "        6, 9, 3, 2, 4, 8, 2, 7, 1, 9, 4, 0, 3, 6, 1, 4, 5, 8, 8, 2, 0, 9, 1, 2,\n",
      "        2, 2, 9, 2, 3, 4, 1, 7, 4, 3, 2, 5, 6, 2, 0, 9, 8, 0, 4, 7, 7, 1, 0, 5,\n",
      "        9, 6, 1, 9, 8, 9, 1, 8, 1, 0, 2, 8, 5, 0, 8, 2, 5, 7, 8, 8, 1, 7, 4, 2,\n",
      "        4, 9, 6, 1, 7, 7, 9, 0, 6, 7, 9, 6, 6, 4, 8, 3, 9, 0, 4, 4, 1, 1, 9, 6,\n",
      "        8, 3, 3, 9, 9, 1, 7, 6, 4, 0, 9, 5, 3, 8, 7, 0, 8, 4, 7, 6, 0, 7, 3, 7,\n",
      "        4, 5, 8, 8, 0, 1, 0, 1, 3, 3, 2, 2, 1, 2, 1, 7, 3, 4, 4, 2, 5, 7, 6, 4,\n",
      "        8, 3, 0, 7, 7, 7, 7, 1, 1, 6, 0, 5, 4, 4, 1, 9, 9, 5, 8, 7, 6, 3, 8, 7,\n",
      "        1, 7, 9, 5, 2, 7, 0, 0, 0, 5, 4, 4, 5, 3, 9, 9, 2, 5, 0, 9, 8, 0, 7, 4,\n",
      "        0, 5, 6, 5, 7, 7, 5, 0, 1, 2, 6, 7, 0, 1, 2, 7, 7, 8, 1, 1, 4, 6, 3, 9,\n",
      "        6, 0, 9, 6, 0, 6, 8, 5, 1, 2, 7, 2, 6, 5, 6, 0])\n"
     ]
    }
   ],
   "source": [
    "# Look at what view function does\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(images.shape)\n",
    "    print(images.view(-1,28*28).shape)\n",
    "    print(Variable(images.view(-1,28*28)))\n",
    "    print(Variable(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50  Loss: 2.287703514099121  Accuracy: 11%\n",
      "Iteration: 100  Loss: 2.2112255096435547  Accuracy: 19%\n",
      "Iteration: 150  Loss: 2.1753032207489014  Accuracy: 31%\n",
      "Iteration: 200  Loss: 2.1263885498046875  Accuracy: 43%\n",
      "Iteration: 250  Loss: 2.065826892852783  Accuracy: 51%\n",
      "Iteration: 300  Loss: 2.021009683609009  Accuracy: 57%\n",
      "Iteration: 350  Loss: 1.98491632938385  Accuracy: 61%\n",
      "Iteration: 400  Loss: 1.953876256942749  Accuracy: 64%\n",
      "Iteration: 450  Loss: 1.9139834642410278  Accuracy: 66%\n",
      "Iteration: 500  Loss: 1.8435100317001343  Accuracy: 68%\n",
      "Iteration: 550  Loss: 1.8211106061935425  Accuracy: 70%\n",
      "Iteration: 600  Loss: 1.7958122491836548  Accuracy: 71%\n",
      "Iteration: 650  Loss: 1.7837857007980347  Accuracy: 72%\n",
      "Iteration: 700  Loss: 1.742455244064331  Accuracy: 73%\n",
      "Iteration: 750  Loss: 1.700171709060669  Accuracy: 73%\n",
      "Iteration: 800  Loss: 1.6729165315628052  Accuracy: 74%\n",
      "Iteration: 850  Loss: 1.63593590259552  Accuracy: 75%\n",
      "Iteration: 900  Loss: 1.6111561059951782  Accuracy: 75%\n",
      "Iteration: 950  Loss: 1.5752394199371338  Accuracy: 75%\n",
      "Iteration: 1000  Loss: 1.5683152675628662  Accuracy: 76%\n",
      "Iteration: 1050  Loss: 1.5460807085037231  Accuracy: 76%\n",
      "Iteration: 1100  Loss: 1.5393779277801514  Accuracy: 76%\n",
      "Iteration: 1150  Loss: 1.4625651836395264  Accuracy: 77%\n",
      "Iteration: 1200  Loss: 1.4674111604690552  Accuracy: 77%\n",
      "Iteration: 1250  Loss: 1.4400010108947754  Accuracy: 77%\n",
      "Iteration: 1300  Loss: 1.422195315361023  Accuracy: 78%\n",
      "Iteration: 1350  Loss: 1.4093718528747559  Accuracy: 78%\n",
      "Iteration: 1400  Loss: 1.3752775192260742  Accuracy: 78%\n",
      "Iteration: 1450  Loss: 1.3353967666625977  Accuracy: 78%\n",
      "Iteration: 1500  Loss: 1.3567928075790405  Accuracy: 78%\n",
      "Iteration: 1550  Loss: 1.3650058507919312  Accuracy: 79%\n",
      "Iteration: 1600  Loss: 1.3157886266708374  Accuracy: 79%\n",
      "Iteration: 1650  Loss: 1.2751309871673584  Accuracy: 79%\n",
      "Iteration: 1700  Loss: 1.2905923128128052  Accuracy: 79%\n",
      "Iteration: 1750  Loss: 1.2891508340835571  Accuracy: 79%\n",
      "Iteration: 1800  Loss: 1.2528889179229736  Accuracy: 79%\n",
      "Iteration: 1850  Loss: 1.2376713752746582  Accuracy: 79%\n",
      "Iteration: 1900  Loss: 1.1822642087936401  Accuracy: 80%\n",
      "Iteration: 1950  Loss: 1.2350831031799316  Accuracy: 80%\n",
      "Iteration: 2000  Loss: 1.1437427997589111  Accuracy: 80%\n",
      "Iteration: 2050  Loss: 1.1630792617797852  Accuracy: 80%\n",
      "Iteration: 2100  Loss: 1.1318038702011108  Accuracy: 80%\n",
      "Iteration: 2150  Loss: 1.1314963102340698  Accuracy: 80%\n",
      "Iteration: 2200  Loss: 1.1549162864685059  Accuracy: 80%\n",
      "Iteration: 2250  Loss: 1.0864765644073486  Accuracy: 81%\n",
      "Iteration: 2300  Loss: 1.1010338068008423  Accuracy: 81%\n",
      "Iteration: 2350  Loss: 1.1364109516143799  Accuracy: 81%\n",
      "Iteration: 2400  Loss: 1.1176146268844604  Accuracy: 81%\n",
      "Iteration: 2450  Loss: 1.0481889247894287  Accuracy: 81%\n",
      "Iteration: 2500  Loss: 1.072802186012268  Accuracy: 81%\n",
      "Iteration: 2550  Loss: 1.077156901359558  Accuracy: 81%\n",
      "Iteration: 2600  Loss: 1.0762995481491089  Accuracy: 81%\n",
      "Iteration: 2650  Loss: 1.030784010887146  Accuracy: 81%\n",
      "Iteration: 2700  Loss: 1.0222444534301758  Accuracy: 82%\n",
      "Iteration: 2750  Loss: 1.0180784463882446  Accuracy: 82%\n",
      "Iteration: 2800  Loss: 1.0660068988800049  Accuracy: 82%\n",
      "Iteration: 2850  Loss: 1.056795358657837  Accuracy: 82%\n",
      "Iteration: 2900  Loss: 0.9952123761177063  Accuracy: 82%\n",
      "Iteration: 2950  Loss: 0.939376711845398  Accuracy: 82%\n",
      "Iteration: 3000  Loss: 1.0177432298660278  Accuracy: 82%\n",
      "Iteration: 3050  Loss: 1.0346463918685913  Accuracy: 82%\n",
      "Iteration: 3100  Loss: 0.9084565043449402  Accuracy: 82%\n",
      "Iteration: 3150  Loss: 1.0153788328170776  Accuracy: 82%\n",
      "Iteration: 3200  Loss: 0.9762699604034424  Accuracy: 82%\n",
      "Iteration: 3250  Loss: 0.9867101311683655  Accuracy: 82%\n",
      "Iteration: 3300  Loss: 0.9823752641677856  Accuracy: 82%\n",
      "Iteration: 3350  Loss: 0.9318216443061829  Accuracy: 82%\n",
      "Iteration: 3400  Loss: 0.9467188119888306  Accuracy: 83%\n",
      "Iteration: 3450  Loss: 0.9134669899940491  Accuracy: 83%\n",
      "Iteration: 3500  Loss: 1.0435729026794434  Accuracy: 83%\n",
      "Iteration: 3550  Loss: 0.9402878284454346  Accuracy: 83%\n",
      "Iteration: 3600  Loss: 0.9389670491218567  Accuracy: 83%\n",
      "Iteration: 3650  Loss: 0.9603402018547058  Accuracy: 83%\n",
      "Iteration: 3700  Loss: 0.8866531848907471  Accuracy: 83%\n",
      "Iteration: 3750  Loss: 0.9328557252883911  Accuracy: 83%\n",
      "Iteration: 3800  Loss: 0.8682962656021118  Accuracy: 83%\n",
      "Iteration: 3850  Loss: 0.8995182514190674  Accuracy: 83%\n",
      "Iteration: 3900  Loss: 0.8767919540405273  Accuracy: 83%\n",
      "Iteration: 3950  Loss: 0.8862071633338928  Accuracy: 83%\n",
      "Iteration: 4000  Loss: 0.8205165863037109  Accuracy: 83%\n",
      "Iteration: 4050  Loss: 0.9286642670631409  Accuracy: 83%\n",
      "Iteration: 4100  Loss: 0.8658754229545593  Accuracy: 83%\n",
      "Iteration: 4150  Loss: 0.8582690358161926  Accuracy: 83%\n",
      "Iteration: 4200  Loss: 0.869432806968689  Accuracy: 83%\n",
      "Iteration: 4250  Loss: 0.9149630069732666  Accuracy: 83%\n",
      "Iteration: 4300  Loss: 0.8041980862617493  Accuracy: 83%\n",
      "Iteration: 4350  Loss: 0.8205381035804749  Accuracy: 83%\n",
      "Iteration: 4400  Loss: 0.8188319206237793  Accuracy: 83%\n",
      "Iteration: 4450  Loss: 0.8499544262886047  Accuracy: 84%\n",
      "Iteration: 4500  Loss: 0.9006280899047852  Accuracy: 84%\n",
      "Iteration: 4550  Loss: 0.7532298564910889  Accuracy: 84%\n",
      "Iteration: 4600  Loss: 0.7974529266357422  Accuracy: 84%\n",
      "Iteration: 4650  Loss: 0.892857551574707  Accuracy: 84%\n",
      "Iteration: 4700  Loss: 0.8525766730308533  Accuracy: 84%\n",
      "Iteration: 4750  Loss: 0.7834737300872803  Accuracy: 84%\n",
      "Iteration: 4800  Loss: 0.7530572414398193  Accuracy: 84%\n",
      "Iteration: 4850  Loss: 0.8337554335594177  Accuracy: 84%\n",
      "Iteration: 4900  Loss: 0.842247486114502  Accuracy: 84%\n",
      "Iteration: 4950  Loss: 0.7471349239349365  Accuracy: 84%\n",
      "Iteration: 5000  Loss: 0.8482731580734253  Accuracy: 84%\n",
      "Iteration: 5050  Loss: 0.8032116889953613  Accuracy: 84%\n",
      "Iteration: 5100  Loss: 0.8423295021057129  Accuracy: 84%\n",
      "Iteration: 5150  Loss: 0.8015410900115967  Accuracy: 84%\n",
      "Iteration: 5200  Loss: 0.6752680540084839  Accuracy: 84%\n",
      "Iteration: 5250  Loss: 0.8126767873764038  Accuracy: 84%\n",
      "Iteration: 5300  Loss: 0.7079960107803345  Accuracy: 84%\n",
      "Iteration: 5350  Loss: 0.7942537069320679  Accuracy: 84%\n",
      "Iteration: 5400  Loss: 0.8127629160881042  Accuracy: 84%\n",
      "Iteration: 5450  Loss: 0.8349288702011108  Accuracy: 84%\n",
      "Iteration: 5500  Loss: 0.7980082631111145  Accuracy: 84%\n",
      "Iteration: 5550  Loss: 0.735439121723175  Accuracy: 84%\n",
      "Iteration: 5600  Loss: 0.6906786561012268  Accuracy: 84%\n",
      "Iteration: 5650  Loss: 0.7655012011528015  Accuracy: 84%\n",
      "Iteration: 5700  Loss: 0.7892466187477112  Accuracy: 84%\n",
      "Iteration: 5750  Loss: 0.6786832213401794  Accuracy: 84%\n",
      "Iteration: 5800  Loss: 0.7716920375823975  Accuracy: 84%\n",
      "Iteration: 5850  Loss: 0.7382772564888  Accuracy: 84%\n",
      "Iteration: 5900  Loss: 0.7685301303863525  Accuracy: 85%\n",
      "Iteration: 5950  Loss: 0.7781543135643005  Accuracy: 85%\n",
      "Iteration: 6000  Loss: 0.7165539264678955  Accuracy: 85%\n",
      "Iteration: 6050  Loss: 0.7218313217163086  Accuracy: 85%\n",
      "Iteration: 6100  Loss: 0.7700344324111938  Accuracy: 85%\n",
      "Iteration: 6150  Loss: 0.7642284631729126  Accuracy: 85%\n",
      "Iteration: 6200  Loss: 0.7724108695983887  Accuracy: 85%\n",
      "Iteration: 6250  Loss: 0.6802130341529846  Accuracy: 85%\n",
      "Iteration: 6300  Loss: 0.7394654154777527  Accuracy: 85%\n",
      "Iteration: 6350  Loss: 0.6820597648620605  Accuracy: 85%\n",
      "Iteration: 6400  Loss: 0.6936708092689514  Accuracy: 85%\n",
      "Iteration: 6450  Loss: 0.7374817728996277  Accuracy: 85%\n",
      "Iteration: 6500  Loss: 0.709796130657196  Accuracy: 85%\n",
      "Iteration: 6550  Loss: 0.7298592329025269  Accuracy: 85%\n",
      "Iteration: 6600  Loss: 0.6821545362472534  Accuracy: 85%\n",
      "Iteration: 6650  Loss: 0.7404723167419434  Accuracy: 85%\n",
      "Iteration: 6700  Loss: 0.7899796962738037  Accuracy: 85%\n",
      "Iteration: 6750  Loss: 0.7424591779708862  Accuracy: 85%\n",
      "Iteration: 6800  Loss: 0.7549347281455994  Accuracy: 85%\n",
      "Iteration: 6850  Loss: 0.7118799090385437  Accuracy: 85%\n",
      "Iteration: 6900  Loss: 0.6708870530128479  Accuracy: 85%\n",
      "Iteration: 6950  Loss: 0.6695887446403503  Accuracy: 85%\n",
      "Iteration: 7000  Loss: 0.7489574551582336  Accuracy: 85%\n",
      "Iteration: 7050  Loss: 0.6812425255775452  Accuracy: 85%\n",
      "Iteration: 7100  Loss: 0.7544872760772705  Accuracy: 85%\n",
      "Iteration: 7150  Loss: 0.6276071071624756  Accuracy: 85%\n",
      "Iteration: 7200  Loss: 0.6337634921073914  Accuracy: 85%\n",
      "Iteration: 7250  Loss: 0.685111403465271  Accuracy: 85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7300  Loss: 0.7459213137626648  Accuracy: 85%\n",
      "Iteration: 7350  Loss: 0.6909012794494629  Accuracy: 85%\n",
      "Iteration: 7400  Loss: 0.6874634623527527  Accuracy: 85%\n",
      "Iteration: 7450  Loss: 0.6557784676551819  Accuracy: 85%\n",
      "Iteration: 7500  Loss: 0.6858440041542053  Accuracy: 85%\n",
      "Iteration: 7550  Loss: 0.6180493235588074  Accuracy: 85%\n",
      "Iteration: 7600  Loss: 0.6601381897926331  Accuracy: 85%\n",
      "Iteration: 7650  Loss: 0.7060355544090271  Accuracy: 85%\n",
      "Iteration: 7700  Loss: 0.6622881293296814  Accuracy: 85%\n",
      "Iteration: 7750  Loss: 0.5812384486198425  Accuracy: 85%\n",
      "Iteration: 7800  Loss: 0.6123431921005249  Accuracy: 85%\n",
      "Iteration: 7850  Loss: 0.6348337531089783  Accuracy: 86%\n",
      "Iteration: 7900  Loss: 0.6641796231269836  Accuracy: 86%\n",
      "Iteration: 7950  Loss: 0.6087910532951355  Accuracy: 86%\n",
      "Iteration: 8000  Loss: 0.7023168206214905  Accuracy: 86%\n",
      "Iteration: 8050  Loss: 0.6930726766586304  Accuracy: 86%\n",
      "Iteration: 8100  Loss: 0.7418193221092224  Accuracy: 86%\n",
      "Iteration: 8150  Loss: 0.6537778377532959  Accuracy: 86%\n",
      "Iteration: 8200  Loss: 0.632545530796051  Accuracy: 86%\n",
      "Iteration: 8250  Loss: 0.6334905028343201  Accuracy: 86%\n",
      "Iteration: 8300  Loss: 0.6319584846496582  Accuracy: 86%\n",
      "Iteration: 8350  Loss: 0.6292786598205566  Accuracy: 86%\n",
      "Iteration: 8400  Loss: 0.684441328048706  Accuracy: 86%\n",
      "Iteration: 8450  Loss: 0.6319080591201782  Accuracy: 86%\n",
      "Iteration: 8500  Loss: 0.6869165897369385  Accuracy: 86%\n",
      "Iteration: 8550  Loss: 0.6718657612800598  Accuracy: 86%\n",
      "Iteration: 8600  Loss: 0.5746058821678162  Accuracy: 86%\n",
      "Iteration: 8650  Loss: 0.6682400703430176  Accuracy: 86%\n",
      "Iteration: 8700  Loss: 0.7511322498321533  Accuracy: 86%\n",
      "Iteration: 8750  Loss: 0.6316598653793335  Accuracy: 86%\n",
      "Iteration: 8800  Loss: 0.6131627559661865  Accuracy: 86%\n",
      "Iteration: 8850  Loss: 0.7436468601226807  Accuracy: 86%\n",
      "Iteration: 8900  Loss: 0.6390073299407959  Accuracy: 86%\n",
      "Iteration: 8950  Loss: 0.6324469447135925  Accuracy: 86%\n",
      "Iteration: 9000  Loss: 0.6427596807479858  Accuracy: 86%\n",
      "Iteration: 9050  Loss: 0.6724086403846741  Accuracy: 86%\n",
      "Iteration: 9100  Loss: 0.5810307860374451  Accuracy: 86%\n",
      "Iteration: 9150  Loss: 0.5937828421592712  Accuracy: 86%\n",
      "Iteration: 9200  Loss: 0.6932932138442993  Accuracy: 86%\n",
      "Iteration: 9250  Loss: 0.6641883254051208  Accuracy: 86%\n",
      "Iteration: 9300  Loss: 0.8264064788818359  Accuracy: 86%\n",
      "Iteration: 9350  Loss: 0.6170406937599182  Accuracy: 86%\n",
      "Iteration: 9400  Loss: 0.6489964723587036  Accuracy: 86%\n",
      "Iteration: 9450  Loss: 0.560064971446991  Accuracy: 86%\n",
      "Iteration: 9500  Loss: 0.5812268257141113  Accuracy: 86%\n",
      "Iteration: 9550  Loss: 0.7018435001373291  Accuracy: 86%\n",
      "Iteration: 9600  Loss: 0.5935751795768738  Accuracy: 86%\n",
      "Iteration: 9650  Loss: 0.6151311993598938  Accuracy: 86%\n",
      "Iteration: 9700  Loss: 0.6312613487243652  Accuracy: 86%\n",
      "Iteration: 9750  Loss: 0.5660488605499268  Accuracy: 86%\n",
      "Iteration: 9800  Loss: 0.650716245174408  Accuracy: 86%\n",
      "Iteration: 9850  Loss: 0.6098222136497498  Accuracy: 86%\n",
      "Iteration: 9900  Loss: 0.8378380537033081  Accuracy: 86%\n",
      "Iteration: 9950  Loss: 0.7103015780448914  Accuracy: 86%\n",
      "Iteration: 10000  Loss: 0.545877993106842  Accuracy: 86%\n",
      "Iteration: 10050  Loss: 0.5847964882850647  Accuracy: 86%\n",
      "Iteration: 10100  Loss: 0.5435613989830017  Accuracy: 86%\n",
      "Iteration: 10150  Loss: 0.6000869274139404  Accuracy: 86%\n",
      "Iteration: 10200  Loss: 0.6311039328575134  Accuracy: 86%\n",
      "Iteration: 10250  Loss: 0.6020084619522095  Accuracy: 86%\n",
      "Iteration: 10300  Loss: 0.6410927176475525  Accuracy: 86%\n",
      "Iteration: 10350  Loss: 0.5837367177009583  Accuracy: 86%\n",
      "Iteration: 10400  Loss: 0.6468218564987183  Accuracy: 86%\n",
      "Iteration: 10450  Loss: 0.5898615717887878  Accuracy: 86%\n",
      "Iteration: 10500  Loss: 0.5556274056434631  Accuracy: 86%\n",
      "Iteration: 10550  Loss: 0.6304663419723511  Accuracy: 86%\n",
      "Iteration: 10600  Loss: 0.6761744618415833  Accuracy: 86%\n",
      "Iteration: 10650  Loss: 0.6417352557182312  Accuracy: 86%\n",
      "Iteration: 10700  Loss: 0.6077027916908264  Accuracy: 86%\n",
      "Iteration: 10750  Loss: 0.5703115463256836  Accuracy: 86%\n",
      "Iteration: 10800  Loss: 0.5867544412612915  Accuracy: 86%\n",
      "Iteration: 10850  Loss: 0.6050200462341309  Accuracy: 86%\n",
      "Iteration: 10900  Loss: 0.6261947751045227  Accuracy: 86%\n",
      "Iteration: 10950  Loss: 0.6405633091926575  Accuracy: 86%\n",
      "Iteration: 11000  Loss: 0.5610736608505249  Accuracy: 86%\n",
      "Iteration: 11050  Loss: 0.6078081130981445  Accuracy: 86%\n",
      "Iteration: 11100  Loss: 0.608880341053009  Accuracy: 86%\n",
      "Iteration: 11150  Loss: 0.6010454893112183  Accuracy: 86%\n",
      "Iteration: 11200  Loss: 0.5951472520828247  Accuracy: 86%\n",
      "Iteration: 11250  Loss: 0.5979622006416321  Accuracy: 86%\n",
      "Iteration: 11300  Loss: 0.6351531147956848  Accuracy: 86%\n",
      "Iteration: 11350  Loss: 0.5828542113304138  Accuracy: 86%\n",
      "Iteration: 11400  Loss: 0.5507264137268066  Accuracy: 86%\n",
      "Iteration: 11450  Loss: 0.521876335144043  Accuracy: 86%\n",
      "Iteration: 11500  Loss: 0.5471874475479126  Accuracy: 86%\n",
      "Iteration: 11550  Loss: 0.5679080486297607  Accuracy: 86%\n",
      "Iteration: 11600  Loss: 0.5687814950942993  Accuracy: 86%\n",
      "Iteration: 11650  Loss: 0.577418863773346  Accuracy: 86%\n",
      "Iteration: 11700  Loss: 0.5123019218444824  Accuracy: 87%\n",
      "Iteration: 11750  Loss: 0.5420132279396057  Accuracy: 87%\n",
      "Iteration: 11800  Loss: 0.552819550037384  Accuracy: 87%\n",
      "Iteration: 11850  Loss: 0.4798150658607483  Accuracy: 87%\n",
      "Iteration: 11900  Loss: 0.575765073299408  Accuracy: 87%\n",
      "Iteration: 11950  Loss: 0.5879563689231873  Accuracy: 87%\n",
      "Iteration: 12000  Loss: 0.6679161787033081  Accuracy: 87%\n",
      "Iteration: 12050  Loss: 0.6058636903762817  Accuracy: 87%\n",
      "Iteration: 12100  Loss: 0.5283854007720947  Accuracy: 87%\n",
      "Iteration: 12150  Loss: 0.6416478753089905  Accuracy: 87%\n",
      "Iteration: 12200  Loss: 0.5551590919494629  Accuracy: 87%\n",
      "Iteration: 12250  Loss: 0.6036204099655151  Accuracy: 87%\n",
      "Iteration: 12300  Loss: 0.6189424395561218  Accuracy: 87%\n",
      "Iteration: 12350  Loss: 0.5776360630989075  Accuracy: 87%\n",
      "Iteration: 12400  Loss: 0.5760015249252319  Accuracy: 87%\n",
      "Iteration: 12450  Loss: 0.5437877774238586  Accuracy: 87%\n",
      "Iteration: 12500  Loss: 0.5782442688941956  Accuracy: 87%\n",
      "Iteration: 12550  Loss: 0.5944293141365051  Accuracy: 87%\n",
      "Iteration: 12600  Loss: 0.563410758972168  Accuracy: 87%\n",
      "Iteration: 12650  Loss: 0.5932636857032776  Accuracy: 87%\n",
      "Iteration: 12700  Loss: 0.5687775015830994  Accuracy: 87%\n",
      "Iteration: 12750  Loss: 0.5286586880683899  Accuracy: 87%\n",
      "Iteration: 12800  Loss: 0.5968993902206421  Accuracy: 87%\n",
      "Iteration: 12850  Loss: 0.5838514566421509  Accuracy: 87%\n",
      "Iteration: 12900  Loss: 0.5424129366874695  Accuracy: 87%\n",
      "Iteration: 12950  Loss: 0.5631250143051147  Accuracy: 87%\n",
      "Iteration: 13000  Loss: 0.5288733839988708  Accuracy: 87%\n",
      "Iteration: 13050  Loss: 0.5111900568008423  Accuracy: 87%\n",
      "Iteration: 13100  Loss: 0.5691813230514526  Accuracy: 87%\n",
      "Iteration: 13150  Loss: 0.5648389458656311  Accuracy: 87%\n",
      "Iteration: 13200  Loss: 0.47645843029022217  Accuracy: 87%\n",
      "Iteration: 13250  Loss: 0.5121299624443054  Accuracy: 87%\n",
      "Iteration: 13300  Loss: 0.5804272294044495  Accuracy: 87%\n",
      "Iteration: 13350  Loss: 0.5621417760848999  Accuracy: 87%\n",
      "Iteration: 13400  Loss: 0.5470682382583618  Accuracy: 87%\n",
      "Iteration: 13450  Loss: 0.5801444053649902  Accuracy: 87%\n",
      "Iteration: 13500  Loss: 0.5882361531257629  Accuracy: 87%\n",
      "Iteration: 13550  Loss: 0.6022089719772339  Accuracy: 87%\n",
      "Iteration: 13600  Loss: 0.5073742866516113  Accuracy: 87%\n",
      "Iteration: 13650  Loss: 0.5566268563270569  Accuracy: 87%\n",
      "Iteration: 13700  Loss: 0.5052005052566528  Accuracy: 87%\n",
      "Iteration: 13750  Loss: 0.5120907425880432  Accuracy: 87%\n",
      "Iteration: 13800  Loss: 0.527442991733551  Accuracy: 87%\n",
      "Iteration: 13850  Loss: 0.4963173568248749  Accuracy: 87%\n",
      "Iteration: 13900  Loss: 0.5494356751441956  Accuracy: 87%\n",
      "Iteration: 13950  Loss: 0.5400646924972534  Accuracy: 87%\n",
      "Iteration: 14000  Loss: 0.46545717120170593  Accuracy: 87%\n",
      "Iteration: 14050  Loss: 0.5308312773704529  Accuracy: 87%\n",
      "Iteration: 14100  Loss: 0.5932968854904175  Accuracy: 87%\n",
      "Iteration: 14150  Loss: 0.5150480270385742  Accuracy: 87%\n",
      "Iteration: 14200  Loss: 0.5182167291641235  Accuracy: 87%\n",
      "Iteration: 14250  Loss: 0.5816262364387512  Accuracy: 87%\n",
      "Iteration: 14300  Loss: 0.5827059149742126  Accuracy: 87%\n",
      "Iteration: 14350  Loss: 0.5128584504127502  Accuracy: 87%\n",
      "Iteration: 14400  Loss: 0.5062708258628845  Accuracy: 87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14450  Loss: 0.5395583510398865  Accuracy: 87%\n",
      "Iteration: 14500  Loss: 0.5785431265830994  Accuracy: 87%\n",
      "Iteration: 14550  Loss: 0.49935010075569153  Accuracy: 87%\n",
      "Iteration: 14600  Loss: 0.4860354959964752  Accuracy: 87%\n",
      "Iteration: 14650  Loss: 0.5465217232704163  Accuracy: 87%\n",
      "Iteration: 14700  Loss: 0.5081053376197815  Accuracy: 87%\n",
      "Iteration: 14750  Loss: 0.5760383605957031  Accuracy: 87%\n",
      "Iteration: 14800  Loss: 0.5262783169746399  Accuracy: 87%\n",
      "Iteration: 14850  Loss: 0.5643715858459473  Accuracy: 87%\n",
      "Iteration: 14900  Loss: 0.6012578010559082  Accuracy: 87%\n",
      "Iteration: 14950  Loss: 0.5623216032981873  Accuracy: 87%\n",
      "Iteration: 15000  Loss: 0.6299659609794617  Accuracy: 87%\n",
      "Iteration: 15050  Loss: 0.5588743090629578  Accuracy: 87%\n",
      "Iteration: 15100  Loss: 0.5162403583526611  Accuracy: 87%\n",
      "Iteration: 15150  Loss: 0.6368148922920227  Accuracy: 87%\n",
      "Iteration: 15200  Loss: 0.5449052453041077  Accuracy: 87%\n",
      "Iteration: 15250  Loss: 0.5017607808113098  Accuracy: 87%\n",
      "Iteration: 15300  Loss: 0.5301900506019592  Accuracy: 87%\n",
      "Iteration: 15350  Loss: 0.5171768069267273  Accuracy: 87%\n",
      "Iteration: 15400  Loss: 0.5261029601097107  Accuracy: 87%\n",
      "Iteration: 15450  Loss: 0.5002767443656921  Accuracy: 87%\n",
      "Iteration: 15500  Loss: 0.48250842094421387  Accuracy: 87%\n",
      "Iteration: 15550  Loss: 0.5416068434715271  Accuracy: 87%\n",
      "Iteration: 15600  Loss: 0.48332974314689636  Accuracy: 87%\n",
      "Iteration: 15650  Loss: 0.48657795786857605  Accuracy: 87%\n",
      "Iteration: 15700  Loss: 0.4708308279514313  Accuracy: 87%\n",
      "Iteration: 15750  Loss: 0.5629799365997314  Accuracy: 87%\n",
      "Iteration: 15800  Loss: 0.5248209834098816  Accuracy: 87%\n",
      "Iteration: 15850  Loss: 0.4929976165294647  Accuracy: 87%\n",
      "Iteration: 15900  Loss: 0.5624186396598816  Accuracy: 87%\n",
      "Iteration: 15950  Loss: 0.5210584402084351  Accuracy: 87%\n",
      "Iteration: 16000  Loss: 0.5513298511505127  Accuracy: 87%\n",
      "Iteration: 16050  Loss: 0.5358368158340454  Accuracy: 87%\n",
      "Iteration: 16100  Loss: 0.5271185636520386  Accuracy: 87%\n",
      "Iteration: 16150  Loss: 0.5107455253601074  Accuracy: 87%\n",
      "Iteration: 16200  Loss: 0.4828858971595764  Accuracy: 87%\n",
      "Iteration: 16250  Loss: 0.5808327794075012  Accuracy: 87%\n",
      "Iteration: 16300  Loss: 0.54750657081604  Accuracy: 87%\n",
      "Iteration: 16350  Loss: 0.5222998857498169  Accuracy: 87%\n",
      "Iteration: 16400  Loss: 0.5274044871330261  Accuracy: 87%\n",
      "Iteration: 16450  Loss: 0.5219356417655945  Accuracy: 87%\n",
      "Iteration: 16500  Loss: 0.5665321946144104  Accuracy: 87%\n",
      "Iteration: 16550  Loss: 0.5961005687713623  Accuracy: 87%\n",
      "Iteration: 16600  Loss: 0.5050411820411682  Accuracy: 87%\n",
      "Iteration: 16650  Loss: 0.5194703936576843  Accuracy: 87%\n",
      "Iteration: 16700  Loss: 0.5178110003471375  Accuracy: 87%\n",
      "Iteration: 16750  Loss: 0.4769410490989685  Accuracy: 87%\n",
      "Iteration: 16800  Loss: 0.4798782169818878  Accuracy: 87%\n",
      "Iteration: 16850  Loss: 0.5433427095413208  Accuracy: 87%\n",
      "Iteration: 16900  Loss: 0.5038837790489197  Accuracy: 87%\n",
      "Iteration: 16950  Loss: 0.4982767701148987  Accuracy: 87%\n",
      "Iteration: 17000  Loss: 0.4514864385128021  Accuracy: 87%\n",
      "Iteration: 17050  Loss: 0.5466822385787964  Accuracy: 87%\n",
      "Iteration: 17100  Loss: 0.5506394505500793  Accuracy: 87%\n",
      "Iteration: 17150  Loss: 0.5081313848495483  Accuracy: 87%\n",
      "Iteration: 17200  Loss: 0.4979991912841797  Accuracy: 87%\n",
      "Iteration: 17250  Loss: 0.5448578596115112  Accuracy: 87%\n",
      "Iteration: 17300  Loss: 0.48621445894241333  Accuracy: 87%\n",
      "Iteration: 17350  Loss: 0.5279874801635742  Accuracy: 87%\n",
      "Iteration: 17400  Loss: 0.534674882888794  Accuracy: 87%\n",
      "Iteration: 17450  Loss: 0.5123441219329834  Accuracy: 87%\n",
      "Iteration: 17500  Loss: 0.5403242707252502  Accuracy: 87%\n",
      "Iteration: 17550  Loss: 0.449798047542572  Accuracy: 87%\n",
      "Iteration: 17600  Loss: 0.5167155861854553  Accuracy: 87%\n",
      "Iteration: 17650  Loss: 0.5793293118476868  Accuracy: 87%\n",
      "Iteration: 17700  Loss: 0.5223544836044312  Accuracy: 87%\n",
      "Iteration: 17750  Loss: 0.5138717293739319  Accuracy: 87%\n",
      "Iteration: 17800  Loss: 0.5814071297645569  Accuracy: 87%\n",
      "Iteration: 17850  Loss: 0.47138887643814087  Accuracy: 87%\n",
      "Iteration: 17900  Loss: 0.5646393895149231  Accuracy: 87%\n",
      "Iteration: 17950  Loss: 0.5044376254081726  Accuracy: 87%\n",
      "Iteration: 18000  Loss: 0.48984596133232117  Accuracy: 87%\n",
      "Iteration: 18050  Loss: 0.4473431408405304  Accuracy: 87%\n",
      "Iteration: 18100  Loss: 0.5594292283058167  Accuracy: 87%\n",
      "Iteration: 18150  Loss: 0.5171217322349548  Accuracy: 87%\n",
      "Iteration: 18200  Loss: 0.549144446849823  Accuracy: 87%\n",
      "Iteration: 18250  Loss: 0.4901222288608551  Accuracy: 87%\n",
      "Iteration: 18300  Loss: 0.45723292231559753  Accuracy: 87%\n",
      "Iteration: 18350  Loss: 0.4691140949726105  Accuracy: 87%\n",
      "Iteration: 18400  Loss: 0.4483977258205414  Accuracy: 87%\n",
      "Iteration: 18450  Loss: 0.4840667247772217  Accuracy: 87%\n",
      "Iteration: 18500  Loss: 0.5152640342712402  Accuracy: 87%\n",
      "Iteration: 18550  Loss: 0.5413833260536194  Accuracy: 87%\n",
      "Iteration: 18600  Loss: 0.42557311058044434  Accuracy: 87%\n",
      "Iteration: 18650  Loss: 0.4788745641708374  Accuracy: 87%\n",
      "Iteration: 18700  Loss: 0.5012599229812622  Accuracy: 87%\n",
      "Iteration: 18750  Loss: 0.44828686118125916  Accuracy: 88%\n",
      "Iteration: 18800  Loss: 0.597848117351532  Accuracy: 88%\n",
      "Iteration: 18850  Loss: 0.5176594257354736  Accuracy: 87%\n",
      "Iteration: 18900  Loss: 0.5407224297523499  Accuracy: 88%\n",
      "Iteration: 18950  Loss: 0.4157055616378784  Accuracy: 88%\n",
      "Iteration: 19000  Loss: 0.5105540752410889  Accuracy: 88%\n",
      "Iteration: 19050  Loss: 0.41676434874534607  Accuracy: 88%\n",
      "Iteration: 19100  Loss: 0.5720842480659485  Accuracy: 88%\n",
      "Iteration: 19150  Loss: 0.48363327980041504  Accuracy: 88%\n",
      "Iteration: 19200  Loss: 0.43767789006233215  Accuracy: 88%\n",
      "Iteration: 19250  Loss: 0.44731929898262024  Accuracy: 88%\n",
      "Iteration: 19300  Loss: 0.4881924092769623  Accuracy: 88%\n",
      "Iteration: 19350  Loss: 0.47520095109939575  Accuracy: 88%\n",
      "Iteration: 19400  Loss: 0.4876568019390106  Accuracy: 88%\n",
      "Iteration: 19450  Loss: 0.4664950370788574  Accuracy: 88%\n",
      "Iteration: 19500  Loss: 0.4831624925136566  Accuracy: 88%\n",
      "Iteration: 19550  Loss: 0.5470924973487854  Accuracy: 88%\n",
      "Iteration: 19600  Loss: 0.4371681809425354  Accuracy: 88%\n",
      "Iteration: 19650  Loss: 0.5808102488517761  Accuracy: 88%\n",
      "Iteration: 19700  Loss: 0.404304563999176  Accuracy: 88%\n",
      "Iteration: 19750  Loss: 0.48956385254859924  Accuracy: 88%\n",
      "Iteration: 19800  Loss: 0.5801536440849304  Accuracy: 88%\n",
      "Iteration: 19850  Loss: 0.524451732635498  Accuracy: 88%\n",
      "Iteration: 19900  Loss: 0.48996254801750183  Accuracy: 88%\n",
      "Iteration: 19950  Loss: 0.4813554883003235  Accuracy: 88%\n",
      "Iteration: 20000  Loss: 0.4886981248855591  Accuracy: 88%\n",
      "Iteration: 20050  Loss: 0.4465714693069458  Accuracy: 88%\n",
      "Iteration: 20100  Loss: 0.5351940393447876  Accuracy: 88%\n",
      "Iteration: 20150  Loss: 0.5025292038917542  Accuracy: 88%\n",
      "Iteration: 20200  Loss: 0.49143707752227783  Accuracy: 88%\n",
      "Iteration: 20250  Loss: 0.42587316036224365  Accuracy: 88%\n",
      "Iteration: 20300  Loss: 0.46433332562446594  Accuracy: 88%\n",
      "Iteration: 20350  Loss: 0.5060247182846069  Accuracy: 88%\n",
      "Iteration: 20400  Loss: 0.5252267718315125  Accuracy: 88%\n",
      "Iteration: 20450  Loss: 0.504117488861084  Accuracy: 88%\n",
      "Iteration: 20500  Loss: 0.44238629937171936  Accuracy: 88%\n",
      "Iteration: 20550  Loss: 0.5595979690551758  Accuracy: 88%\n",
      "Iteration: 20600  Loss: 0.4834056794643402  Accuracy: 88%\n",
      "Iteration: 20650  Loss: 0.4521310031414032  Accuracy: 88%\n",
      "Iteration: 20700  Loss: 0.5002139210700989  Accuracy: 88%\n",
      "Iteration: 20750  Loss: 0.39992764592170715  Accuracy: 88%\n",
      "Iteration: 20800  Loss: 0.5124993920326233  Accuracy: 88%\n",
      "Iteration: 20850  Loss: 0.4707294702529907  Accuracy: 88%\n",
      "Iteration: 20900  Loss: 0.49424639344215393  Accuracy: 88%\n",
      "Iteration: 20950  Loss: 0.4383056163787842  Accuracy: 88%\n",
      "Iteration: 21000  Loss: 0.41185876727104187  Accuracy: 88%\n",
      "Iteration: 21050  Loss: 0.40556246042251587  Accuracy: 88%\n",
      "Iteration: 21100  Loss: 0.4712581932544708  Accuracy: 88%\n",
      "Iteration: 21150  Loss: 0.5405918955802917  Accuracy: 88%\n",
      "Iteration: 21200  Loss: 0.46372759342193604  Accuracy: 88%\n",
      "Iteration: 21250  Loss: 0.5815353393554688  Accuracy: 88%\n",
      "Iteration: 21300  Loss: 0.46390846371650696  Accuracy: 88%\n",
      "Iteration: 21350  Loss: 0.474669486284256  Accuracy: 88%\n",
      "Iteration: 21400  Loss: 0.4789177179336548  Accuracy: 88%\n",
      "Iteration: 21450  Loss: 0.4541539251804352  Accuracy: 88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 21500  Loss: 0.5126732587814331  Accuracy: 88%\n",
      "Iteration: 21550  Loss: 0.49736058712005615  Accuracy: 88%\n",
      "Iteration: 21600  Loss: 0.4059634506702423  Accuracy: 88%\n",
      "Iteration: 21650  Loss: 0.46736738085746765  Accuracy: 88%\n",
      "Iteration: 21700  Loss: 0.44864335656166077  Accuracy: 88%\n",
      "Iteration: 21750  Loss: 0.39482104778289795  Accuracy: 88%\n",
      "Iteration: 21800  Loss: 0.4569936692714691  Accuracy: 88%\n",
      "Iteration: 21850  Loss: 0.40810394287109375  Accuracy: 88%\n",
      "Iteration: 21900  Loss: 0.458219438791275  Accuracy: 88%\n",
      "Iteration: 21950  Loss: 0.4768137037754059  Accuracy: 88%\n",
      "Iteration: 22000  Loss: 0.5300041437149048  Accuracy: 88%\n",
      "Iteration: 22050  Loss: 0.4947007894515991  Accuracy: 88%\n",
      "Iteration: 22100  Loss: 0.5221590995788574  Accuracy: 88%\n",
      "Iteration: 22150  Loss: 0.42243093252182007  Accuracy: 88%\n",
      "Iteration: 22200  Loss: 0.42090076208114624  Accuracy: 88%\n",
      "Iteration: 22250  Loss: 0.4870646893978119  Accuracy: 88%\n",
      "Iteration: 22300  Loss: 0.40351998805999756  Accuracy: 88%\n",
      "Iteration: 22350  Loss: 0.4889553189277649  Accuracy: 88%\n",
      "Iteration: 22400  Loss: 0.45752596855163574  Accuracy: 88%\n",
      "Iteration: 22450  Loss: 0.4750085473060608  Accuracy: 88%\n",
      "Iteration: 22500  Loss: 0.48314639925956726  Accuracy: 88%\n",
      "Iteration: 22550  Loss: 0.5015736222267151  Accuracy: 88%\n",
      "Iteration: 22600  Loss: 0.4715867042541504  Accuracy: 88%\n",
      "Iteration: 22650  Loss: 0.4617009460926056  Accuracy: 88%\n",
      "Iteration: 22700  Loss: 0.5284101366996765  Accuracy: 88%\n",
      "Iteration: 22750  Loss: 0.4720839560031891  Accuracy: 88%\n",
      "Iteration: 22800  Loss: 0.42812392115592957  Accuracy: 88%\n",
      "Iteration: 22850  Loss: 0.511011004447937  Accuracy: 88%\n",
      "Iteration: 22900  Loss: 0.4634166359901428  Accuracy: 88%\n",
      "Iteration: 22950  Loss: 0.5652185082435608  Accuracy: 88%\n",
      "Iteration: 23000  Loss: 0.4293476641178131  Accuracy: 88%\n",
      "Iteration: 23050  Loss: 0.4797755479812622  Accuracy: 88%\n",
      "Iteration: 23100  Loss: 0.6100775003433228  Accuracy: 88%\n",
      "Iteration: 23150  Loss: 0.538205623626709  Accuracy: 88%\n",
      "Iteration: 23200  Loss: 0.44078418612480164  Accuracy: 88%\n",
      "Iteration: 23250  Loss: 0.5053042769432068  Accuracy: 88%\n",
      "Iteration: 23300  Loss: 0.45753082633018494  Accuracy: 88%\n",
      "Iteration: 23350  Loss: 0.47508522868156433  Accuracy: 88%\n",
      "Iteration: 23400  Loss: 0.5264546275138855  Accuracy: 88%\n",
      "Iteration: 23450  Loss: 0.5000814199447632  Accuracy: 88%\n",
      "Iteration: 23500  Loss: 0.45716798305511475  Accuracy: 88%\n",
      "Iteration: 23550  Loss: 0.518759548664093  Accuracy: 88%\n",
      "Iteration: 23600  Loss: 0.4512353241443634  Accuracy: 88%\n",
      "Iteration: 23650  Loss: 0.4194052219390869  Accuracy: 88%\n",
      "Iteration: 23700  Loss: 0.476891428232193  Accuracy: 88%\n",
      "Iteration: 23750  Loss: 0.4470556378364563  Accuracy: 88%\n",
      "Iteration: 23800  Loss: 0.538545548915863  Accuracy: 88%\n",
      "Iteration: 23850  Loss: 0.43576502799987793  Accuracy: 88%\n",
      "Iteration: 23900  Loss: 0.44659245014190674  Accuracy: 88%\n",
      "Iteration: 23950  Loss: 0.44094905257225037  Accuracy: 88%\n",
      "Iteration: 24000  Loss: 0.5225500464439392  Accuracy: 88%\n",
      "Iteration: 24050  Loss: 0.42300158739089966  Accuracy: 88%\n",
      "Iteration: 24100  Loss: 0.4395526945590973  Accuracy: 88%\n",
      "Iteration: 24150  Loss: 0.5338382124900818  Accuracy: 88%\n",
      "Iteration: 24200  Loss: 0.5074885487556458  Accuracy: 88%\n",
      "Iteration: 24250  Loss: 0.502100944519043  Accuracy: 88%\n",
      "Iteration: 24300  Loss: 0.38849470019340515  Accuracy: 88%\n",
      "Iteration: 24350  Loss: 0.5769781470298767  Accuracy: 88%\n",
      "Iteration: 24400  Loss: 0.4245075285434723  Accuracy: 88%\n",
      "Iteration: 24450  Loss: 0.44507989287376404  Accuracy: 88%\n",
      "Iteration: 24500  Loss: 0.3899043798446655  Accuracy: 88%\n",
      "Iteration: 24550  Loss: 0.36981889605522156  Accuracy: 88%\n",
      "Iteration: 24600  Loss: 0.47097161412239075  Accuracy: 88%\n",
      "Iteration: 24650  Loss: 0.5241552591323853  Accuracy: 88%\n",
      "Iteration: 24700  Loss: 0.4432195723056793  Accuracy: 88%\n",
      "Iteration: 24750  Loss: 0.4353276789188385  Accuracy: 88%\n",
      "Iteration: 24800  Loss: 0.525837242603302  Accuracy: 88%\n",
      "Iteration: 24850  Loss: 0.4177892506122589  Accuracy: 88%\n",
      "Iteration: 24900  Loss: 0.4928102195262909  Accuracy: 88%\n",
      "Iteration: 24950  Loss: 0.4117906391620636  Accuracy: 88%\n",
      "Iteration: 25000  Loss: 0.4985804557800293  Accuracy: 88%\n",
      "Iteration: 25050  Loss: 0.5052768588066101  Accuracy: 88%\n",
      "Iteration: 25100  Loss: 0.43895313143730164  Accuracy: 88%\n",
      "Iteration: 25150  Loss: 0.4701864719390869  Accuracy: 88%\n",
      "Iteration: 25200  Loss: 0.4454268515110016  Accuracy: 88%\n",
      "Iteration: 25250  Loss: 0.40078020095825195  Accuracy: 88%\n",
      "Iteration: 25300  Loss: 0.46714383363723755  Accuracy: 88%\n",
      "Iteration: 25350  Loss: 0.41231992840766907  Accuracy: 88%\n",
      "Iteration: 25400  Loss: 0.44488537311553955  Accuracy: 88%\n",
      "Iteration: 25450  Loss: 0.5116299986839294  Accuracy: 88%\n",
      "Iteration: 25500  Loss: 0.43957245349884033  Accuracy: 88%\n",
      "Iteration: 25550  Loss: 0.41128009557724  Accuracy: 88%\n",
      "Iteration: 25600  Loss: 0.3914971649646759  Accuracy: 88%\n",
      "Iteration: 25650  Loss: 0.5579656958580017  Accuracy: 88%\n",
      "Iteration: 25700  Loss: 0.45025530457496643  Accuracy: 88%\n",
      "Iteration: 25750  Loss: 0.4591115415096283  Accuracy: 88%\n",
      "Iteration: 25800  Loss: 0.4282512366771698  Accuracy: 88%\n",
      "Iteration: 25850  Loss: 0.44967129826545715  Accuracy: 88%\n",
      "Iteration: 25900  Loss: 0.46728283166885376  Accuracy: 88%\n",
      "Iteration: 25950  Loss: 0.47588250041007996  Accuracy: 88%\n",
      "Iteration: 26000  Loss: 0.46032652258872986  Accuracy: 88%\n",
      "Iteration: 26050  Loss: 0.4006260931491852  Accuracy: 88%\n",
      "Iteration: 26100  Loss: 0.45285263657569885  Accuracy: 88%\n",
      "Iteration: 26150  Loss: 0.43904420733451843  Accuracy: 88%\n",
      "Iteration: 26200  Loss: 0.4962643086910248  Accuracy: 88%\n",
      "Iteration: 26250  Loss: 0.5105106234550476  Accuracy: 88%\n",
      "Iteration: 26300  Loss: 0.5072804093360901  Accuracy: 88%\n",
      "Iteration: 26350  Loss: 0.39976221323013306  Accuracy: 88%\n",
      "Iteration: 26400  Loss: 0.5620899200439453  Accuracy: 88%\n",
      "Iteration: 26450  Loss: 0.5232306718826294  Accuracy: 88%\n",
      "Iteration: 26500  Loss: 0.5225281715393066  Accuracy: 88%\n",
      "Iteration: 26550  Loss: 0.39638328552246094  Accuracy: 88%\n",
      "Iteration: 26600  Loss: 0.4693565368652344  Accuracy: 88%\n",
      "Iteration: 26650  Loss: 0.3634755313396454  Accuracy: 88%\n",
      "Iteration: 26700  Loss: 0.44593408703804016  Accuracy: 88%\n",
      "Iteration: 26750  Loss: 0.4069552421569824  Accuracy: 88%\n",
      "Iteration: 26800  Loss: 0.40258917212486267  Accuracy: 88%\n",
      "Iteration: 26850  Loss: 0.4142599105834961  Accuracy: 88%\n",
      "Iteration: 26900  Loss: 0.3952580690383911  Accuracy: 88%\n",
      "Iteration: 26950  Loss: 0.4573069214820862  Accuracy: 88%\n",
      "Iteration: 27000  Loss: 0.39778876304626465  Accuracy: 88%\n",
      "Iteration: 27050  Loss: 0.45529842376708984  Accuracy: 88%\n",
      "Iteration: 27100  Loss: 0.4886488914489746  Accuracy: 88%\n",
      "Iteration: 27150  Loss: 0.47040730714797974  Accuracy: 88%\n",
      "Iteration: 27200  Loss: 0.43828949332237244  Accuracy: 88%\n",
      "Iteration: 27250  Loss: 0.480387806892395  Accuracy: 88%\n",
      "Iteration: 27300  Loss: 0.49522775411605835  Accuracy: 88%\n",
      "Iteration: 27350  Loss: 0.4230487048625946  Accuracy: 88%\n",
      "Iteration: 27400  Loss: 0.4770512878894806  Accuracy: 88%\n",
      "Iteration: 27450  Loss: 0.4497976303100586  Accuracy: 88%\n",
      "Iteration: 27500  Loss: 0.3697006404399872  Accuracy: 88%\n",
      "Iteration: 27550  Loss: 0.49087414145469666  Accuracy: 88%\n",
      "Iteration: 27600  Loss: 0.4342966079711914  Accuracy: 88%\n",
      "Iteration: 27650  Loss: 0.5204549431800842  Accuracy: 88%\n",
      "Iteration: 27700  Loss: 0.429439902305603  Accuracy: 88%\n",
      "Iteration: 27750  Loss: 0.44103649258613586  Accuracy: 88%\n",
      "Iteration: 27800  Loss: 0.47044312953948975  Accuracy: 88%\n",
      "Iteration: 27850  Loss: 0.43485915660858154  Accuracy: 88%\n",
      "Iteration: 27900  Loss: 0.455915629863739  Accuracy: 88%\n",
      "Iteration: 27950  Loss: 0.42248469591140747  Accuracy: 88%\n",
      "Iteration: 28000  Loss: 0.376901775598526  Accuracy: 88%\n",
      "Iteration: 28050  Loss: 0.45991772413253784  Accuracy: 88%\n",
      "Iteration: 28100  Loss: 0.3780014216899872  Accuracy: 88%\n",
      "Iteration: 28150  Loss: 0.5145015120506287  Accuracy: 88%\n",
      "Iteration: 28200  Loss: 0.4306558668613434  Accuracy: 88%\n",
      "Iteration: 28250  Loss: 0.44364508986473083  Accuracy: 88%\n",
      "Iteration: 28300  Loss: 0.40249693393707275  Accuracy: 88%\n",
      "Iteration: 28350  Loss: 0.49066418409347534  Accuracy: 88%\n",
      "Iteration: 28400  Loss: 0.4046723544597626  Accuracy: 88%\n",
      "Iteration: 28450  Loss: 0.43912217020988464  Accuracy: 88%\n",
      "Iteration: 28500  Loss: 0.44815579056739807  Accuracy: 88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28550  Loss: 0.43987196683883667  Accuracy: 88%\n",
      "Iteration: 28600  Loss: 0.4111258089542389  Accuracy: 88%\n",
      "Iteration: 28650  Loss: 0.5920518636703491  Accuracy: 88%\n",
      "Iteration: 28700  Loss: 0.4508058428764343  Accuracy: 88%\n",
      "Iteration: 28750  Loss: 0.4666130840778351  Accuracy: 88%\n",
      "Iteration: 28800  Loss: 0.37585583329200745  Accuracy: 88%\n",
      "Iteration: 28850  Loss: 0.479150652885437  Accuracy: 88%\n",
      "Iteration: 28900  Loss: 0.4057624936103821  Accuracy: 88%\n",
      "Iteration: 28950  Loss: 0.47457340359687805  Accuracy: 88%\n",
      "Iteration: 29000  Loss: 0.350464791059494  Accuracy: 88%\n",
      "Iteration: 29050  Loss: 0.35389554500579834  Accuracy: 88%\n",
      "Iteration: 29100  Loss: 0.4836006760597229  Accuracy: 88%\n",
      "Iteration: 29150  Loss: 0.4632928669452667  Accuracy: 88%\n",
      "Iteration: 29200  Loss: 0.4750465452671051  Accuracy: 88%\n",
      "Iteration: 29250  Loss: 0.3945119082927704  Accuracy: 88%\n",
      "Iteration: 29300  Loss: 0.3927062451839447  Accuracy: 88%\n",
      "Iteration: 29350  Loss: 0.361939400434494  Accuracy: 88%\n",
      "Iteration: 29400  Loss: 0.3947620391845703  Accuracy: 88%\n",
      "Iteration: 29450  Loss: 0.41384923458099365  Accuracy: 88%\n",
      "Iteration: 29500  Loss: 0.4414016604423523  Accuracy: 88%\n",
      "Iteration: 29550  Loss: 0.38040924072265625  Accuracy: 88%\n",
      "Iteration: 29600  Loss: 0.4261108934879303  Accuracy: 88%\n",
      "Iteration: 29650  Loss: 0.40456023812294006  Accuracy: 88%\n",
      "Iteration: 29700  Loss: 0.48332077264785767  Accuracy: 88%\n",
      "Iteration: 29750  Loss: 0.3731372654438019  Accuracy: 88%\n",
      "Iteration: 29800  Loss: 0.38313940167427063  Accuracy: 88%\n",
      "Iteration: 29850  Loss: 0.45243993401527405  Accuracy: 88%\n",
      "Iteration: 29900  Loss: 0.5078386068344116  Accuracy: 88%\n",
      "Iteration: 29950  Loss: 0.40577107667922974  Accuracy: 88%\n",
      "Iteration: 30000  Loss: 0.5122262835502625  Accuracy: 88%\n",
      "Iteration: 30050  Loss: 0.4319297671318054  Accuracy: 88%\n",
      "Iteration: 30100  Loss: 0.41811051964759827  Accuracy: 88%\n",
      "Iteration: 30150  Loss: 0.46339577436447144  Accuracy: 88%\n",
      "Iteration: 30200  Loss: 0.4722021818161011  Accuracy: 88%\n",
      "Iteration: 30250  Loss: 0.3896868824958801  Accuracy: 88%\n",
      "Iteration: 30300  Loss: 0.4318307340145111  Accuracy: 88%\n",
      "Iteration: 30350  Loss: 0.45895349979400635  Accuracy: 88%\n",
      "Iteration: 30400  Loss: 0.4886052906513214  Accuracy: 88%\n",
      "Iteration: 30450  Loss: 0.3527722954750061  Accuracy: 88%\n",
      "Iteration: 30500  Loss: 0.4122284948825836  Accuracy: 88%\n",
      "Iteration: 30550  Loss: 0.431660920381546  Accuracy: 88%\n",
      "Iteration: 30600  Loss: 0.49507057666778564  Accuracy: 88%\n",
      "Iteration: 30650  Loss: 0.46953603625297546  Accuracy: 88%\n",
      "Iteration: 30700  Loss: 0.41875898838043213  Accuracy: 88%\n",
      "Iteration: 30750  Loss: 0.44399702548980713  Accuracy: 88%\n",
      "Iteration: 30800  Loss: 0.45020297169685364  Accuracy: 88%\n",
      "Iteration: 30850  Loss: 0.48036399483680725  Accuracy: 88%\n",
      "Iteration: 30900  Loss: 0.5128743052482605  Accuracy: 88%\n",
      "Iteration: 30950  Loss: 0.3702211380004883  Accuracy: 88%\n",
      "Iteration: 31000  Loss: 0.4576817750930786  Accuracy: 88%\n",
      "Iteration: 31050  Loss: 0.48674800992012024  Accuracy: 89%\n",
      "Iteration: 31100  Loss: 0.3584355115890503  Accuracy: 89%\n",
      "Iteration: 31150  Loss: 0.3852638900279999  Accuracy: 89%\n",
      "Iteration: 31200  Loss: 0.44174277782440186  Accuracy: 89%\n",
      "Iteration: 31250  Loss: 0.48150864243507385  Accuracy: 89%\n",
      "Iteration: 31300  Loss: 0.4466257691383362  Accuracy: 89%\n",
      "Iteration: 31350  Loss: 0.4477945864200592  Accuracy: 89%\n",
      "Iteration: 31400  Loss: 0.425273060798645  Accuracy: 89%\n",
      "Iteration: 31450  Loss: 0.47419995069503784  Accuracy: 89%\n",
      "Iteration: 31500  Loss: 0.4253617227077484  Accuracy: 89%\n",
      "Iteration: 31550  Loss: 0.5103041529655457  Accuracy: 89%\n",
      "Iteration: 31600  Loss: 0.49450552463531494  Accuracy: 89%\n",
      "Iteration: 31650  Loss: 0.48350411653518677  Accuracy: 89%\n",
      "Iteration: 31700  Loss: 0.4052533805370331  Accuracy: 89%\n",
      "Iteration: 31750  Loss: 0.4605921506881714  Accuracy: 89%\n",
      "Iteration: 31800  Loss: 0.36839547753334045  Accuracy: 89%\n",
      "Iteration: 31850  Loss: 0.38409966230392456  Accuracy: 89%\n",
      "Iteration: 31900  Loss: 0.44512099027633667  Accuracy: 89%\n",
      "Iteration: 31950  Loss: 0.4372657239437103  Accuracy: 89%\n",
      "Iteration: 32000  Loss: 0.46565312147140503  Accuracy: 89%\n",
      "Iteration: 32050  Loss: 0.4597686529159546  Accuracy: 89%\n",
      "Iteration: 32100  Loss: 0.41747891902923584  Accuracy: 89%\n",
      "Iteration: 32150  Loss: 0.46208176016807556  Accuracy: 89%\n",
      "Iteration: 32200  Loss: 0.39083656668663025  Accuracy: 89%\n",
      "Iteration: 32250  Loss: 0.3710081875324249  Accuracy: 89%\n",
      "Iteration: 32300  Loss: 0.42791956663131714  Accuracy: 89%\n",
      "Iteration: 32350  Loss: 0.4048587381839752  Accuracy: 89%\n",
      "Iteration: 32400  Loss: 0.4542626738548279  Accuracy: 89%\n",
      "Iteration: 32450  Loss: 0.452839195728302  Accuracy: 89%\n",
      "Iteration: 32500  Loss: 0.3894258141517639  Accuracy: 89%\n",
      "Iteration: 32550  Loss: 0.4431542754173279  Accuracy: 89%\n",
      "Iteration: 32600  Loss: 0.38561275601387024  Accuracy: 89%\n",
      "Iteration: 32650  Loss: 0.4505614936351776  Accuracy: 89%\n",
      "Iteration: 32700  Loss: 0.4403441846370697  Accuracy: 89%\n",
      "Iteration: 32750  Loss: 0.3761681914329529  Accuracy: 89%\n",
      "Iteration: 32800  Loss: 0.36346110701560974  Accuracy: 89%\n",
      "Iteration: 32850  Loss: 0.4348074793815613  Accuracy: 89%\n",
      "Iteration: 32900  Loss: 0.44186773896217346  Accuracy: 89%\n",
      "Iteration: 32950  Loss: 0.3779486417770386  Accuracy: 89%\n",
      "Iteration: 33000  Loss: 0.4818643629550934  Accuracy: 89%\n",
      "Iteration: 33050  Loss: 0.5074019432067871  Accuracy: 89%\n",
      "Iteration: 33100  Loss: 0.4815288484096527  Accuracy: 89%\n",
      "Iteration: 33150  Loss: 0.35975417494773865  Accuracy: 89%\n",
      "Iteration: 33200  Loss: 0.44738882780075073  Accuracy: 89%\n",
      "Iteration: 33250  Loss: 0.4367879629135132  Accuracy: 89%\n",
      "Iteration: 33300  Loss: 0.4502585530281067  Accuracy: 89%\n",
      "Iteration: 33350  Loss: 0.39616426825523376  Accuracy: 89%\n",
      "Iteration: 33400  Loss: 0.4729677438735962  Accuracy: 89%\n",
      "Iteration: 33450  Loss: 0.4782801568508148  Accuracy: 89%\n",
      "Iteration: 33500  Loss: 0.4999648332595825  Accuracy: 89%\n",
      "Iteration: 33550  Loss: 0.4394265413284302  Accuracy: 89%\n",
      "Iteration: 33600  Loss: 0.37384137511253357  Accuracy: 89%\n",
      "Iteration: 33650  Loss: 0.3953946530818939  Accuracy: 89%\n",
      "Iteration: 33700  Loss: 0.4155653119087219  Accuracy: 89%\n",
      "Iteration: 33750  Loss: 0.36886727809906006  Accuracy: 89%\n",
      "Iteration: 33800  Loss: 0.48282966017723083  Accuracy: 89%\n",
      "Iteration: 33850  Loss: 0.4282447099685669  Accuracy: 89%\n",
      "Iteration: 33900  Loss: 0.45048850774765015  Accuracy: 89%\n",
      "Iteration: 33950  Loss: 0.3778594434261322  Accuracy: 89%\n",
      "Iteration: 34000  Loss: 0.4673043191432953  Accuracy: 89%\n",
      "Iteration: 34050  Loss: 0.47237497568130493  Accuracy: 89%\n",
      "Iteration: 34100  Loss: 0.4435267448425293  Accuracy: 89%\n",
      "Iteration: 34150  Loss: 0.36502641439437866  Accuracy: 89%\n",
      "Iteration: 34200  Loss: 0.4286627471446991  Accuracy: 89%\n",
      "Iteration: 34250  Loss: 0.40922534465789795  Accuracy: 89%\n",
      "Iteration: 34300  Loss: 0.42188602685928345  Accuracy: 89%\n",
      "Iteration: 34350  Loss: 0.3552919924259186  Accuracy: 89%\n",
      "Iteration: 34400  Loss: 0.43228206038475037  Accuracy: 89%\n",
      "Iteration: 34450  Loss: 0.37547650933265686  Accuracy: 89%\n",
      "Iteration: 34500  Loss: 0.3996330499649048  Accuracy: 89%\n",
      "Iteration: 34550  Loss: 0.44226813316345215  Accuracy: 89%\n",
      "Iteration: 34600  Loss: 0.4316387176513672  Accuracy: 89%\n",
      "Iteration: 34650  Loss: 0.47065436840057373  Accuracy: 89%\n",
      "Iteration: 34700  Loss: 0.3896833658218384  Accuracy: 89%\n",
      "Iteration: 34750  Loss: 0.354525625705719  Accuracy: 89%\n",
      "Iteration: 34800  Loss: 0.4901047348976135  Accuracy: 89%\n",
      "Iteration: 34850  Loss: 0.43980100750923157  Accuracy: 89%\n",
      "Iteration: 34900  Loss: 0.4526478052139282  Accuracy: 89%\n",
      "Iteration: 34950  Loss: 0.4428981840610504  Accuracy: 89%\n",
      "Iteration: 35000  Loss: 0.44702693819999695  Accuracy: 89%\n",
      "Iteration: 35050  Loss: 0.5045220851898193  Accuracy: 89%\n",
      "Iteration: 35100  Loss: 0.35183852910995483  Accuracy: 89%\n",
      "Iteration: 35150  Loss: 0.43970155715942383  Accuracy: 89%\n",
      "Iteration: 35200  Loss: 0.450775146484375  Accuracy: 89%\n",
      "Iteration: 35250  Loss: 0.39803391695022583  Accuracy: 89%\n",
      "Iteration: 35300  Loss: 0.49027925729751587  Accuracy: 89%\n",
      "Iteration: 35350  Loss: 0.39778587222099304  Accuracy: 89%\n",
      "Iteration: 35400  Loss: 0.45730623602867126  Accuracy: 89%\n",
      "Iteration: 35450  Loss: 0.41161447763442993  Accuracy: 89%\n",
      "Iteration: 35500  Loss: 0.4162212610244751  Accuracy: 89%\n",
      "Iteration: 35550  Loss: 0.40273672342300415  Accuracy: 89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 35600  Loss: 0.4011894166469574  Accuracy: 89%\n",
      "Iteration: 35650  Loss: 0.4859866797924042  Accuracy: 89%\n",
      "Iteration: 35700  Loss: 0.3776416778564453  Accuracy: 89%\n",
      "Iteration: 35750  Loss: 0.43878719210624695  Accuracy: 89%\n",
      "Iteration: 35800  Loss: 0.455872118473053  Accuracy: 89%\n",
      "Iteration: 35850  Loss: 0.46146899461746216  Accuracy: 89%\n",
      "Iteration: 35900  Loss: 0.3357788026332855  Accuracy: 89%\n",
      "Iteration: 35950  Loss: 0.41811293363571167  Accuracy: 89%\n",
      "Iteration: 36000  Loss: 0.4468613266944885  Accuracy: 89%\n",
      "Iteration: 36050  Loss: 0.4252408742904663  Accuracy: 89%\n",
      "Iteration: 36100  Loss: 0.419264018535614  Accuracy: 89%\n",
      "Iteration: 36150  Loss: 0.44965335726737976  Accuracy: 89%\n",
      "Iteration: 36200  Loss: 0.43062353134155273  Accuracy: 89%\n",
      "Iteration: 36250  Loss: 0.3755440413951874  Accuracy: 89%\n",
      "Iteration: 36300  Loss: 0.5108059048652649  Accuracy: 89%\n",
      "Iteration: 36350  Loss: 0.4168337881565094  Accuracy: 89%\n",
      "Iteration: 36400  Loss: 0.48356741666793823  Accuracy: 89%\n",
      "Iteration: 36450  Loss: 0.4630427062511444  Accuracy: 89%\n",
      "Iteration: 36500  Loss: 0.4289376735687256  Accuracy: 89%\n",
      "Iteration: 36550  Loss: 0.4004051685333252  Accuracy: 89%\n",
      "Iteration: 36600  Loss: 0.46200430393218994  Accuracy: 89%\n",
      "Iteration: 36650  Loss: 0.32858186960220337  Accuracy: 89%\n",
      "Iteration: 36700  Loss: 0.37287530303001404  Accuracy: 89%\n",
      "Iteration: 36750  Loss: 0.3544243276119232  Accuracy: 89%\n",
      "Iteration: 36800  Loss: 0.3894360363483429  Accuracy: 89%\n",
      "Iteration: 36850  Loss: 0.4422733187675476  Accuracy: 89%\n",
      "Iteration: 36900  Loss: 0.4633639454841614  Accuracy: 89%\n",
      "Iteration: 36950  Loss: 0.36695075035095215  Accuracy: 89%\n",
      "Iteration: 37000  Loss: 0.4119701683521271  Accuracy: 89%\n",
      "Iteration: 37050  Loss: 0.4708133935928345  Accuracy: 89%\n",
      "Iteration: 37100  Loss: 0.40739455819129944  Accuracy: 89%\n",
      "Iteration: 37150  Loss: 0.4259779751300812  Accuracy: 89%\n",
      "Iteration: 37200  Loss: 0.42141634225845337  Accuracy: 89%\n",
      "Iteration: 37250  Loss: 0.4479881823062897  Accuracy: 89%\n",
      "Iteration: 37300  Loss: 0.35127079486846924  Accuracy: 89%\n",
      "Iteration: 37350  Loss: 0.46087998151779175  Accuracy: 89%\n",
      "Iteration: 37400  Loss: 0.40493646264076233  Accuracy: 89%\n",
      "Iteration: 37450  Loss: 0.4047497808933258  Accuracy: 89%\n",
      "Iteration: 37500  Loss: 0.5507727861404419  Accuracy: 89%\n",
      "Iteration: 37550  Loss: 0.3890729546546936  Accuracy: 89%\n",
      "Iteration: 37600  Loss: 0.44860681891441345  Accuracy: 89%\n",
      "Iteration: 37650  Loss: 0.3864816725254059  Accuracy: 89%\n",
      "Iteration: 37700  Loss: 0.41422563791275024  Accuracy: 89%\n",
      "Iteration: 37750  Loss: 0.3800075948238373  Accuracy: 89%\n",
      "Iteration: 37800  Loss: 0.36535993218421936  Accuracy: 89%\n",
      "Iteration: 37850  Loss: 0.3488941192626953  Accuracy: 89%\n",
      "Iteration: 37900  Loss: 0.35130858421325684  Accuracy: 89%\n",
      "Iteration: 37950  Loss: 0.43174582719802856  Accuracy: 89%\n",
      "Iteration: 38000  Loss: 0.41020187735557556  Accuracy: 89%\n",
      "Iteration: 38050  Loss: 0.40122175216674805  Accuracy: 89%\n",
      "Iteration: 38100  Loss: 0.42093420028686523  Accuracy: 89%\n",
      "Iteration: 38150  Loss: 0.3789104223251343  Accuracy: 89%\n",
      "Iteration: 38200  Loss: 0.42242497205734253  Accuracy: 89%\n",
      "Iteration: 38250  Loss: 0.4583306312561035  Accuracy: 89%\n",
      "Iteration: 38300  Loss: 0.38306063413619995  Accuracy: 89%\n",
      "Iteration: 38350  Loss: 0.3962421715259552  Accuracy: 89%\n",
      "Iteration: 38400  Loss: 0.44227319955825806  Accuracy: 89%\n",
      "Iteration: 38450  Loss: 0.4640066921710968  Accuracy: 89%\n",
      "Iteration: 38500  Loss: 0.3877948224544525  Accuracy: 89%\n",
      "Iteration: 38550  Loss: 0.4874899089336395  Accuracy: 89%\n",
      "Iteration: 38600  Loss: 0.44110167026519775  Accuracy: 89%\n",
      "Iteration: 38650  Loss: 0.3661945164203644  Accuracy: 89%\n",
      "Iteration: 38700  Loss: 0.36937975883483887  Accuracy: 89%\n",
      "Iteration: 38750  Loss: 0.45954909920692444  Accuracy: 89%\n",
      "Iteration: 38800  Loss: 0.4292498826980591  Accuracy: 89%\n",
      "Iteration: 38850  Loss: 0.394559770822525  Accuracy: 89%\n",
      "Iteration: 38900  Loss: 0.41272208094596863  Accuracy: 89%\n",
      "Iteration: 38950  Loss: 0.4257255792617798  Accuracy: 89%\n",
      "Iteration: 39000  Loss: 0.34493136405944824  Accuracy: 89%\n",
      "Iteration: 39050  Loss: 0.4034094214439392  Accuracy: 89%\n",
      "Iteration: 39100  Loss: 0.41745948791503906  Accuracy: 89%\n",
      "Iteration: 39150  Loss: 0.3329848647117615  Accuracy: 89%\n",
      "Iteration: 39200  Loss: 0.3754802644252777  Accuracy: 89%\n",
      "Iteration: 39250  Loss: 0.3910396695137024  Accuracy: 89%\n",
      "Iteration: 39300  Loss: 0.4649336636066437  Accuracy: 89%\n",
      "Iteration: 39350  Loss: 0.5052024126052856  Accuracy: 89%\n",
      "Iteration: 39400  Loss: 0.38635608553886414  Accuracy: 89%\n",
      "Iteration: 39450  Loss: 0.3707992434501648  Accuracy: 89%\n",
      "Iteration: 39500  Loss: 0.3833774924278259  Accuracy: 89%\n",
      "Iteration: 39550  Loss: 0.4047340452671051  Accuracy: 89%\n",
      "Iteration: 39600  Loss: 0.4049466550350189  Accuracy: 89%\n",
      "Iteration: 39650  Loss: 0.44236481189727783  Accuracy: 89%\n",
      "Iteration: 39700  Loss: 0.3956620395183563  Accuracy: 89%\n",
      "Iteration: 39750  Loss: 0.47874021530151367  Accuracy: 89%\n",
      "Iteration: 39800  Loss: 0.39575210213661194  Accuracy: 89%\n",
      "Iteration: 39850  Loss: 0.44852131605148315  Accuracy: 89%\n",
      "Iteration: 39900  Loss: 0.42557111382484436  Accuracy: 89%\n",
      "Iteration: 39950  Loss: 0.39993125200271606  Accuracy: 89%\n",
      "Iteration: 40000  Loss: 0.4137159585952759  Accuracy: 89%\n",
      "Iteration: 40050  Loss: 0.36614686250686646  Accuracy: 89%\n",
      "Iteration: 40100  Loss: 0.37900784611701965  Accuracy: 89%\n",
      "Iteration: 40150  Loss: 0.40987733006477356  Accuracy: 89%\n",
      "Iteration: 40200  Loss: 0.43300706148147583  Accuracy: 89%\n",
      "Iteration: 40250  Loss: 0.28059151768684387  Accuracy: 89%\n",
      "Iteration: 40300  Loss: 0.372311532497406  Accuracy: 89%\n",
      "Iteration: 40350  Loss: 0.4639154374599457  Accuracy: 89%\n",
      "Iteration: 40400  Loss: 0.4910504221916199  Accuracy: 89%\n",
      "Iteration: 40450  Loss: 0.3800048530101776  Accuracy: 89%\n",
      "Iteration: 40500  Loss: 0.42876896262168884  Accuracy: 89%\n",
      "Iteration: 40550  Loss: 0.48339828848838806  Accuracy: 89%\n",
      "Iteration: 40600  Loss: 0.41184017062187195  Accuracy: 89%\n",
      "Iteration: 40650  Loss: 0.4727035462856293  Accuracy: 89%\n",
      "Iteration: 40700  Loss: 0.4434240758419037  Accuracy: 89%\n",
      "Iteration: 40750  Loss: 0.40591302514076233  Accuracy: 89%\n",
      "Iteration: 40800  Loss: 0.41511136293411255  Accuracy: 89%\n",
      "Iteration: 40850  Loss: 0.36312249302864075  Accuracy: 89%\n",
      "Iteration: 40900  Loss: 0.307956725358963  Accuracy: 89%\n",
      "Iteration: 40950  Loss: 0.38871070742607117  Accuracy: 89%\n",
      "Iteration: 41000  Loss: 0.39023837447166443  Accuracy: 89%\n",
      "Iteration: 41050  Loss: 0.4513157308101654  Accuracy: 89%\n",
      "Iteration: 41100  Loss: 0.30310913920402527  Accuracy: 89%\n",
      "Iteration: 41150  Loss: 0.3354833424091339  Accuracy: 89%\n",
      "Iteration: 41200  Loss: 0.39876797795295715  Accuracy: 89%\n",
      "Iteration: 41250  Loss: 0.32787951827049255  Accuracy: 89%\n",
      "Iteration: 41300  Loss: 0.3971104323863983  Accuracy: 89%\n",
      "Iteration: 41350  Loss: 0.3526613414287567  Accuracy: 89%\n",
      "Iteration: 41400  Loss: 0.3753655254840851  Accuracy: 89%\n",
      "Iteration: 41450  Loss: 0.4179127514362335  Accuracy: 89%\n",
      "Iteration: 41500  Loss: 0.5174142122268677  Accuracy: 89%\n",
      "Iteration: 41550  Loss: 0.5051592588424683  Accuracy: 89%\n",
      "Iteration: 41600  Loss: 0.494964063167572  Accuracy: 89%\n",
      "Iteration: 41650  Loss: 0.4293760359287262  Accuracy: 89%\n",
      "Iteration: 41700  Loss: 0.3863232731819153  Accuracy: 89%\n",
      "Iteration: 41750  Loss: 0.4207192659378052  Accuracy: 89%\n",
      "Iteration: 41800  Loss: 0.3547452986240387  Accuracy: 89%\n",
      "Iteration: 41850  Loss: 0.42215496301651  Accuracy: 89%\n",
      "Iteration: 41900  Loss: 0.4466141164302826  Accuracy: 89%\n",
      "Iteration: 41950  Loss: 0.3337487578392029  Accuracy: 89%\n",
      "Iteration: 42000  Loss: 0.44005998969078064  Accuracy: 89%\n",
      "Iteration: 42050  Loss: 0.48046693205833435  Accuracy: 89%\n",
      "Iteration: 42100  Loss: 0.46044713258743286  Accuracy: 89%\n",
      "Iteration: 42150  Loss: 0.45317623019218445  Accuracy: 89%\n",
      "Iteration: 42200  Loss: 0.4765324890613556  Accuracy: 89%\n",
      "Iteration: 42250  Loss: 0.36827710270881653  Accuracy: 89%\n",
      "Iteration: 42300  Loss: 0.49467068910598755  Accuracy: 89%\n",
      "Iteration: 42350  Loss: 0.3537956178188324  Accuracy: 89%\n",
      "Iteration: 42400  Loss: 0.40687933564186096  Accuracy: 89%\n",
      "Iteration: 42450  Loss: 0.5032039284706116  Accuracy: 89%\n",
      "Iteration: 42500  Loss: 0.308991938829422  Accuracy: 89%\n",
      "Iteration: 42550  Loss: 0.45031508803367615  Accuracy: 89%\n",
      "Iteration: 42600  Loss: 0.4374541640281677  Accuracy: 89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 42650  Loss: 0.35116562247276306  Accuracy: 89%\n",
      "Iteration: 42700  Loss: 0.39099693298339844  Accuracy: 89%\n",
      "Iteration: 42750  Loss: 0.4001069962978363  Accuracy: 89%\n",
      "Iteration: 42800  Loss: 0.40137749910354614  Accuracy: 89%\n",
      "Iteration: 42850  Loss: 0.45504507422447205  Accuracy: 89%\n",
      "Iteration: 42900  Loss: 0.4593439996242523  Accuracy: 89%\n",
      "Iteration: 42950  Loss: 0.32568836212158203  Accuracy: 89%\n",
      "Iteration: 43000  Loss: 0.379256010055542  Accuracy: 89%\n",
      "Iteration: 43050  Loss: 0.3700474202632904  Accuracy: 89%\n",
      "Iteration: 43100  Loss: 0.2879791557788849  Accuracy: 89%\n",
      "Iteration: 43150  Loss: 0.44683176279067993  Accuracy: 89%\n",
      "Iteration: 43200  Loss: 0.29510051012039185  Accuracy: 89%\n",
      "Iteration: 43250  Loss: 0.36512017250061035  Accuracy: 89%\n",
      "Iteration: 43300  Loss: 0.3824140131473541  Accuracy: 89%\n",
      "Iteration: 43350  Loss: 0.48451998829841614  Accuracy: 89%\n",
      "Iteration: 43400  Loss: 0.5137386918067932  Accuracy: 89%\n",
      "Iteration: 43450  Loss: 0.3518655598163605  Accuracy: 89%\n",
      "Iteration: 43500  Loss: 0.39585426449775696  Accuracy: 89%\n",
      "Iteration: 43550  Loss: 0.4004949927330017  Accuracy: 89%\n",
      "Iteration: 43600  Loss: 0.41017475724220276  Accuracy: 89%\n",
      "Iteration: 43650  Loss: 0.367947518825531  Accuracy: 89%\n",
      "Iteration: 43700  Loss: 0.3835897743701935  Accuracy: 89%\n",
      "Iteration: 43750  Loss: 0.4700120985507965  Accuracy: 89%\n",
      "Iteration: 43800  Loss: 0.39131179451942444  Accuracy: 89%\n",
      "Iteration: 43850  Loss: 0.43066057562828064  Accuracy: 89%\n",
      "Iteration: 43900  Loss: 0.27313464879989624  Accuracy: 89%\n",
      "Iteration: 43950  Loss: 0.4142061173915863  Accuracy: 89%\n",
      "Iteration: 44000  Loss: 0.3860638737678528  Accuracy: 89%\n",
      "Iteration: 44050  Loss: 0.4371398687362671  Accuracy: 89%\n",
      "Iteration: 44100  Loss: 0.4213034510612488  Accuracy: 89%\n",
      "Iteration: 44150  Loss: 0.2854747474193573  Accuracy: 89%\n",
      "Iteration: 44200  Loss: 0.5095475316047668  Accuracy: 89%\n",
      "Iteration: 44250  Loss: 0.4156585931777954  Accuracy: 89%\n",
      "Iteration: 44300  Loss: 0.44206199049949646  Accuracy: 89%\n",
      "Iteration: 44350  Loss: 0.3917872905731201  Accuracy: 89%\n",
      "Iteration: 44400  Loss: 0.3832255005836487  Accuracy: 89%\n",
      "Iteration: 44450  Loss: 0.3826994001865387  Accuracy: 89%\n",
      "Iteration: 44500  Loss: 0.4335179626941681  Accuracy: 89%\n",
      "Iteration: 44550  Loss: 0.3648402690887451  Accuracy: 89%\n",
      "Iteration: 44600  Loss: 0.3666512072086334  Accuracy: 89%\n",
      "Iteration: 44650  Loss: 0.34742793440818787  Accuracy: 89%\n",
      "Iteration: 44700  Loss: 0.38372358679771423  Accuracy: 89%\n",
      "Iteration: 44750  Loss: 0.3773057758808136  Accuracy: 89%\n",
      "Iteration: 44800  Loss: 0.3345125913619995  Accuracy: 89%\n",
      "Iteration: 44850  Loss: 0.42369845509529114  Accuracy: 89%\n",
      "Iteration: 44900  Loss: 0.35556140542030334  Accuracy: 89%\n",
      "Iteration: 44950  Loss: 0.4188573956489563  Accuracy: 89%\n",
      "Iteration: 45000  Loss: 0.38882124423980713  Accuracy: 89%\n",
      "Iteration: 45050  Loss: 0.3892073929309845  Accuracy: 89%\n",
      "Iteration: 45100  Loss: 0.46617239713668823  Accuracy: 89%\n",
      "Iteration: 45150  Loss: 0.43513306975364685  Accuracy: 89%\n",
      "Iteration: 45200  Loss: 0.37420740723609924  Accuracy: 89%\n",
      "Iteration: 45250  Loss: 0.4475586414337158  Accuracy: 89%\n",
      "Iteration: 45300  Loss: 0.3090730607509613  Accuracy: 89%\n",
      "Iteration: 45350  Loss: 0.3232981562614441  Accuracy: 89%\n",
      "Iteration: 45400  Loss: 0.38173845410346985  Accuracy: 89%\n",
      "Iteration: 45450  Loss: 0.3277933597564697  Accuracy: 89%\n",
      "Iteration: 45500  Loss: 0.462218701839447  Accuracy: 89%\n",
      "Iteration: 45550  Loss: 0.4228159487247467  Accuracy: 89%\n",
      "Iteration: 45600  Loss: 0.4476524293422699  Accuracy: 89%\n",
      "Iteration: 45650  Loss: 0.3523772060871124  Accuracy: 89%\n",
      "Iteration: 45700  Loss: 0.3517625629901886  Accuracy: 89%\n",
      "Iteration: 45750  Loss: 0.520171046257019  Accuracy: 89%\n",
      "Iteration: 45800  Loss: 0.35463225841522217  Accuracy: 89%\n",
      "Iteration: 45850  Loss: 0.4188569486141205  Accuracy: 89%\n",
      "Iteration: 45900  Loss: 0.46778708696365356  Accuracy: 89%\n",
      "Iteration: 45950  Loss: 0.39922165870666504  Accuracy: 89%\n",
      "Iteration: 46000  Loss: 0.4005453884601593  Accuracy: 89%\n",
      "Iteration: 46050  Loss: 0.42029789090156555  Accuracy: 89%\n",
      "Iteration: 46100  Loss: 0.42607802152633667  Accuracy: 89%\n",
      "Iteration: 46150  Loss: 0.38384178280830383  Accuracy: 89%\n",
      "Iteration: 46200  Loss: 0.5768337845802307  Accuracy: 89%\n",
      "Iteration: 46250  Loss: 0.39597028493881226  Accuracy: 89%\n",
      "Iteration: 46300  Loss: 0.2983956038951874  Accuracy: 89%\n",
      "Iteration: 46350  Loss: 0.359749972820282  Accuracy: 89%\n",
      "Iteration: 46400  Loss: 0.38740116357803345  Accuracy: 89%\n",
      "Iteration: 46450  Loss: 0.3767867982387543  Accuracy: 89%\n",
      "Iteration: 46500  Loss: 0.3243538439273834  Accuracy: 89%\n",
      "Iteration: 46550  Loss: 0.40401482582092285  Accuracy: 89%\n",
      "Iteration: 46600  Loss: 0.413201242685318  Accuracy: 89%\n",
      "Iteration: 46650  Loss: 0.4760962426662445  Accuracy: 89%\n",
      "Iteration: 46700  Loss: 0.3739745318889618  Accuracy: 89%\n",
      "Iteration: 46750  Loss: 0.41027820110321045  Accuracy: 89%\n",
      "Iteration: 46800  Loss: 0.38361215591430664  Accuracy: 89%\n",
      "Iteration: 46850  Loss: 0.3955017030239105  Accuracy: 89%\n",
      "Iteration: 46900  Loss: 0.42657536268234253  Accuracy: 89%\n",
      "Iteration: 46950  Loss: 0.4089592397212982  Accuracy: 89%\n",
      "Iteration: 47000  Loss: 0.5553804039955139  Accuracy: 89%\n",
      "Iteration: 47050  Loss: 0.5025482177734375  Accuracy: 89%\n",
      "Iteration: 47100  Loss: 0.37044647336006165  Accuracy: 89%\n",
      "Iteration: 47150  Loss: 0.39184629917144775  Accuracy: 89%\n",
      "Iteration: 47200  Loss: 0.4168449640274048  Accuracy: 89%\n",
      "Iteration: 47250  Loss: 0.3990265130996704  Accuracy: 89%\n",
      "Iteration: 47300  Loss: 0.3613230586051941  Accuracy: 89%\n",
      "Iteration: 47350  Loss: 0.3537973463535309  Accuracy: 89%\n",
      "Iteration: 47400  Loss: 0.38555312156677246  Accuracy: 89%\n",
      "Iteration: 47450  Loss: 0.39029207825660706  Accuracy: 89%\n",
      "Iteration: 47500  Loss: 0.3471687436103821  Accuracy: 89%\n",
      "Iteration: 47550  Loss: 0.46789002418518066  Accuracy: 89%\n",
      "Iteration: 47600  Loss: 0.4015886187553406  Accuracy: 89%\n",
      "Iteration: 47650  Loss: 0.3615321218967438  Accuracy: 89%\n",
      "Iteration: 47700  Loss: 0.33463603258132935  Accuracy: 89%\n",
      "Iteration: 47750  Loss: 0.3887058198451996  Accuracy: 89%\n",
      "Iteration: 47800  Loss: 0.37167084217071533  Accuracy: 89%\n",
      "Iteration: 47850  Loss: 0.40327268838882446  Accuracy: 89%\n",
      "Iteration: 47900  Loss: 0.41645848751068115  Accuracy: 89%\n",
      "Iteration: 47950  Loss: 0.37564241886138916  Accuracy: 89%\n",
      "Iteration: 48000  Loss: 0.403401643037796  Accuracy: 89%\n",
      "Iteration: 48050  Loss: 0.3924846351146698  Accuracy: 89%\n",
      "Iteration: 48100  Loss: 0.42251935601234436  Accuracy: 89%\n",
      "Iteration: 48150  Loss: 0.39160478115081787  Accuracy: 89%\n",
      "Iteration: 48200  Loss: 0.5420264005661011  Accuracy: 89%\n",
      "Iteration: 48250  Loss: 0.4187384843826294  Accuracy: 89%\n",
      "Iteration: 48300  Loss: 0.5263683199882507  Accuracy: 89%\n",
      "Iteration: 48350  Loss: 0.4025866687297821  Accuracy: 89%\n",
      "Iteration: 48400  Loss: 0.37040507793426514  Accuracy: 89%\n",
      "Iteration: 48450  Loss: 0.34135565161705017  Accuracy: 89%\n",
      "Iteration: 48500  Loss: 0.3478536009788513  Accuracy: 89%\n",
      "Iteration: 48550  Loss: 0.36149901151657104  Accuracy: 89%\n",
      "Iteration: 48600  Loss: 0.3527556359767914  Accuracy: 89%\n",
      "Iteration: 48650  Loss: 0.35024967789649963  Accuracy: 89%\n",
      "Iteration: 48700  Loss: 0.3468298614025116  Accuracy: 89%\n",
      "Iteration: 48750  Loss: 0.3971554636955261  Accuracy: 89%\n",
      "Iteration: 48800  Loss: 0.4369029700756073  Accuracy: 89%\n",
      "Iteration: 48850  Loss: 0.4448711574077606  Accuracy: 89%\n",
      "Iteration: 48900  Loss: 0.3381882607936859  Accuracy: 89%\n",
      "Iteration: 48950  Loss: 0.4241158068180084  Accuracy: 89%\n",
      "Iteration: 49000  Loss: 0.39979130029678345  Accuracy: 89%\n",
      "Iteration: 49050  Loss: 0.3847275376319885  Accuracy: 89%\n",
      "Iteration: 49100  Loss: 0.40024277567863464  Accuracy: 89%\n",
      "Iteration: 49150  Loss: 0.3612337112426758  Accuracy: 89%\n",
      "Iteration: 49200  Loss: 0.39671987295150757  Accuracy: 89%\n",
      "Iteration: 49250  Loss: 0.3543362021446228  Accuracy: 89%\n",
      "Iteration: 49300  Loss: 0.3693167269229889  Accuracy: 89%\n",
      "Iteration: 49350  Loss: 0.3188702464103699  Accuracy: 89%\n",
      "Iteration: 49400  Loss: 0.4581805169582367  Accuracy: 89%\n",
      "Iteration: 49450  Loss: 0.3138672411441803  Accuracy: 89%\n",
      "Iteration: 49500  Loss: 0.4043436646461487  Accuracy: 89%\n",
      "Iteration: 49550  Loss: 0.4808002710342407  Accuracy: 89%\n",
      "Iteration: 49600  Loss: 0.3598659634590149  Accuracy: 89%\n",
      "Iteration: 49650  Loss: 0.3792739510536194  Accuracy: 89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 49700  Loss: 0.44707515835762024  Accuracy: 89%\n",
      "Iteration: 49750  Loss: 0.38007304072380066  Accuracy: 89%\n",
      "Iteration: 49800  Loss: 0.47072654962539673  Accuracy: 89%\n",
      "Iteration: 49850  Loss: 0.3831368088722229  Accuracy: 89%\n",
      "Iteration: 49900  Loss: 0.44377654790878296  Accuracy: 89%\n",
      "Iteration: 49950  Loss: 0.4706014096736908  Accuracy: 89%\n",
      "Iteration: 50000  Loss: 0.31862667202949524  Accuracy: 89%\n",
      "Iteration: 50050  Loss: 0.2937600314617157  Accuracy: 89%\n",
      "Iteration: 50100  Loss: 0.37500667572021484  Accuracy: 89%\n",
      "Iteration: 50150  Loss: 0.3338630199432373  Accuracy: 89%\n",
      "Iteration: 50200  Loss: 0.45299825072288513  Accuracy: 89%\n",
      "Iteration: 50250  Loss: 0.38514769077301025  Accuracy: 89%\n",
      "Iteration: 50300  Loss: 0.33011874556541443  Accuracy: 89%\n",
      "Iteration: 50350  Loss: 0.374053418636322  Accuracy: 89%\n",
      "Iteration: 50400  Loss: 0.40514466166496277  Accuracy: 89%\n",
      "Iteration: 50450  Loss: 0.3565075993537903  Accuracy: 89%\n",
      "Iteration: 50500  Loss: 0.4260198771953583  Accuracy: 89%\n",
      "Iteration: 50550  Loss: 0.3901911675930023  Accuracy: 89%\n",
      "Iteration: 50600  Loss: 0.3400852084159851  Accuracy: 89%\n",
      "Iteration: 50650  Loss: 0.3450378179550171  Accuracy: 89%\n",
      "Iteration: 50700  Loss: 0.346559077501297  Accuracy: 89%\n",
      "Iteration: 50750  Loss: 0.4138493835926056  Accuracy: 89%\n",
      "Iteration: 50800  Loss: 0.4058416783809662  Accuracy: 89%\n",
      "Iteration: 50850  Loss: 0.4232574701309204  Accuracy: 89%\n",
      "Iteration: 50900  Loss: 0.41355881094932556  Accuracy: 89%\n",
      "Iteration: 50950  Loss: 0.3686865270137787  Accuracy: 89%\n",
      "Iteration: 51000  Loss: 0.5569090843200684  Accuracy: 89%\n",
      "Iteration: 51050  Loss: 0.40896841883659363  Accuracy: 89%\n",
      "Iteration: 51100  Loss: 0.37659236788749695  Accuracy: 89%\n",
      "Iteration: 51150  Loss: 0.38290610909461975  Accuracy: 89%\n",
      "Iteration: 51200  Loss: 0.38724663853645325  Accuracy: 89%\n",
      "Iteration: 51250  Loss: 0.38302481174468994  Accuracy: 89%\n",
      "Iteration: 51300  Loss: 0.43987536430358887  Accuracy: 89%\n",
      "Iteration: 51350  Loss: 0.4124404489994049  Accuracy: 89%\n",
      "Iteration: 51400  Loss: 0.45599478483200073  Accuracy: 89%\n",
      "Iteration: 51450  Loss: 0.3481525182723999  Accuracy: 89%\n",
      "Iteration: 51500  Loss: 0.3085578680038452  Accuracy: 89%\n",
      "Iteration: 51550  Loss: 0.3386036455631256  Accuracy: 89%\n",
      "Iteration: 51600  Loss: 0.3424696922302246  Accuracy: 89%\n",
      "Iteration: 51650  Loss: 0.4558261036872864  Accuracy: 89%\n",
      "Iteration: 51700  Loss: 0.40647074580192566  Accuracy: 89%\n",
      "Iteration: 51750  Loss: 0.44208648800849915  Accuracy: 89%\n",
      "Iteration: 51800  Loss: 0.40814661979675293  Accuracy: 89%\n",
      "Iteration: 51850  Loss: 0.39840513467788696  Accuracy: 89%\n",
      "Iteration: 51900  Loss: 0.40894651412963867  Accuracy: 89%\n",
      "Iteration: 51950  Loss: 0.3740086555480957  Accuracy: 89%\n",
      "Iteration: 52000  Loss: 0.4404207170009613  Accuracy: 89%\n",
      "Iteration: 52050  Loss: 0.37349408864974976  Accuracy: 89%\n",
      "Iteration: 52100  Loss: 0.34373584389686584  Accuracy: 89%\n",
      "Iteration: 52150  Loss: 0.399451345205307  Accuracy: 89%\n",
      "Iteration: 52200  Loss: 0.3475864827632904  Accuracy: 89%\n",
      "Iteration: 52250  Loss: 0.37110865116119385  Accuracy: 89%\n",
      "Iteration: 52300  Loss: 0.32606393098831177  Accuracy: 89%\n",
      "Iteration: 52350  Loss: 0.37087172269821167  Accuracy: 89%\n",
      "Iteration: 52400  Loss: 0.46075356006622314  Accuracy: 89%\n",
      "Iteration: 52450  Loss: 0.34944388270378113  Accuracy: 89%\n",
      "Iteration: 52500  Loss: 0.4487358331680298  Accuracy: 89%\n",
      "Iteration: 52550  Loss: 0.4247400164604187  Accuracy: 89%\n",
      "Iteration: 52600  Loss: 0.4315422475337982  Accuracy: 89%\n",
      "Iteration: 52650  Loss: 0.3783530592918396  Accuracy: 89%\n",
      "Iteration: 52700  Loss: 0.42763543128967285  Accuracy: 89%\n",
      "Iteration: 52750  Loss: 0.37353065609931946  Accuracy: 89%\n",
      "Iteration: 52800  Loss: 0.4078919589519501  Accuracy: 89%\n",
      "Iteration: 52850  Loss: 0.4681609272956848  Accuracy: 89%\n",
      "Iteration: 52900  Loss: 0.4414844810962677  Accuracy: 89%\n",
      "Iteration: 52950  Loss: 0.44186827540397644  Accuracy: 89%\n",
      "Iteration: 53000  Loss: 0.327300101518631  Accuracy: 89%\n",
      "Iteration: 53050  Loss: 0.34477245807647705  Accuracy: 89%\n",
      "Iteration: 53100  Loss: 0.3971145749092102  Accuracy: 89%\n",
      "Iteration: 53150  Loss: 0.33959031105041504  Accuracy: 89%\n",
      "Iteration: 53200  Loss: 0.3789689242839813  Accuracy: 89%\n",
      "Iteration: 53250  Loss: 0.37488386034965515  Accuracy: 89%\n",
      "Iteration: 53300  Loss: 0.37497419118881226  Accuracy: 89%\n",
      "Iteration: 53350  Loss: 0.3124164342880249  Accuracy: 89%\n",
      "Iteration: 53400  Loss: 0.38017213344573975  Accuracy: 89%\n",
      "Iteration: 53450  Loss: 0.3358587324619293  Accuracy: 89%\n",
      "Iteration: 53500  Loss: 0.3985695540904999  Accuracy: 89%\n",
      "Iteration: 53550  Loss: 0.384015291929245  Accuracy: 89%\n",
      "Iteration: 53600  Loss: 0.350231796503067  Accuracy: 89%\n",
      "Iteration: 53650  Loss: 0.4262304902076721  Accuracy: 89%\n",
      "Iteration: 53700  Loss: 0.3600162863731384  Accuracy: 89%\n",
      "Iteration: 53750  Loss: 0.361642062664032  Accuracy: 89%\n",
      "Iteration: 53800  Loss: 0.39951980113983154  Accuracy: 89%\n",
      "Iteration: 53850  Loss: 0.4234258234500885  Accuracy: 89%\n",
      "Iteration: 53900  Loss: 0.4521172344684601  Accuracy: 89%\n",
      "Iteration: 53950  Loss: 0.38208597898483276  Accuracy: 89%\n",
      "Iteration: 54000  Loss: 0.35922953486442566  Accuracy: 89%\n",
      "Iteration: 54050  Loss: 0.3576847016811371  Accuracy: 89%\n",
      "Iteration: 54100  Loss: 0.4563176929950714  Accuracy: 89%\n",
      "Iteration: 54150  Loss: 0.3817199170589447  Accuracy: 89%\n",
      "Iteration: 54200  Loss: 0.33704623579978943  Accuracy: 89%\n",
      "Iteration: 54250  Loss: 0.44425615668296814  Accuracy: 89%\n",
      "Iteration: 54300  Loss: 0.388252317905426  Accuracy: 89%\n",
      "Iteration: 54350  Loss: 0.34763607382774353  Accuracy: 89%\n",
      "Iteration: 54400  Loss: 0.34568023681640625  Accuracy: 89%\n",
      "Iteration: 54450  Loss: 0.4363771677017212  Accuracy: 89%\n",
      "Iteration: 54500  Loss: 0.45012953877449036  Accuracy: 89%\n",
      "Iteration: 54550  Loss: 0.3230368196964264  Accuracy: 89%\n",
      "Iteration: 54600  Loss: 0.30550819635391235  Accuracy: 89%\n",
      "Iteration: 54650  Loss: 0.4007999897003174  Accuracy: 89%\n",
      "Iteration: 54700  Loss: 0.38428133726119995  Accuracy: 89%\n",
      "Iteration: 54750  Loss: 0.3231520652770996  Accuracy: 89%\n",
      "Iteration: 54800  Loss: 0.3840520977973938  Accuracy: 89%\n",
      "Iteration: 54850  Loss: 0.40837106108665466  Accuracy: 89%\n",
      "Iteration: 54900  Loss: 0.451160192489624  Accuracy: 89%\n",
      "Iteration: 54950  Loss: 0.37462928891181946  Accuracy: 89%\n",
      "Iteration: 55000  Loss: 0.3662481904029846  Accuracy: 89%\n",
      "Iteration: 55050  Loss: 0.42205387353897095  Accuracy: 89%\n",
      "Iteration: 55100  Loss: 0.40503841638565063  Accuracy: 89%\n",
      "Iteration: 55150  Loss: 0.36509841680526733  Accuracy: 89%\n",
      "Iteration: 55200  Loss: 0.3642420470714569  Accuracy: 89%\n",
      "Iteration: 55250  Loss: 0.45847177505493164  Accuracy: 89%\n",
      "Iteration: 55300  Loss: 0.45409587025642395  Accuracy: 89%\n",
      "Iteration: 55350  Loss: 0.33981063961982727  Accuracy: 89%\n",
      "Iteration: 55400  Loss: 0.3232002258300781  Accuracy: 89%\n",
      "Iteration: 55450  Loss: 0.3456476330757141  Accuracy: 89%\n",
      "Iteration: 55500  Loss: 0.3475872278213501  Accuracy: 89%\n",
      "Iteration: 55550  Loss: 0.36345532536506653  Accuracy: 89%\n",
      "Iteration: 55600  Loss: 0.3547367751598358  Accuracy: 89%\n",
      "Iteration: 55650  Loss: 0.32732975482940674  Accuracy: 89%\n",
      "Iteration: 55700  Loss: 0.4503032863140106  Accuracy: 89%\n",
      "Iteration: 55750  Loss: 0.45121681690216064  Accuracy: 89%\n",
      "Iteration: 55800  Loss: 0.4261663258075714  Accuracy: 89%\n",
      "Iteration: 55850  Loss: 0.40474966168403625  Accuracy: 89%\n",
      "Iteration: 55900  Loss: 0.40656232833862305  Accuracy: 89%\n",
      "Iteration: 55950  Loss: 0.3903534710407257  Accuracy: 89%\n",
      "Iteration: 56000  Loss: 0.3778955042362213  Accuracy: 89%\n",
      "Iteration: 56050  Loss: 0.3559926152229309  Accuracy: 89%\n",
      "Iteration: 56100  Loss: 0.33007243275642395  Accuracy: 89%\n",
      "Iteration: 56150  Loss: 0.4157227873802185  Accuracy: 89%\n",
      "Iteration: 56200  Loss: 0.32068970799446106  Accuracy: 89%\n",
      "Iteration: 56250  Loss: 0.39733171463012695  Accuracy: 89%\n",
      "Iteration: 56300  Loss: 0.4038923382759094  Accuracy: 89%\n",
      "Iteration: 56350  Loss: 0.37121954560279846  Accuracy: 89%\n",
      "Iteration: 56400  Loss: 0.40363046526908875  Accuracy: 89%\n",
      "Iteration: 56450  Loss: 0.4479743242263794  Accuracy: 89%\n",
      "Iteration: 56500  Loss: 0.4137403964996338  Accuracy: 89%\n",
      "Iteration: 56550  Loss: 0.38866305351257324  Accuracy: 89%\n",
      "Iteration: 56600  Loss: 0.29422444105148315  Accuracy: 89%\n",
      "Iteration: 56650  Loss: 0.3361084461212158  Accuracy: 89%\n",
      "Iteration: 56700  Loss: 0.339061439037323  Accuracy: 89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 56750  Loss: 0.3441919684410095  Accuracy: 89%\n",
      "Iteration: 56800  Loss: 0.45754390954971313  Accuracy: 89%\n",
      "Iteration: 56850  Loss: 0.3358529806137085  Accuracy: 89%\n",
      "Iteration: 56900  Loss: 0.35208314657211304  Accuracy: 89%\n",
      "Iteration: 56950  Loss: 0.4192185699939728  Accuracy: 89%\n",
      "Iteration: 57000  Loss: 0.4653468728065491  Accuracy: 89%\n",
      "Iteration: 57050  Loss: 0.3526209592819214  Accuracy: 89%\n",
      "Iteration: 57100  Loss: 0.3657154440879822  Accuracy: 89%\n",
      "Iteration: 57150  Loss: 0.3909849524497986  Accuracy: 89%\n",
      "Iteration: 57200  Loss: 0.4250829517841339  Accuracy: 89%\n",
      "Iteration: 57250  Loss: 0.4207861125469208  Accuracy: 89%\n",
      "Iteration: 57300  Loss: 0.41499394178390503  Accuracy: 89%\n",
      "Iteration: 57350  Loss: 0.45969435572624207  Accuracy: 89%\n",
      "Iteration: 57400  Loss: 0.39335504174232483  Accuracy: 89%\n",
      "Iteration: 57450  Loss: 0.4427497684955597  Accuracy: 89%\n",
      "Iteration: 57500  Loss: 0.37929201126098633  Accuracy: 89%\n",
      "Iteration: 57550  Loss: 0.4230189323425293  Accuracy: 89%\n",
      "Iteration: 57600  Loss: 0.4112832248210907  Accuracy: 89%\n",
      "Iteration: 57650  Loss: 0.35811352729797363  Accuracy: 89%\n",
      "Iteration: 57700  Loss: 0.34415921568870544  Accuracy: 89%\n",
      "Iteration: 57750  Loss: 0.3807332217693329  Accuracy: 89%\n",
      "Iteration: 57800  Loss: 0.3491785526275635  Accuracy: 89%\n",
      "Iteration: 57850  Loss: 0.420783668756485  Accuracy: 89%\n",
      "Iteration: 57900  Loss: 0.3742579519748688  Accuracy: 89%\n",
      "Iteration: 57950  Loss: 0.4049883484840393  Accuracy: 89%\n",
      "Iteration: 58000  Loss: 0.3847065269947052  Accuracy: 89%\n",
      "Iteration: 58050  Loss: 0.32056480646133423  Accuracy: 89%\n",
      "Iteration: 58100  Loss: 0.40014585852622986  Accuracy: 89%\n",
      "Iteration: 58150  Loss: 0.3822585642337799  Accuracy: 89%\n",
      "Iteration: 58200  Loss: 0.33743715286254883  Accuracy: 89%\n",
      "Iteration: 58250  Loss: 0.37428033351898193  Accuracy: 89%\n",
      "Iteration: 58300  Loss: 0.35186967253685  Accuracy: 89%\n",
      "Iteration: 58350  Loss: 0.3923005759716034  Accuracy: 89%\n",
      "Iteration: 58400  Loss: 0.3909657597541809  Accuracy: 89%\n",
      "Iteration: 58450  Loss: 0.3494836688041687  Accuracy: 89%\n",
      "Iteration: 58500  Loss: 0.3712795674800873  Accuracy: 89%\n",
      "Iteration: 58550  Loss: 0.37159428000450134  Accuracy: 89%\n",
      "Iteration: 58600  Loss: 0.35840997099876404  Accuracy: 89%\n",
      "Iteration: 58650  Loss: 0.2982335686683655  Accuracy: 89%\n",
      "Iteration: 58700  Loss: 0.32057884335517883  Accuracy: 89%\n",
      "Iteration: 58750  Loss: 0.37157994508743286  Accuracy: 89%\n",
      "Iteration: 58800  Loss: 0.3185426890850067  Accuracy: 89%\n",
      "Iteration: 58850  Loss: 0.3897944688796997  Accuracy: 89%\n",
      "Iteration: 58900  Loss: 0.4091237783432007  Accuracy: 89%\n",
      "Iteration: 58950  Loss: 0.29171714186668396  Accuracy: 89%\n",
      "Iteration: 59000  Loss: 0.35895591974258423  Accuracy: 89%\n",
      "Iteration: 59050  Loss: 0.3327554762363434  Accuracy: 89%\n",
      "Iteration: 59100  Loss: 0.36753514409065247  Accuracy: 89%\n",
      "Iteration: 59150  Loss: 0.3365204334259033  Accuracy: 89%\n",
      "Iteration: 59200  Loss: 0.42469871044158936  Accuracy: 89%\n",
      "Iteration: 59250  Loss: 0.3096141517162323  Accuracy: 89%\n",
      "Iteration: 59300  Loss: 0.4361986815929413  Accuracy: 89%\n",
      "Iteration: 59350  Loss: 0.4028574228286743  Accuracy: 89%\n",
      "Iteration: 59400  Loss: 0.18658310174942017  Accuracy: 89%\n",
      "Iteration: 59450  Loss: 0.37734454870224  Accuracy: 89%\n",
      "Iteration: 59500  Loss: 0.4375108778476715  Accuracy: 89%\n",
      "Iteration: 59550  Loss: 0.44619351625442505  Accuracy: 89%\n",
      "Iteration: 59600  Loss: 0.3605331480503082  Accuracy: 89%\n",
      "Iteration: 59650  Loss: 0.3412623703479767  Accuracy: 89%\n",
      "Iteration: 59700  Loss: 0.42819181084632874  Accuracy: 89%\n",
      "Iteration: 59750  Loss: 0.4136492609977722  Accuracy: 89%\n",
      "Iteration: 59800  Loss: 0.31132957339286804  Accuracy: 89%\n",
      "Iteration: 59850  Loss: 0.34715166687965393  Accuracy: 89%\n",
      "Iteration: 59900  Loss: 0.3811117708683014  Accuracy: 89%\n",
      "Iteration: 59950  Loss: 0.4156293570995331  Accuracy: 89%\n",
      "Iteration: 60000  Loss: 0.40266889333724976  Accuracy: 89%\n",
      "Iteration: 60050  Loss: 0.3644082546234131  Accuracy: 89%\n",
      "Iteration: 60100  Loss: 0.36634430289268494  Accuracy: 89%\n",
      "Iteration: 60150  Loss: 0.33263468742370605  Accuracy: 89%\n",
      "Iteration: 60200  Loss: 0.3376292884349823  Accuracy: 89%\n",
      "Iteration: 60250  Loss: 0.38059672713279724  Accuracy: 89%\n",
      "Iteration: 60300  Loss: 0.28480109572410583  Accuracy: 89%\n",
      "Iteration: 60350  Loss: 0.31757596135139465  Accuracy: 89%\n",
      "Iteration: 60400  Loss: 0.4048800766468048  Accuracy: 89%\n",
      "Iteration: 60450  Loss: 0.44627052545547485  Accuracy: 89%\n",
      "Iteration: 60500  Loss: 0.45236289501190186  Accuracy: 89%\n",
      "Iteration: 60550  Loss: 0.3155261278152466  Accuracy: 89%\n",
      "Iteration: 60600  Loss: 0.3858054578304291  Accuracy: 89%\n",
      "Iteration: 60650  Loss: 0.3390965759754181  Accuracy: 89%\n",
      "Iteration: 60700  Loss: 0.41660743951797485  Accuracy: 89%\n",
      "Iteration: 60750  Loss: 0.359895259141922  Accuracy: 89%\n",
      "Iteration: 60800  Loss: 0.39944663643836975  Accuracy: 89%\n",
      "Iteration: 60850  Loss: 0.39285096526145935  Accuracy: 89%\n",
      "Iteration: 60900  Loss: 0.46885791420936584  Accuracy: 89%\n",
      "Iteration: 60950  Loss: 0.3826463222503662  Accuracy: 89%\n",
      "Iteration: 61000  Loss: 0.3448059856891632  Accuracy: 89%\n",
      "Iteration: 61050  Loss: 0.3814111053943634  Accuracy: 89%\n",
      "Iteration: 61100  Loss: 0.42184293270111084  Accuracy: 89%\n",
      "Iteration: 61150  Loss: 0.3187047839164734  Accuracy: 89%\n",
      "Iteration: 61200  Loss: 0.3992021381855011  Accuracy: 89%\n",
      "Iteration: 61250  Loss: 0.4038299322128296  Accuracy: 89%\n",
      "Iteration: 61300  Loss: 0.33462709188461304  Accuracy: 89%\n",
      "Iteration: 61350  Loss: 0.38540083169937134  Accuracy: 89%\n",
      "Iteration: 61400  Loss: 0.3656933903694153  Accuracy: 89%\n",
      "Iteration: 61450  Loss: 0.3726387917995453  Accuracy: 89%\n",
      "Iteration: 61500  Loss: 0.34017661213874817  Accuracy: 89%\n",
      "Iteration: 61550  Loss: 0.3068068325519562  Accuracy: 89%\n",
      "Iteration: 61600  Loss: 0.44936731457710266  Accuracy: 90%\n",
      "Iteration: 61650  Loss: 0.4221227765083313  Accuracy: 89%\n",
      "Iteration: 61700  Loss: 0.34522172808647156  Accuracy: 89%\n",
      "Iteration: 61750  Loss: 0.4330679774284363  Accuracy: 89%\n",
      "Iteration: 61800  Loss: 0.3626999855041504  Accuracy: 89%\n",
      "Iteration: 61850  Loss: 0.3855946958065033  Accuracy: 89%\n",
      "Iteration: 61900  Loss: 0.4021521806716919  Accuracy: 89%\n",
      "Iteration: 61950  Loss: 0.4503679573535919  Accuracy: 89%\n",
      "Iteration: 62000  Loss: 0.26341551542282104  Accuracy: 89%\n",
      "Iteration: 62050  Loss: 0.3211751878261566  Accuracy: 89%\n",
      "Iteration: 62100  Loss: 0.4253349006175995  Accuracy: 89%\n",
      "Iteration: 62150  Loss: 0.35937362909317017  Accuracy: 89%\n",
      "Iteration: 62200  Loss: 0.3549315929412842  Accuracy: 89%\n",
      "Iteration: 62250  Loss: 0.4201716482639313  Accuracy: 89%\n",
      "Iteration: 62300  Loss: 0.3782733976840973  Accuracy: 89%\n",
      "Iteration: 62350  Loss: 0.3856205344200134  Accuracy: 89%\n",
      "Iteration: 62400  Loss: 0.41407909989356995  Accuracy: 89%\n",
      "Iteration: 62450  Loss: 0.3824958801269531  Accuracy: 89%\n",
      "Iteration: 62500  Loss: 0.3858352601528168  Accuracy: 89%\n",
      "Iteration: 62550  Loss: 0.3043287992477417  Accuracy: 89%\n",
      "Iteration: 62600  Loss: 0.3691776394844055  Accuracy: 89%\n",
      "Iteration: 62650  Loss: 0.3430384397506714  Accuracy: 89%\n",
      "Iteration: 62700  Loss: 0.3644914925098419  Accuracy: 89%\n",
      "Iteration: 62750  Loss: 0.3882322907447815  Accuracy: 89%\n",
      "Iteration: 62800  Loss: 0.39410534501075745  Accuracy: 89%\n",
      "Iteration: 62850  Loss: 0.4151107668876648  Accuracy: 89%\n",
      "Iteration: 62900  Loss: 0.34190458059310913  Accuracy: 89%\n",
      "Iteration: 62950  Loss: 0.2806406021118164  Accuracy: 89%\n",
      "Iteration: 63000  Loss: 0.3805031180381775  Accuracy: 90%\n",
      "Iteration: 63050  Loss: 0.42968660593032837  Accuracy: 90%\n",
      "Iteration: 63100  Loss: 0.46982356905937195  Accuracy: 89%\n",
      "Iteration: 63150  Loss: 0.39950770139694214  Accuracy: 89%\n",
      "Iteration: 63200  Loss: 0.3836159408092499  Accuracy: 89%\n",
      "Iteration: 63250  Loss: 0.36070552468299866  Accuracy: 89%\n",
      "Iteration: 63300  Loss: 0.3425341546535492  Accuracy: 89%\n",
      "Iteration: 63350  Loss: 0.3555252254009247  Accuracy: 89%\n",
      "Iteration: 63400  Loss: 0.3516092002391815  Accuracy: 89%\n",
      "Iteration: 63450  Loss: 0.4238113462924957  Accuracy: 89%\n",
      "Iteration: 63500  Loss: 0.42542845010757446  Accuracy: 89%\n",
      "Iteration: 63550  Loss: 0.3342451751232147  Accuracy: 89%\n",
      "Iteration: 63600  Loss: 0.45367231965065  Accuracy: 89%\n",
      "Iteration: 63650  Loss: 0.3555583655834198  Accuracy: 90%\n",
      "Iteration: 63700  Loss: 0.35645270347595215  Accuracy: 89%\n",
      "Iteration: 63750  Loss: 0.31041550636291504  Accuracy: 89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 63800  Loss: 0.3666175603866577  Accuracy: 89%\n",
      "Iteration: 63850  Loss: 0.40920475125312805  Accuracy: 89%\n",
      "Iteration: 63900  Loss: 0.3442157208919525  Accuracy: 89%\n",
      "Iteration: 63950  Loss: 0.45308586955070496  Accuracy: 89%\n",
      "Iteration: 64000  Loss: 0.3330625295639038  Accuracy: 89%\n",
      "Iteration: 64050  Loss: 0.34894677996635437  Accuracy: 89%\n",
      "Iteration: 64100  Loss: 0.3491891324520111  Accuracy: 89%\n",
      "Iteration: 64150  Loss: 0.3003422021865845  Accuracy: 89%\n",
      "Iteration: 64200  Loss: 0.28304508328437805  Accuracy: 89%\n",
      "Iteration: 64250  Loss: 0.32269129157066345  Accuracy: 89%\n",
      "Iteration: 64300  Loss: 0.37804627418518066  Accuracy: 89%\n",
      "Iteration: 64350  Loss: 0.43041563034057617  Accuracy: 89%\n",
      "Iteration: 64400  Loss: 0.39621442556381226  Accuracy: 89%\n",
      "Iteration: 64450  Loss: 0.39800533652305603  Accuracy: 89%\n",
      "Iteration: 64500  Loss: 0.3650057315826416  Accuracy: 89%\n",
      "Iteration: 64550  Loss: 0.4080163836479187  Accuracy: 89%\n",
      "Iteration: 64600  Loss: 0.4041079878807068  Accuracy: 89%\n",
      "Iteration: 64650  Loss: 0.4176509976387024  Accuracy: 89%\n",
      "Iteration: 64700  Loss: 0.41197600960731506  Accuracy: 89%\n",
      "Iteration: 64750  Loss: 0.3689928948879242  Accuracy: 89%\n",
      "Iteration: 64800  Loss: 0.38386499881744385  Accuracy: 89%\n",
      "Iteration: 64850  Loss: 0.4113653302192688  Accuracy: 89%\n",
      "Iteration: 64900  Loss: 0.32153478264808655  Accuracy: 89%\n",
      "Iteration: 64950  Loss: 0.3692586421966553  Accuracy: 89%\n",
      "Iteration: 65000  Loss: 0.31874701380729675  Accuracy: 90%\n",
      "Iteration: 65050  Loss: 0.3583388030529022  Accuracy: 90%\n",
      "Iteration: 65100  Loss: 0.3902956545352936  Accuracy: 90%\n",
      "Iteration: 65150  Loss: 0.3377479612827301  Accuracy: 90%\n",
      "Iteration: 65200  Loss: 0.42333924770355225  Accuracy: 90%\n",
      "Iteration: 65250  Loss: 0.36299553513526917  Accuracy: 90%\n",
      "Iteration: 65300  Loss: 0.3244343400001526  Accuracy: 90%\n",
      "Iteration: 65350  Loss: 0.3526325821876526  Accuracy: 90%\n",
      "Iteration: 65400  Loss: 0.41396671533584595  Accuracy: 90%\n",
      "Iteration: 65450  Loss: 0.42079901695251465  Accuracy: 90%\n",
      "Iteration: 65500  Loss: 0.38409900665283203  Accuracy: 90%\n",
      "Iteration: 65550  Loss: 0.3585992455482483  Accuracy: 90%\n",
      "Iteration: 65600  Loss: 0.34354695677757263  Accuracy: 90%\n",
      "Iteration: 65650  Loss: 0.39342838525772095  Accuracy: 90%\n",
      "Iteration: 65700  Loss: 0.3717714846134186  Accuracy: 90%\n",
      "Iteration: 65750  Loss: 0.5726454257965088  Accuracy: 90%\n",
      "Iteration: 65800  Loss: 0.3387524485588074  Accuracy: 90%\n",
      "Iteration: 65850  Loss: 0.4143115282058716  Accuracy: 90%\n",
      "Iteration: 65900  Loss: 0.3748604655265808  Accuracy: 90%\n",
      "Iteration: 65950  Loss: 0.4401189386844635  Accuracy: 90%\n",
      "Iteration: 66000  Loss: 0.29775404930114746  Accuracy: 90%\n",
      "Iteration: 66050  Loss: 0.4000032842159271  Accuracy: 90%\n",
      "Iteration: 66100  Loss: 0.3781314194202423  Accuracy: 90%\n",
      "Iteration: 66150  Loss: 0.3199785649776459  Accuracy: 90%\n",
      "Iteration: 66200  Loss: 0.368685781955719  Accuracy: 90%\n",
      "Iteration: 66250  Loss: 0.36441677808761597  Accuracy: 90%\n",
      "Iteration: 66300  Loss: 0.29284608364105225  Accuracy: 90%\n",
      "Iteration: 66350  Loss: 0.3978005051612854  Accuracy: 90%\n",
      "Iteration: 66400  Loss: 0.38720861077308655  Accuracy: 90%\n",
      "Iteration: 66450  Loss: 0.3187522888183594  Accuracy: 90%\n",
      "Iteration: 66500  Loss: 0.35682034492492676  Accuracy: 90%\n",
      "Iteration: 66550  Loss: 0.35523098707199097  Accuracy: 90%\n",
      "Iteration: 66600  Loss: 0.44766101241111755  Accuracy: 90%\n",
      "Iteration: 66650  Loss: 0.36745256185531616  Accuracy: 90%\n",
      "Iteration: 66700  Loss: 0.41941988468170166  Accuracy: 90%\n",
      "Iteration: 66750  Loss: 0.3580280840396881  Accuracy: 90%\n",
      "Iteration: 66800  Loss: 0.5313063859939575  Accuracy: 90%\n",
      "Iteration: 66850  Loss: 0.381119966506958  Accuracy: 90%\n",
      "Iteration: 66900  Loss: 0.3329907953739166  Accuracy: 90%\n",
      "Iteration: 66950  Loss: 0.37529459595680237  Accuracy: 90%\n",
      "Iteration: 67000  Loss: 0.30666449666023254  Accuracy: 90%\n",
      "Iteration: 67050  Loss: 0.3203134834766388  Accuracy: 90%\n",
      "Iteration: 67100  Loss: 0.34466397762298584  Accuracy: 90%\n",
      "Iteration: 67150  Loss: 0.4393276870250702  Accuracy: 90%\n",
      "Iteration: 67200  Loss: 0.4178006052970886  Accuracy: 90%\n",
      "Iteration: 67250  Loss: 0.3362179100513458  Accuracy: 90%\n",
      "Iteration: 67300  Loss: 0.3992581069469452  Accuracy: 90%\n",
      "Iteration: 67350  Loss: 0.3993731737136841  Accuracy: 90%\n",
      "Iteration: 67400  Loss: 0.3688214421272278  Accuracy: 90%\n",
      "Iteration: 67450  Loss: 0.39652225375175476  Accuracy: 90%\n",
      "Iteration: 67500  Loss: 0.4706353545188904  Accuracy: 90%\n",
      "Iteration: 67550  Loss: 0.4482070505619049  Accuracy: 90%\n",
      "Iteration: 67600  Loss: 0.3803243339061737  Accuracy: 90%\n",
      "Iteration: 67650  Loss: 0.45751670002937317  Accuracy: 90%\n",
      "Iteration: 67700  Loss: 0.39594659209251404  Accuracy: 90%\n",
      "Iteration: 67750  Loss: 0.29380425810813904  Accuracy: 90%\n",
      "Iteration: 67800  Loss: 0.4245540499687195  Accuracy: 90%\n",
      "Iteration: 67850  Loss: 0.2916576862335205  Accuracy: 90%\n",
      "Iteration: 67900  Loss: 0.32144224643707275  Accuracy: 90%\n",
      "Iteration: 67950  Loss: 0.33955004811286926  Accuracy: 90%\n",
      "Iteration: 68000  Loss: 0.3444210886955261  Accuracy: 90%\n",
      "Iteration: 68050  Loss: 0.4280954897403717  Accuracy: 90%\n",
      "Iteration: 68100  Loss: 0.3708976209163666  Accuracy: 90%\n",
      "Iteration: 68150  Loss: 0.31294605135917664  Accuracy: 90%\n",
      "Iteration: 68200  Loss: 0.32970279455184937  Accuracy: 90%\n",
      "Iteration: 68250  Loss: 0.37046369910240173  Accuracy: 90%\n",
      "Iteration: 68300  Loss: 0.349589467048645  Accuracy: 90%\n",
      "Iteration: 68350  Loss: 0.3735019862651825  Accuracy: 90%\n",
      "Iteration: 68400  Loss: 0.38875484466552734  Accuracy: 90%\n",
      "Iteration: 68450  Loss: 0.42751434445381165  Accuracy: 90%\n",
      "Iteration: 68500  Loss: 0.3713052570819855  Accuracy: 90%\n",
      "Iteration: 68550  Loss: 0.3509525656700134  Accuracy: 90%\n",
      "Iteration: 68600  Loss: 0.28833118081092834  Accuracy: 90%\n",
      "Iteration: 68650  Loss: 0.2676810920238495  Accuracy: 90%\n",
      "Iteration: 68700  Loss: 0.35686060786247253  Accuracy: 90%\n",
      "Iteration: 68750  Loss: 0.38321325182914734  Accuracy: 90%\n",
      "Iteration: 68800  Loss: 0.35530033707618713  Accuracy: 90%\n",
      "Iteration: 68850  Loss: 0.41642457246780396  Accuracy: 90%\n",
      "Iteration: 68900  Loss: 0.28276973962783813  Accuracy: 90%\n",
      "Iteration: 68950  Loss: 0.38651564717292786  Accuracy: 90%\n",
      "Iteration: 69000  Loss: 0.3698466122150421  Accuracy: 90%\n",
      "Iteration: 69050  Loss: 0.3440530300140381  Accuracy: 90%\n",
      "Iteration: 69100  Loss: 0.38337984681129456  Accuracy: 90%\n",
      "Iteration: 69150  Loss: 0.298368364572525  Accuracy: 90%\n",
      "Iteration: 69200  Loss: 0.42470479011535645  Accuracy: 90%\n",
      "Iteration: 69250  Loss: 0.31945154070854187  Accuracy: 90%\n",
      "Iteration: 69300  Loss: 0.22664153575897217  Accuracy: 90%\n",
      "Iteration: 69350  Loss: 0.3335082232952118  Accuracy: 90%\n",
      "Iteration: 69400  Loss: 0.32764506340026855  Accuracy: 90%\n",
      "Iteration: 69450  Loss: 0.36551931500434875  Accuracy: 90%\n",
      "Iteration: 69500  Loss: 0.312137246131897  Accuracy: 90%\n",
      "Iteration: 69550  Loss: 0.3186851441860199  Accuracy: 90%\n",
      "Iteration: 69600  Loss: 0.4559516906738281  Accuracy: 90%\n",
      "Iteration: 69650  Loss: 0.4108625650405884  Accuracy: 90%\n",
      "Iteration: 69700  Loss: 0.4417848587036133  Accuracy: 90%\n",
      "Iteration: 69750  Loss: 0.30860114097595215  Accuracy: 90%\n",
      "Iteration: 69800  Loss: 0.40588894486427307  Accuracy: 90%\n",
      "Iteration: 69850  Loss: 0.39689382910728455  Accuracy: 90%\n",
      "Iteration: 69900  Loss: 0.35582736134529114  Accuracy: 90%\n",
      "Iteration: 69950  Loss: 0.36739444732666016  Accuracy: 90%\n",
      "Iteration: 70000  Loss: 0.3504274785518646  Accuracy: 90%\n",
      "Iteration: 70050  Loss: 0.4800083339214325  Accuracy: 90%\n",
      "Iteration: 70100  Loss: 0.30818256735801697  Accuracy: 90%\n",
      "Iteration: 70150  Loss: 0.3260437846183777  Accuracy: 90%\n",
      "Iteration: 70200  Loss: 0.296856552362442  Accuracy: 90%\n",
      "Iteration: 70250  Loss: 0.4645855128765106  Accuracy: 90%\n",
      "Iteration: 70300  Loss: 0.3000050187110901  Accuracy: 90%\n",
      "Iteration: 70350  Loss: 0.3763080835342407  Accuracy: 90%\n",
      "Iteration: 70400  Loss: 0.2933763265609741  Accuracy: 90%\n",
      "Iteration: 70450  Loss: 0.37548235058784485  Accuracy: 90%\n",
      "Iteration: 70500  Loss: 0.34523701667785645  Accuracy: 90%\n",
      "Iteration: 70550  Loss: 0.364685595035553  Accuracy: 90%\n",
      "Iteration: 70600  Loss: 0.3434417247772217  Accuracy: 90%\n",
      "Iteration: 70650  Loss: 0.3745885193347931  Accuracy: 90%\n",
      "Iteration: 70700  Loss: 0.3420724868774414  Accuracy: 90%\n",
      "Iteration: 70750  Loss: 0.4135821759700775  Accuracy: 90%\n",
      "Iteration: 70800  Loss: 0.2700745463371277  Accuracy: 90%\n",
      "Iteration: 70850  Loss: 0.47310373187065125  Accuracy: 90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 70900  Loss: 0.4362812042236328  Accuracy: 90%\n",
      "Iteration: 70950  Loss: 0.3623337149620056  Accuracy: 90%\n",
      "Iteration: 71000  Loss: 0.3541599214076996  Accuracy: 90%\n",
      "Iteration: 71050  Loss: 0.26971444487571716  Accuracy: 90%\n",
      "Iteration: 71100  Loss: 0.34196823835372925  Accuracy: 90%\n",
      "Iteration: 71150  Loss: 0.38092875480651855  Accuracy: 90%\n",
      "Iteration: 71200  Loss: 0.37285515666007996  Accuracy: 90%\n",
      "Iteration: 71250  Loss: 0.4450341463088989  Accuracy: 90%\n",
      "Iteration: 71300  Loss: 0.3137570023536682  Accuracy: 90%\n",
      "Iteration: 71350  Loss: 0.329904168844223  Accuracy: 90%\n",
      "Iteration: 71400  Loss: 0.3692610263824463  Accuracy: 90%\n",
      "Iteration: 71450  Loss: 0.40397605299949646  Accuracy: 90%\n",
      "Iteration: 71500  Loss: 0.28721341490745544  Accuracy: 90%\n",
      "Iteration: 71550  Loss: 0.36766865849494934  Accuracy: 90%\n",
      "Iteration: 71600  Loss: 0.34921908378601074  Accuracy: 90%\n",
      "Iteration: 71650  Loss: 0.4427907168865204  Accuracy: 90%\n",
      "Iteration: 71700  Loss: 0.40805408358573914  Accuracy: 90%\n",
      "Iteration: 71750  Loss: 0.4049418270587921  Accuracy: 90%\n",
      "Iteration: 71800  Loss: 0.3710409998893738  Accuracy: 90%\n",
      "Iteration: 71850  Loss: 0.47477179765701294  Accuracy: 90%\n",
      "Iteration: 71900  Loss: 0.37803518772125244  Accuracy: 90%\n",
      "Iteration: 71950  Loss: 0.3344865143299103  Accuracy: 90%\n",
      "Iteration: 72000  Loss: 0.37116238474845886  Accuracy: 90%\n",
      "Iteration: 72050  Loss: 0.4568297863006592  Accuracy: 90%\n",
      "Iteration: 72100  Loss: 0.3250189423561096  Accuracy: 90%\n",
      "Iteration: 72150  Loss: 0.36652079224586487  Accuracy: 90%\n",
      "Iteration: 72200  Loss: 0.30829378962516785  Accuracy: 90%\n",
      "Iteration: 72250  Loss: 0.5444850325584412  Accuracy: 90%\n",
      "Iteration: 72300  Loss: 0.3266697824001312  Accuracy: 90%\n",
      "Iteration: 72350  Loss: 0.3174118101596832  Accuracy: 90%\n",
      "Iteration: 72400  Loss: 0.3497581481933594  Accuracy: 90%\n",
      "Iteration: 72450  Loss: 0.4066102206707001  Accuracy: 90%\n",
      "Iteration: 72500  Loss: 0.3197019100189209  Accuracy: 90%\n",
      "Iteration: 72550  Loss: 0.33614540100097656  Accuracy: 90%\n",
      "Iteration: 72600  Loss: 0.3947967290878296  Accuracy: 90%\n",
      "Iteration: 72650  Loss: 0.39854758977890015  Accuracy: 90%\n",
      "Iteration: 72700  Loss: 0.3396676182746887  Accuracy: 90%\n",
      "Iteration: 72750  Loss: 0.42590975761413574  Accuracy: 90%\n",
      "Iteration: 72800  Loss: 0.33548426628112793  Accuracy: 90%\n",
      "Iteration: 72850  Loss: 0.42633184790611267  Accuracy: 90%\n",
      "Iteration: 72900  Loss: 0.39897528290748596  Accuracy: 90%\n",
      "Iteration: 72950  Loss: 0.3978428840637207  Accuracy: 90%\n",
      "Iteration: 73000  Loss: 0.3637509047985077  Accuracy: 90%\n",
      "Iteration: 73050  Loss: 0.3621251583099365  Accuracy: 90%\n",
      "Iteration: 73100  Loss: 0.3275429606437683  Accuracy: 90%\n",
      "Iteration: 73150  Loss: 0.34634968638420105  Accuracy: 90%\n",
      "Iteration: 73200  Loss: 0.39366570115089417  Accuracy: 90%\n",
      "Iteration: 73250  Loss: 0.39375439286231995  Accuracy: 90%\n",
      "Iteration: 73300  Loss: 0.384202241897583  Accuracy: 90%\n",
      "Iteration: 73350  Loss: 0.409083753824234  Accuracy: 90%\n",
      "Iteration: 73400  Loss: 0.2743057310581207  Accuracy: 90%\n",
      "Iteration: 73450  Loss: 0.2924554646015167  Accuracy: 90%\n",
      "Iteration: 73500  Loss: 0.4381892681121826  Accuracy: 90%\n",
      "Iteration: 73550  Loss: 0.38677746057510376  Accuracy: 90%\n",
      "Iteration: 73600  Loss: 0.3369410037994385  Accuracy: 90%\n",
      "Iteration: 73650  Loss: 0.38332557678222656  Accuracy: 90%\n",
      "Iteration: 73700  Loss: 0.3403897285461426  Accuracy: 90%\n",
      "Iteration: 73750  Loss: 0.39093664288520813  Accuracy: 90%\n",
      "Iteration: 73800  Loss: 0.38841065764427185  Accuracy: 90%\n",
      "Iteration: 73850  Loss: 0.4571426510810852  Accuracy: 90%\n",
      "Iteration: 73900  Loss: 0.2916371524333954  Accuracy: 90%\n",
      "Iteration: 73950  Loss: 0.3879251480102539  Accuracy: 90%\n",
      "Iteration: 74000  Loss: 0.38082435727119446  Accuracy: 90%\n",
      "Iteration: 74050  Loss: 0.35176992416381836  Accuracy: 90%\n",
      "Iteration: 74100  Loss: 0.43540289998054504  Accuracy: 90%\n",
      "Iteration: 74150  Loss: 0.4666004478931427  Accuracy: 90%\n",
      "Iteration: 74200  Loss: 0.3697153329849243  Accuracy: 90%\n",
      "Iteration: 74250  Loss: 0.3452470004558563  Accuracy: 90%\n",
      "Iteration: 74300  Loss: 0.450247585773468  Accuracy: 90%\n",
      "Iteration: 74350  Loss: 0.3466540575027466  Accuracy: 90%\n",
      "Iteration: 74400  Loss: 0.4120141863822937  Accuracy: 90%\n",
      "Iteration: 74450  Loss: 0.4184505343437195  Accuracy: 90%\n",
      "Iteration: 74500  Loss: 0.4737747609615326  Accuracy: 90%\n",
      "Iteration: 74550  Loss: 0.34425804018974304  Accuracy: 90%\n",
      "Iteration: 74600  Loss: 0.3894875645637512  Accuracy: 90%\n",
      "Iteration: 74650  Loss: 0.36597901582717896  Accuracy: 90%\n",
      "Iteration: 74700  Loss: 0.40077751874923706  Accuracy: 90%\n",
      "Iteration: 74750  Loss: 0.3360166549682617  Accuracy: 90%\n",
      "Iteration: 74800  Loss: 0.3045055568218231  Accuracy: 90%\n",
      "Iteration: 74850  Loss: 0.42003992199897766  Accuracy: 90%\n",
      "Iteration: 74900  Loss: 0.33757734298706055  Accuracy: 90%\n",
      "Iteration: 74950  Loss: 0.30157604813575745  Accuracy: 90%\n",
      "Iteration: 75000  Loss: 0.3239847421646118  Accuracy: 90%\n",
      "Iteration: 75050  Loss: 0.33855733275413513  Accuracy: 90%\n",
      "Iteration: 75100  Loss: 0.3763376772403717  Accuracy: 90%\n",
      "Iteration: 75150  Loss: 0.3712058663368225  Accuracy: 90%\n",
      "Iteration: 75200  Loss: 0.2915441393852234  Accuracy: 90%\n",
      "Iteration: 75250  Loss: 0.4133831858634949  Accuracy: 90%\n",
      "Iteration: 75300  Loss: 0.3614645302295685  Accuracy: 90%\n",
      "Iteration: 75350  Loss: 0.35845303535461426  Accuracy: 90%\n",
      "Iteration: 75400  Loss: 0.390577107667923  Accuracy: 90%\n",
      "Iteration: 75450  Loss: 0.36787429451942444  Accuracy: 90%\n",
      "Iteration: 75500  Loss: 0.3867691457271576  Accuracy: 90%\n",
      "Iteration: 75550  Loss: 0.3888702690601349  Accuracy: 90%\n",
      "Iteration: 75600  Loss: 0.43614962697029114  Accuracy: 90%\n",
      "Iteration: 75650  Loss: 0.4249899089336395  Accuracy: 90%\n",
      "Iteration: 75700  Loss: 0.35609671473503113  Accuracy: 90%\n",
      "Iteration: 75750  Loss: 0.30260491371154785  Accuracy: 90%\n",
      "Iteration: 75800  Loss: 0.30721986293792725  Accuracy: 90%\n",
      "Iteration: 75850  Loss: 0.27838295698165894  Accuracy: 90%\n",
      "Iteration: 75900  Loss: 0.30769920349121094  Accuracy: 90%\n",
      "Iteration: 75950  Loss: 0.3020192086696625  Accuracy: 90%\n",
      "Iteration: 76000  Loss: 0.3196144104003906  Accuracy: 90%\n",
      "Iteration: 76050  Loss: 0.47814542055130005  Accuracy: 90%\n",
      "Iteration: 76100  Loss: 0.3930996060371399  Accuracy: 90%\n",
      "Iteration: 76150  Loss: 0.2831290662288666  Accuracy: 90%\n",
      "Iteration: 76200  Loss: 0.28752797842025757  Accuracy: 90%\n",
      "Iteration: 76250  Loss: 0.28716519474983215  Accuracy: 90%\n",
      "Iteration: 76300  Loss: 0.35077160596847534  Accuracy: 90%\n",
      "Iteration: 76350  Loss: 0.3733068108558655  Accuracy: 90%\n",
      "Iteration: 76400  Loss: 0.34748324751853943  Accuracy: 90%\n",
      "Iteration: 76450  Loss: 0.30133771896362305  Accuracy: 90%\n",
      "Iteration: 76500  Loss: 0.2572520971298218  Accuracy: 90%\n",
      "Iteration: 76550  Loss: 0.3288421034812927  Accuracy: 90%\n",
      "Iteration: 76600  Loss: 0.3908618986606598  Accuracy: 90%\n",
      "Iteration: 76650  Loss: 0.42898091673851013  Accuracy: 90%\n",
      "Iteration: 76700  Loss: 0.31238967180252075  Accuracy: 90%\n",
      "Iteration: 76750  Loss: 0.3289896547794342  Accuracy: 90%\n",
      "Iteration: 76800  Loss: 0.3912959396839142  Accuracy: 90%\n",
      "Iteration: 76850  Loss: 0.47755706310272217  Accuracy: 90%\n",
      "Iteration: 76900  Loss: 0.27951616048812866  Accuracy: 90%\n",
      "Iteration: 76950  Loss: 0.3457741141319275  Accuracy: 90%\n",
      "Iteration: 77000  Loss: 0.39996814727783203  Accuracy: 90%\n",
      "Iteration: 77050  Loss: 0.2991419732570648  Accuracy: 90%\n",
      "Iteration: 77100  Loss: 0.26921582221984863  Accuracy: 90%\n",
      "Iteration: 77150  Loss: 0.35012832283973694  Accuracy: 90%\n",
      "Iteration: 77200  Loss: 0.3476026952266693  Accuracy: 90%\n",
      "Iteration: 77250  Loss: 0.3306524455547333  Accuracy: 90%\n",
      "Iteration: 77300  Loss: 0.31341955065727234  Accuracy: 90%\n",
      "Iteration: 77350  Loss: 0.3621058464050293  Accuracy: 90%\n",
      "Iteration: 77400  Loss: 0.36736947298049927  Accuracy: 90%\n",
      "Iteration: 77450  Loss: 0.3698732554912567  Accuracy: 90%\n",
      "Iteration: 77500  Loss: 0.40288448333740234  Accuracy: 90%\n",
      "Iteration: 77550  Loss: 0.3448665142059326  Accuracy: 90%\n",
      "Iteration: 77600  Loss: 0.32857832312583923  Accuracy: 90%\n",
      "Iteration: 77650  Loss: 0.4047093391418457  Accuracy: 90%\n",
      "Iteration: 77700  Loss: 0.3687303066253662  Accuracy: 90%\n",
      "Iteration: 77750  Loss: 0.41821786761283875  Accuracy: 90%\n",
      "Iteration: 77800  Loss: 0.3431294560432434  Accuracy: 90%\n",
      "Iteration: 77850  Loss: 0.37614160776138306  Accuracy: 90%\n",
      "Iteration: 77900  Loss: 0.35220205783843994  Accuracy: 90%\n",
      "Iteration: 77950  Loss: 0.413857102394104  Accuracy: 90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 78000  Loss: 0.3188141882419586  Accuracy: 90%\n",
      "Iteration: 78050  Loss: 0.3932499885559082  Accuracy: 90%\n",
      "Iteration: 78100  Loss: 0.2998169958591461  Accuracy: 90%\n",
      "Iteration: 78150  Loss: 0.33724385499954224  Accuracy: 90%\n",
      "Iteration: 78200  Loss: 0.32202303409576416  Accuracy: 90%\n",
      "Iteration: 78250  Loss: 0.3969856798648834  Accuracy: 90%\n",
      "Iteration: 78300  Loss: 0.3646443784236908  Accuracy: 90%\n",
      "Iteration: 78350  Loss: 0.4085139036178589  Accuracy: 90%\n",
      "Iteration: 78400  Loss: 0.3303925693035126  Accuracy: 90%\n",
      "Iteration: 78450  Loss: 0.40215304493904114  Accuracy: 90%\n",
      "Iteration: 78500  Loss: 0.3185856342315674  Accuracy: 90%\n",
      "Iteration: 78550  Loss: 0.3256218433380127  Accuracy: 90%\n",
      "Iteration: 78600  Loss: 0.3154769837856293  Accuracy: 90%\n",
      "Iteration: 78650  Loss: 0.3151993155479431  Accuracy: 90%\n",
      "Iteration: 78700  Loss: 0.426918089389801  Accuracy: 90%\n",
      "Iteration: 78750  Loss: 0.33604902029037476  Accuracy: 90%\n",
      "Iteration: 78800  Loss: 0.4965944290161133  Accuracy: 90%\n",
      "Iteration: 78850  Loss: 0.3233067989349365  Accuracy: 90%\n",
      "Iteration: 78900  Loss: 0.3243424892425537  Accuracy: 90%\n",
      "Iteration: 78950  Loss: 0.3607257604598999  Accuracy: 90%\n",
      "Iteration: 79000  Loss: 0.3158556818962097  Accuracy: 90%\n",
      "Iteration: 79050  Loss: 0.369804322719574  Accuracy: 90%\n",
      "Iteration: 79100  Loss: 0.37495994567871094  Accuracy: 90%\n",
      "Iteration: 79150  Loss: 0.31189802289009094  Accuracy: 90%\n",
      "Iteration: 79200  Loss: 0.30593520402908325  Accuracy: 90%\n",
      "Iteration: 79250  Loss: 0.36408695578575134  Accuracy: 90%\n",
      "Iteration: 79300  Loss: 0.4004010558128357  Accuracy: 90%\n",
      "Iteration: 79350  Loss: 0.34276074171066284  Accuracy: 90%\n",
      "Iteration: 79400  Loss: 0.3208351731300354  Accuracy: 90%\n",
      "Iteration: 79450  Loss: 0.3334929645061493  Accuracy: 90%\n",
      "Iteration: 79500  Loss: 0.33208298683166504  Accuracy: 90%\n",
      "Iteration: 79550  Loss: 0.2318093180656433  Accuracy: 90%\n",
      "Iteration: 79600  Loss: 0.36477935314178467  Accuracy: 90%\n",
      "Iteration: 79650  Loss: 0.32362446188926697  Accuracy: 90%\n",
      "Iteration: 79700  Loss: 0.3069765865802765  Accuracy: 90%\n",
      "Iteration: 79750  Loss: 0.27676650881767273  Accuracy: 90%\n",
      "Iteration: 79800  Loss: 0.2872121036052704  Accuracy: 90%\n",
      "Iteration: 79850  Loss: 0.3172341585159302  Accuracy: 90%\n",
      "Iteration: 79900  Loss: 0.4012346565723419  Accuracy: 90%\n",
      "Iteration: 79950  Loss: 0.42238348722457886  Accuracy: 90%\n",
      "Iteration: 80000  Loss: 0.3750070333480835  Accuracy: 90%\n",
      "Iteration: 80050  Loss: 0.38732507824897766  Accuracy: 90%\n",
      "Iteration: 80100  Loss: 0.3598482012748718  Accuracy: 90%\n",
      "Iteration: 80150  Loss: 0.2961387634277344  Accuracy: 90%\n",
      "Iteration: 80200  Loss: 0.3145832121372223  Accuracy: 90%\n",
      "Iteration: 80250  Loss: 0.3862007260322571  Accuracy: 90%\n",
      "Iteration: 80300  Loss: 0.38981199264526367  Accuracy: 90%\n",
      "Iteration: 80350  Loss: 0.45121535658836365  Accuracy: 90%\n",
      "Iteration: 80400  Loss: 0.4186972975730896  Accuracy: 90%\n",
      "Iteration: 80450  Loss: 0.3302764892578125  Accuracy: 90%\n",
      "Iteration: 80500  Loss: 0.3870922327041626  Accuracy: 90%\n",
      "Iteration: 80550  Loss: 0.35521209239959717  Accuracy: 90%\n",
      "Iteration: 80600  Loss: 0.3549126088619232  Accuracy: 90%\n",
      "Iteration: 80650  Loss: 0.33351951837539673  Accuracy: 90%\n",
      "Iteration: 80700  Loss: 0.36089271306991577  Accuracy: 90%\n",
      "Iteration: 80750  Loss: 0.3841545879840851  Accuracy: 90%\n",
      "Iteration: 80800  Loss: 0.3110971748828888  Accuracy: 90%\n",
      "Iteration: 80850  Loss: 0.32591453194618225  Accuracy: 90%\n",
      "Iteration: 80900  Loss: 0.2780877351760864  Accuracy: 90%\n",
      "Iteration: 80950  Loss: 0.38650718331336975  Accuracy: 90%\n",
      "Iteration: 81000  Loss: 0.32960715889930725  Accuracy: 90%\n",
      "Iteration: 81050  Loss: 0.42739245295524597  Accuracy: 90%\n",
      "Iteration: 81100  Loss: 0.40705016255378723  Accuracy: 90%\n",
      "Iteration: 81150  Loss: 0.329894483089447  Accuracy: 90%\n",
      "Iteration: 81200  Loss: 0.33862048387527466  Accuracy: 90%\n",
      "Iteration: 81250  Loss: 0.3295053541660309  Accuracy: 90%\n",
      "Iteration: 81300  Loss: 0.33609187602996826  Accuracy: 90%\n",
      "Iteration: 81350  Loss: 0.43969613313674927  Accuracy: 90%\n",
      "Iteration: 81400  Loss: 0.3915020227432251  Accuracy: 90%\n",
      "Iteration: 81450  Loss: 0.30049264430999756  Accuracy: 90%\n",
      "Iteration: 81500  Loss: 0.31124231219291687  Accuracy: 90%\n",
      "Iteration: 81550  Loss: 0.36211898922920227  Accuracy: 90%\n",
      "Iteration: 81600  Loss: 0.3437166213989258  Accuracy: 90%\n",
      "Iteration: 81650  Loss: 0.31984567642211914  Accuracy: 90%\n",
      "Iteration: 81700  Loss: 0.3309890627861023  Accuracy: 90%\n",
      "Iteration: 81750  Loss: 0.34113818407058716  Accuracy: 90%\n",
      "Iteration: 81800  Loss: 0.3335005044937134  Accuracy: 90%\n",
      "Iteration: 81850  Loss: 0.34370943903923035  Accuracy: 90%\n",
      "Iteration: 81900  Loss: 0.4221751093864441  Accuracy: 90%\n",
      "Iteration: 81950  Loss: 0.3487067222595215  Accuracy: 90%\n",
      "Iteration: 82000  Loss: 0.4172324538230896  Accuracy: 90%\n",
      "Iteration: 82050  Loss: 0.3654381334781647  Accuracy: 90%\n",
      "Iteration: 82100  Loss: 0.32754549384117126  Accuracy: 90%\n",
      "Iteration: 82150  Loss: 0.3436696529388428  Accuracy: 90%\n",
      "Iteration: 82200  Loss: 0.32231274247169495  Accuracy: 90%\n",
      "Iteration: 82250  Loss: 0.2892196476459503  Accuracy: 90%\n",
      "Iteration: 82300  Loss: 0.3548305034637451  Accuracy: 90%\n",
      "Iteration: 82350  Loss: 0.32698655128479004  Accuracy: 90%\n",
      "Iteration: 82400  Loss: 0.4070017337799072  Accuracy: 90%\n",
      "Iteration: 82450  Loss: 0.4424591660499573  Accuracy: 90%\n",
      "Iteration: 82500  Loss: 0.30628278851509094  Accuracy: 90%\n",
      "Iteration: 82550  Loss: 0.36035865545272827  Accuracy: 90%\n",
      "Iteration: 82600  Loss: 0.317730188369751  Accuracy: 90%\n",
      "Iteration: 82650  Loss: 0.3754345774650574  Accuracy: 90%\n",
      "Iteration: 82700  Loss: 0.43150240182876587  Accuracy: 90%\n",
      "Iteration: 82750  Loss: 0.3860165476799011  Accuracy: 90%\n",
      "Iteration: 82800  Loss: 0.36003467440605164  Accuracy: 90%\n",
      "Iteration: 82850  Loss: 0.2725696861743927  Accuracy: 90%\n",
      "Iteration: 82900  Loss: 0.3458019495010376  Accuracy: 90%\n",
      "Iteration: 82950  Loss: 0.3132075369358063  Accuracy: 90%\n",
      "Iteration: 83000  Loss: 0.29917457699775696  Accuracy: 90%\n",
      "Iteration: 83050  Loss: 0.4203834533691406  Accuracy: 90%\n",
      "Iteration: 83100  Loss: 0.3324264585971832  Accuracy: 90%\n",
      "Iteration: 83150  Loss: 0.3374873399734497  Accuracy: 90%\n",
      "Iteration: 83200  Loss: 0.3651049733161926  Accuracy: 90%\n",
      "Iteration: 83250  Loss: 0.3248780071735382  Accuracy: 90%\n",
      "Iteration: 83300  Loss: 0.33755382895469666  Accuracy: 90%\n",
      "Iteration: 83350  Loss: 0.2759542167186737  Accuracy: 90%\n",
      "Iteration: 83400  Loss: 0.39589184522628784  Accuracy: 90%\n",
      "Iteration: 83450  Loss: 0.2854762375354767  Accuracy: 90%\n",
      "Iteration: 83500  Loss: 0.31226813793182373  Accuracy: 90%\n",
      "Iteration: 83550  Loss: 0.30176854133605957  Accuracy: 90%\n",
      "Iteration: 83600  Loss: 0.3302294611930847  Accuracy: 90%\n",
      "Iteration: 83650  Loss: 0.4641895592212677  Accuracy: 90%\n",
      "Iteration: 83700  Loss: 0.35641899704933167  Accuracy: 90%\n",
      "Iteration: 83750  Loss: 0.41872429847717285  Accuracy: 90%\n",
      "Iteration: 83800  Loss: 0.2986478805541992  Accuracy: 90%\n",
      "Iteration: 83850  Loss: 0.33073675632476807  Accuracy: 90%\n",
      "Iteration: 83900  Loss: 0.36144235730171204  Accuracy: 90%\n",
      "Iteration: 83950  Loss: 0.30350586771965027  Accuracy: 90%\n",
      "Iteration: 84000  Loss: 0.3761994540691376  Accuracy: 90%\n",
      "Iteration: 84050  Loss: 0.27631399035453796  Accuracy: 90%\n",
      "Iteration: 84100  Loss: 0.36435022950172424  Accuracy: 90%\n",
      "Iteration: 84150  Loss: 0.33650144934654236  Accuracy: 90%\n",
      "Iteration: 84200  Loss: 0.3001045882701874  Accuracy: 90%\n",
      "Iteration: 84250  Loss: 0.35471418499946594  Accuracy: 90%\n",
      "Iteration: 84300  Loss: 0.412112295627594  Accuracy: 90%\n",
      "Iteration: 84350  Loss: 0.3679693043231964  Accuracy: 90%\n",
      "Iteration: 84400  Loss: 0.40696772933006287  Accuracy: 90%\n",
      "Iteration: 84450  Loss: 0.47038590908050537  Accuracy: 90%\n",
      "Iteration: 84500  Loss: 0.33215513825416565  Accuracy: 90%\n",
      "Iteration: 84550  Loss: 0.41444918513298035  Accuracy: 90%\n",
      "Iteration: 84600  Loss: 0.3056716322898865  Accuracy: 90%\n",
      "Iteration: 84650  Loss: 0.3780173361301422  Accuracy: 90%\n",
      "Iteration: 84700  Loss: 0.3563719391822815  Accuracy: 90%\n",
      "Iteration: 84750  Loss: 0.2542056739330292  Accuracy: 90%\n",
      "Iteration: 84800  Loss: 0.39454448223114014  Accuracy: 90%\n",
      "Iteration: 84850  Loss: 0.29909420013427734  Accuracy: 90%\n",
      "Iteration: 84900  Loss: 0.3579501509666443  Accuracy: 90%\n",
      "Iteration: 84950  Loss: 0.30184227228164673  Accuracy: 90%\n",
      "Iteration: 85000  Loss: 0.4453417658805847  Accuracy: 90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 85050  Loss: 0.32816994190216064  Accuracy: 90%\n",
      "Iteration: 85100  Loss: 0.28539520502090454  Accuracy: 90%\n",
      "Iteration: 85150  Loss: 0.3750263452529907  Accuracy: 90%\n",
      "Iteration: 85200  Loss: 0.3416357934474945  Accuracy: 90%\n",
      "Iteration: 85250  Loss: 0.3490826487541199  Accuracy: 90%\n",
      "Iteration: 85300  Loss: 0.28158482909202576  Accuracy: 90%\n",
      "Iteration: 85350  Loss: 0.3864726722240448  Accuracy: 90%\n",
      "Iteration: 85400  Loss: 0.301938533782959  Accuracy: 90%\n",
      "Iteration: 85450  Loss: 0.3261074423789978  Accuracy: 90%\n",
      "Iteration: 85500  Loss: 0.3546612858772278  Accuracy: 90%\n",
      "Iteration: 85550  Loss: 0.3652542531490326  Accuracy: 90%\n",
      "Iteration: 85600  Loss: 0.3403179943561554  Accuracy: 90%\n",
      "Iteration: 85650  Loss: 0.31806477904319763  Accuracy: 90%\n",
      "Iteration: 85700  Loss: 0.32854098081588745  Accuracy: 90%\n",
      "Iteration: 85750  Loss: 0.31069040298461914  Accuracy: 90%\n",
      "Iteration: 85800  Loss: 0.23873507976531982  Accuracy: 90%\n",
      "Iteration: 85850  Loss: 0.3334212005138397  Accuracy: 90%\n",
      "Iteration: 85900  Loss: 0.30659517645835876  Accuracy: 90%\n",
      "Iteration: 85950  Loss: 0.4306259751319885  Accuracy: 90%\n",
      "Iteration: 86000  Loss: 0.31305378675460815  Accuracy: 90%\n",
      "Iteration: 86050  Loss: 0.35599496960639954  Accuracy: 90%\n",
      "Iteration: 86100  Loss: 0.3397739827632904  Accuracy: 90%\n",
      "Iteration: 86150  Loss: 0.344060480594635  Accuracy: 90%\n",
      "Iteration: 86200  Loss: 0.35270699858665466  Accuracy: 90%\n",
      "Iteration: 86250  Loss: 0.38211849331855774  Accuracy: 90%\n",
      "Iteration: 86300  Loss: 0.4252338409423828  Accuracy: 90%\n",
      "Iteration: 86350  Loss: 0.440628319978714  Accuracy: 90%\n",
      "Iteration: 86400  Loss: 0.39205387234687805  Accuracy: 90%\n",
      "Iteration: 86450  Loss: 0.3669910728931427  Accuracy: 90%\n",
      "Iteration: 86500  Loss: 0.3307742476463318  Accuracy: 90%\n",
      "Iteration: 86550  Loss: 0.3237606883049011  Accuracy: 90%\n",
      "Iteration: 86600  Loss: 0.34802472591400146  Accuracy: 90%\n",
      "Iteration: 86650  Loss: 0.3189486265182495  Accuracy: 90%\n",
      "Iteration: 86700  Loss: 0.2650952935218811  Accuracy: 90%\n",
      "Iteration: 86750  Loss: 0.38470447063446045  Accuracy: 90%\n",
      "Iteration: 86800  Loss: 0.3393961489200592  Accuracy: 90%\n",
      "Iteration: 86850  Loss: 0.31802257895469666  Accuracy: 90%\n",
      "Iteration: 86900  Loss: 0.3782213032245636  Accuracy: 90%\n",
      "Iteration: 86950  Loss: 0.35959580540657043  Accuracy: 90%\n",
      "Iteration: 87000  Loss: 0.3256462514400482  Accuracy: 90%\n",
      "Iteration: 87050  Loss: 0.44582974910736084  Accuracy: 90%\n",
      "Iteration: 87100  Loss: 0.3053731620311737  Accuracy: 90%\n",
      "Iteration: 87150  Loss: 0.29376280307769775  Accuracy: 90%\n",
      "Iteration: 87200  Loss: 0.3587072491645813  Accuracy: 90%\n",
      "Iteration: 87250  Loss: 0.28264281153678894  Accuracy: 90%\n",
      "Iteration: 87300  Loss: 0.34693267941474915  Accuracy: 90%\n",
      "Iteration: 87350  Loss: 0.37131834030151367  Accuracy: 90%\n",
      "Iteration: 87400  Loss: 0.32168644666671753  Accuracy: 90%\n",
      "Iteration: 87450  Loss: 0.40795162320137024  Accuracy: 90%\n",
      "Iteration: 87500  Loss: 0.336066871881485  Accuracy: 90%\n",
      "Iteration: 87550  Loss: 0.2963900864124298  Accuracy: 90%\n",
      "Iteration: 87600  Loss: 0.29375505447387695  Accuracy: 90%\n",
      "Iteration: 87650  Loss: 0.3710698187351227  Accuracy: 90%\n",
      "Iteration: 87700  Loss: 0.27487295866012573  Accuracy: 90%\n",
      "Iteration: 87750  Loss: 0.32059887051582336  Accuracy: 90%\n",
      "Iteration: 87800  Loss: 0.2579320967197418  Accuracy: 90%\n",
      "Iteration: 87850  Loss: 0.3761492371559143  Accuracy: 90%\n",
      "Iteration: 87900  Loss: 0.3758328855037689  Accuracy: 90%\n",
      "Iteration: 87950  Loss: 0.45231109857559204  Accuracy: 90%\n",
      "Iteration: 88000  Loss: 0.34385600686073303  Accuracy: 90%\n",
      "Iteration: 88050  Loss: 0.3392998278141022  Accuracy: 90%\n",
      "Iteration: 88100  Loss: 0.38422825932502747  Accuracy: 90%\n",
      "Iteration: 88150  Loss: 0.3790920674800873  Accuracy: 90%\n",
      "Iteration: 88200  Loss: 0.27959519624710083  Accuracy: 90%\n",
      "Iteration: 88250  Loss: 0.28287428617477417  Accuracy: 90%\n",
      "Iteration: 88300  Loss: 0.3597346544265747  Accuracy: 90%\n",
      "Iteration: 88350  Loss: 0.3036016523838043  Accuracy: 90%\n",
      "Iteration: 88400  Loss: 0.2339165210723877  Accuracy: 90%\n",
      "Iteration: 88450  Loss: 0.4315246641635895  Accuracy: 90%\n",
      "Iteration: 88500  Loss: 0.38101696968078613  Accuracy: 90%\n",
      "Iteration: 88550  Loss: 0.3214241862297058  Accuracy: 90%\n",
      "Iteration: 88600  Loss: 0.3753053545951843  Accuracy: 90%\n",
      "Iteration: 88650  Loss: 0.317994624376297  Accuracy: 90%\n",
      "Iteration: 88700  Loss: 0.36183133721351624  Accuracy: 90%\n",
      "Iteration: 88750  Loss: 0.41478264331817627  Accuracy: 90%\n",
      "Iteration: 88800  Loss: 0.41485995054244995  Accuracy: 90%\n",
      "Iteration: 88850  Loss: 0.3159352242946625  Accuracy: 90%\n",
      "Iteration: 88900  Loss: 0.34760212898254395  Accuracy: 90%\n",
      "Iteration: 88950  Loss: 0.37229058146476746  Accuracy: 90%\n",
      "Iteration: 89000  Loss: 0.28292331099510193  Accuracy: 90%\n",
      "Iteration: 89050  Loss: 0.2689337432384491  Accuracy: 90%\n",
      "Iteration: 89100  Loss: 0.3485026955604553  Accuracy: 90%\n",
      "Iteration: 89150  Loss: 0.325223833322525  Accuracy: 90%\n",
      "Iteration: 89200  Loss: 0.3141021430492401  Accuracy: 90%\n",
      "Iteration: 89250  Loss: 0.3558347225189209  Accuracy: 90%\n",
      "Iteration: 89300  Loss: 0.4433653950691223  Accuracy: 90%\n",
      "Iteration: 89350  Loss: 0.426837295293808  Accuracy: 90%\n",
      "Iteration: 89400  Loss: 0.34889015555381775  Accuracy: 90%\n",
      "Iteration: 89450  Loss: 0.3588983714580536  Accuracy: 90%\n",
      "Iteration: 89500  Loss: 0.3126288652420044  Accuracy: 90%\n",
      "Iteration: 89550  Loss: 0.38296958804130554  Accuracy: 90%\n",
      "Iteration: 89600  Loss: 0.2905471622943878  Accuracy: 90%\n",
      "Iteration: 89650  Loss: 0.29812103509902954  Accuracy: 90%\n",
      "Iteration: 89700  Loss: 0.3388039767742157  Accuracy: 90%\n",
      "Iteration: 89750  Loss: 0.3623898923397064  Accuracy: 90%\n",
      "Iteration: 89800  Loss: 0.37001508474349976  Accuracy: 90%\n",
      "Iteration: 89850  Loss: 0.3766402304172516  Accuracy: 90%\n",
      "Iteration: 89900  Loss: 0.3511861264705658  Accuracy: 90%\n",
      "Iteration: 89950  Loss: 0.42397043108940125  Accuracy: 90%\n",
      "Iteration: 90000  Loss: 0.4161958694458008  Accuracy: 90%\n",
      "Iteration: 90050  Loss: 0.32993170619010925  Accuracy: 90%\n",
      "Iteration: 90100  Loss: 0.3804680109024048  Accuracy: 90%\n",
      "Iteration: 90150  Loss: 0.2623234689235687  Accuracy: 90%\n",
      "Iteration: 90200  Loss: 0.317689448595047  Accuracy: 90%\n",
      "Iteration: 90250  Loss: 0.4142010509967804  Accuracy: 90%\n",
      "Iteration: 90300  Loss: 0.30950695276260376  Accuracy: 90%\n",
      "Iteration: 90350  Loss: 0.3669867515563965  Accuracy: 90%\n",
      "Iteration: 90400  Loss: 0.3369464874267578  Accuracy: 90%\n",
      "Iteration: 90450  Loss: 0.3981285095214844  Accuracy: 90%\n",
      "Iteration: 90500  Loss: 0.4070524275302887  Accuracy: 90%\n",
      "Iteration: 90550  Loss: 0.35090357065200806  Accuracy: 90%\n",
      "Iteration: 90600  Loss: 0.3365985155105591  Accuracy: 90%\n",
      "Iteration: 90650  Loss: 0.3611302673816681  Accuracy: 90%\n",
      "Iteration: 90700  Loss: 0.3388391435146332  Accuracy: 90%\n",
      "Iteration: 90750  Loss: 0.2956927716732025  Accuracy: 90%\n",
      "Iteration: 90800  Loss: 0.2969377040863037  Accuracy: 90%\n",
      "Iteration: 90850  Loss: 0.3309861421585083  Accuracy: 90%\n",
      "Iteration: 90900  Loss: 0.35742810368537903  Accuracy: 90%\n",
      "Iteration: 90950  Loss: 0.3624887764453888  Accuracy: 90%\n",
      "Iteration: 91000  Loss: 0.3493587374687195  Accuracy: 90%\n",
      "Iteration: 91050  Loss: 0.35751718282699585  Accuracy: 90%\n",
      "Iteration: 91100  Loss: 0.4452747404575348  Accuracy: 90%\n",
      "Iteration: 91150  Loss: 0.34218844771385193  Accuracy: 90%\n",
      "Iteration: 91200  Loss: 0.3780737817287445  Accuracy: 90%\n",
      "Iteration: 91250  Loss: 0.2879357635974884  Accuracy: 90%\n",
      "Iteration: 91300  Loss: 0.4183378517627716  Accuracy: 90%\n",
      "Iteration: 91350  Loss: 0.3353137671947479  Accuracy: 90%\n",
      "Iteration: 91400  Loss: 0.28687432408332825  Accuracy: 90%\n",
      "Iteration: 91450  Loss: 0.3384076654911041  Accuracy: 90%\n",
      "Iteration: 91500  Loss: 0.33279305696487427  Accuracy: 90%\n",
      "Iteration: 91550  Loss: 0.3890984058380127  Accuracy: 90%\n",
      "Iteration: 91600  Loss: 0.3109774589538574  Accuracy: 90%\n",
      "Iteration: 91650  Loss: 0.3318990170955658  Accuracy: 90%\n",
      "Iteration: 91700  Loss: 0.31837740540504456  Accuracy: 90%\n",
      "Iteration: 91750  Loss: 0.4632311463356018  Accuracy: 90%\n",
      "Iteration: 91800  Loss: 0.3578641712665558  Accuracy: 90%\n",
      "Iteration: 91850  Loss: 0.28839561343193054  Accuracy: 90%\n",
      "Iteration: 91900  Loss: 0.34807488322257996  Accuracy: 90%\n",
      "Iteration: 91950  Loss: 0.3653622269630432  Accuracy: 90%\n",
      "Iteration: 92000  Loss: 0.2486211359500885  Accuracy: 90%\n",
      "Iteration: 92050  Loss: 0.37939774990081787  Accuracy: 90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 92100  Loss: 0.3535400629043579  Accuracy: 90%\n",
      "Iteration: 92150  Loss: 0.3728708028793335  Accuracy: 90%\n",
      "Iteration: 92200  Loss: 0.33718419075012207  Accuracy: 90%\n",
      "Iteration: 92250  Loss: 0.3329932391643524  Accuracy: 90%\n",
      "Iteration: 92300  Loss: 0.39454057812690735  Accuracy: 90%\n",
      "Iteration: 92350  Loss: 0.24932122230529785  Accuracy: 90%\n",
      "Iteration: 92400  Loss: 0.39460861682891846  Accuracy: 90%\n",
      "Iteration: 92450  Loss: 0.29037976264953613  Accuracy: 90%\n",
      "Iteration: 92500  Loss: 0.30196288228034973  Accuracy: 90%\n",
      "Iteration: 92550  Loss: 0.3038654327392578  Accuracy: 90%\n",
      "Iteration: 92600  Loss: 0.3459480404853821  Accuracy: 90%\n",
      "Iteration: 92650  Loss: 0.3793271481990814  Accuracy: 90%\n",
      "Iteration: 92700  Loss: 0.3292561173439026  Accuracy: 90%\n",
      "Iteration: 92750  Loss: 0.3113241493701935  Accuracy: 90%\n",
      "Iteration: 92800  Loss: 0.304222047328949  Accuracy: 90%\n",
      "Iteration: 92850  Loss: 0.33648842573165894  Accuracy: 90%\n",
      "Iteration: 92900  Loss: 0.34151649475097656  Accuracy: 90%\n",
      "Iteration: 92950  Loss: 0.33011114597320557  Accuracy: 90%\n",
      "Iteration: 93000  Loss: 0.33699020743370056  Accuracy: 90%\n",
      "Iteration: 93050  Loss: 0.37803295254707336  Accuracy: 90%\n",
      "Iteration: 93100  Loss: 0.39050179719924927  Accuracy: 90%\n",
      "Iteration: 93150  Loss: 0.39957195520401  Accuracy: 90%\n",
      "Iteration: 93200  Loss: 0.3485029637813568  Accuracy: 90%\n",
      "Iteration: 93250  Loss: 0.2998493015766144  Accuracy: 90%\n",
      "Iteration: 93300  Loss: 0.32920393347740173  Accuracy: 90%\n",
      "Iteration: 93350  Loss: 0.30925679206848145  Accuracy: 90%\n",
      "Iteration: 93400  Loss: 0.4459771513938904  Accuracy: 90%\n",
      "Iteration: 93450  Loss: 0.2912673354148865  Accuracy: 90%\n",
      "Iteration: 93500  Loss: 0.3417434096336365  Accuracy: 90%\n",
      "Iteration: 93550  Loss: 0.38720667362213135  Accuracy: 90%\n",
      "Iteration: 93600  Loss: 0.4130169153213501  Accuracy: 90%\n",
      "Iteration: 93650  Loss: 0.40572506189346313  Accuracy: 90%\n",
      "Iteration: 93700  Loss: 0.3728126883506775  Accuracy: 90%\n",
      "Iteration: 93750  Loss: 0.36045894026756287  Accuracy: 90%\n",
      "Iteration: 93800  Loss: 0.32515278458595276  Accuracy: 90%\n",
      "Iteration: 93850  Loss: 0.33803218603134155  Accuracy: 90%\n",
      "Iteration: 93900  Loss: 0.38868945837020874  Accuracy: 90%\n",
      "Iteration: 93950  Loss: 0.305644690990448  Accuracy: 90%\n",
      "Iteration: 94000  Loss: 0.38565072417259216  Accuracy: 90%\n",
      "Iteration: 94050  Loss: 0.2644517719745636  Accuracy: 90%\n",
      "Iteration: 94100  Loss: 0.3676469624042511  Accuracy: 90%\n",
      "Iteration: 94150  Loss: 0.30351579189300537  Accuracy: 90%\n",
      "Iteration: 94200  Loss: 0.28555771708488464  Accuracy: 90%\n",
      "Iteration: 94250  Loss: 0.343737930059433  Accuracy: 90%\n",
      "Iteration: 94300  Loss: 0.3152311146259308  Accuracy: 90%\n",
      "Iteration: 94350  Loss: 0.315644234418869  Accuracy: 90%\n",
      "Iteration: 94400  Loss: 0.3666565716266632  Accuracy: 90%\n",
      "Iteration: 94450  Loss: 0.3123507797718048  Accuracy: 90%\n",
      "Iteration: 94500  Loss: 0.2761628031730652  Accuracy: 90%\n",
      "Iteration: 94550  Loss: 0.3257211148738861  Accuracy: 90%\n",
      "Iteration: 94600  Loss: 0.38031646609306335  Accuracy: 90%\n",
      "Iteration: 94650  Loss: 0.31712082028388977  Accuracy: 90%\n",
      "Iteration: 94700  Loss: 0.3372083902359009  Accuracy: 90%\n",
      "Iteration: 94750  Loss: 0.41322022676467896  Accuracy: 90%\n",
      "Iteration: 94800  Loss: 0.3349166512489319  Accuracy: 90%\n",
      "Iteration: 94850  Loss: 0.3937207758426666  Accuracy: 90%\n",
      "Iteration: 94900  Loss: 0.38786187767982483  Accuracy: 90%\n",
      "Iteration: 94950  Loss: 0.3393104374408722  Accuracy: 90%\n",
      "Iteration: 95000  Loss: 0.364942729473114  Accuracy: 90%\n",
      "Iteration: 95050  Loss: 0.2953105568885803  Accuracy: 90%\n",
      "Iteration: 95100  Loss: 0.3425987958908081  Accuracy: 90%\n",
      "Iteration: 95150  Loss: 0.2821698784828186  Accuracy: 90%\n",
      "Iteration: 95200  Loss: 0.3478139638900757  Accuracy: 90%\n",
      "Iteration: 95250  Loss: 0.3161368668079376  Accuracy: 90%\n",
      "Iteration: 95300  Loss: 0.3777562081813812  Accuracy: 90%\n",
      "Iteration: 95350  Loss: 0.3465278446674347  Accuracy: 90%\n",
      "Iteration: 95400  Loss: 0.3079894185066223  Accuracy: 90%\n",
      "Iteration: 95450  Loss: 0.3630596697330475  Accuracy: 90%\n",
      "Iteration: 95500  Loss: 0.31379809975624084  Accuracy: 90%\n",
      "Iteration: 95550  Loss: 0.33358296751976013  Accuracy: 90%\n",
      "Iteration: 95600  Loss: 0.34262242913246155  Accuracy: 90%\n",
      "Iteration: 95650  Loss: 0.3010309338569641  Accuracy: 90%\n",
      "Iteration: 95700  Loss: 0.23843355476856232  Accuracy: 90%\n",
      "Iteration: 95750  Loss: 0.3726077079772949  Accuracy: 90%\n",
      "Iteration: 95800  Loss: 0.35724785923957825  Accuracy: 90%\n",
      "Iteration: 95850  Loss: 0.4176023006439209  Accuracy: 90%\n",
      "Iteration: 95900  Loss: 0.35799482464790344  Accuracy: 90%\n",
      "Iteration: 95950  Loss: 0.4069821238517761  Accuracy: 90%\n",
      "Iteration: 96000  Loss: 0.36366334557533264  Accuracy: 90%\n",
      "Iteration: 96050  Loss: 0.32211485505104065  Accuracy: 90%\n",
      "Iteration: 96100  Loss: 0.2723592519760132  Accuracy: 90%\n",
      "Iteration: 96150  Loss: 0.26769426465034485  Accuracy: 90%\n",
      "Iteration: 96200  Loss: 0.2989580035209656  Accuracy: 90%\n",
      "Iteration: 96250  Loss: 0.37078648805618286  Accuracy: 90%\n",
      "Iteration: 96300  Loss: 0.4305762052536011  Accuracy: 90%\n",
      "Iteration: 96350  Loss: 0.31127113103866577  Accuracy: 90%\n",
      "Iteration: 96400  Loss: 0.32481101155281067  Accuracy: 90%\n",
      "Iteration: 96450  Loss: 0.33870962262153625  Accuracy: 90%\n",
      "Iteration: 96500  Loss: 0.35162514448165894  Accuracy: 90%\n",
      "Iteration: 96550  Loss: 0.3681412637233734  Accuracy: 90%\n",
      "Iteration: 96600  Loss: 0.41728779673576355  Accuracy: 90%\n",
      "Iteration: 96650  Loss: 0.36703234910964966  Accuracy: 90%\n",
      "Iteration: 96700  Loss: 0.2716831564903259  Accuracy: 90%\n",
      "Iteration: 96750  Loss: 0.38768917322158813  Accuracy: 90%\n",
      "Iteration: 96800  Loss: 0.3476168215274811  Accuracy: 90%\n",
      "Iteration: 96850  Loss: 0.33674412965774536  Accuracy: 90%\n",
      "Iteration: 96900  Loss: 0.3718034327030182  Accuracy: 90%\n",
      "Iteration: 96950  Loss: 0.40321192145347595  Accuracy: 90%\n",
      "Iteration: 97000  Loss: 0.3757983148097992  Accuracy: 90%\n",
      "Iteration: 97050  Loss: 0.2762835621833801  Accuracy: 90%\n",
      "Iteration: 97100  Loss: 0.3701176941394806  Accuracy: 90%\n",
      "Iteration: 97150  Loss: 0.42788583040237427  Accuracy: 90%\n",
      "Iteration: 97200  Loss: 0.35712963342666626  Accuracy: 90%\n",
      "Iteration: 97250  Loss: 0.3970157206058502  Accuracy: 90%\n",
      "Iteration: 97300  Loss: 0.25462958216667175  Accuracy: 90%\n",
      "Iteration: 97350  Loss: 0.2819630801677704  Accuracy: 90%\n",
      "Iteration: 97400  Loss: 0.2850051522254944  Accuracy: 90%\n",
      "Iteration: 97450  Loss: 0.40387946367263794  Accuracy: 90%\n",
      "Iteration: 97500  Loss: 0.2633824348449707  Accuracy: 90%\n",
      "Iteration: 97550  Loss: 0.30226877331733704  Accuracy: 90%\n",
      "Iteration: 97600  Loss: 0.26226091384887695  Accuracy: 90%\n",
      "Iteration: 97650  Loss: 0.32113319635391235  Accuracy: 90%\n",
      "Iteration: 97700  Loss: 0.30659782886505127  Accuracy: 90%\n",
      "Iteration: 97750  Loss: 0.3019373118877411  Accuracy: 90%\n",
      "Iteration: 97800  Loss: 0.308874249458313  Accuracy: 90%\n",
      "Iteration: 97850  Loss: 0.23497581481933594  Accuracy: 90%\n",
      "Iteration: 97900  Loss: 0.3892136812210083  Accuracy: 90%\n",
      "Iteration: 97950  Loss: 0.45239102840423584  Accuracy: 90%\n",
      "Iteration: 98000  Loss: 0.3610273599624634  Accuracy: 90%\n",
      "Iteration: 98050  Loss: 0.38580191135406494  Accuracy: 90%\n",
      "Iteration: 98100  Loss: 0.30185189843177795  Accuracy: 90%\n",
      "Iteration: 98150  Loss: 0.3617786169052124  Accuracy: 90%\n",
      "Iteration: 98200  Loss: 0.2701074779033661  Accuracy: 90%\n",
      "Iteration: 98250  Loss: 0.27115193009376526  Accuracy: 90%\n",
      "Iteration: 98300  Loss: 0.32954826951026917  Accuracy: 90%\n",
      "Iteration: 98350  Loss: 0.32051584124565125  Accuracy: 90%\n",
      "Iteration: 98400  Loss: 0.2842918336391449  Accuracy: 90%\n",
      "Iteration: 98450  Loss: 0.3027257025241852  Accuracy: 90%\n",
      "Iteration: 98500  Loss: 0.315158873796463  Accuracy: 90%\n",
      "Iteration: 98550  Loss: 0.3524691164493561  Accuracy: 90%\n",
      "Iteration: 98600  Loss: 0.37636110186576843  Accuracy: 90%\n",
      "Iteration: 98650  Loss: 0.42043259739875793  Accuracy: 90%\n",
      "Iteration: 98700  Loss: 0.24665595591068268  Accuracy: 90%\n",
      "Iteration: 98750  Loss: 0.3112909495830536  Accuracy: 90%\n",
      "Iteration: 98800  Loss: 0.30936047434806824  Accuracy: 90%\n",
      "Iteration: 98850  Loss: 0.29557228088378906  Accuracy: 90%\n",
      "Iteration: 98900  Loss: 0.3210366368293762  Accuracy: 90%\n",
      "Iteration: 98950  Loss: 0.3506317436695099  Accuracy: 90%\n",
      "Iteration: 99000  Loss: 0.3376995325088501  Accuracy: 90%\n",
      "Iteration: 99050  Loss: 0.301017165184021  Accuracy: 90%\n",
      "Iteration: 99100  Loss: 0.26396644115448  Accuracy: 90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99150  Loss: 0.37585702538490295  Accuracy: 90%\n",
      "Iteration: 99200  Loss: 0.37154173851013184  Accuracy: 90%\n",
      "Iteration: 99250  Loss: 0.4046391546726227  Accuracy: 90%\n",
      "Iteration: 99300  Loss: 0.28029486536979675  Accuracy: 90%\n",
      "Iteration: 99350  Loss: 0.3747292459011078  Accuracy: 90%\n",
      "Iteration: 99400  Loss: 0.35438770055770874  Accuracy: 90%\n",
      "Iteration: 99450  Loss: 0.4613003730773926  Accuracy: 90%\n",
      "Iteration: 99500  Loss: 0.3261808156967163  Accuracy: 90%\n",
      "Iteration: 99550  Loss: 0.27916043996810913  Accuracy: 90%\n",
      "Iteration: 99600  Loss: 0.3362926244735718  Accuracy: 90%\n",
      "Iteration: 99650  Loss: 0.3342100977897644  Accuracy: 90%\n",
      "Iteration: 99700  Loss: 0.43757545948028564  Accuracy: 90%\n",
      "Iteration: 99750  Loss: 0.3038769066333771  Accuracy: 90%\n",
      "Iteration: 99800  Loss: 0.34717464447021484  Accuracy: 90%\n",
      "Iteration: 99850  Loss: 0.2968670725822449  Accuracy: 90%\n",
      "Iteration: 99900  Loss: 0.3165780007839203  Accuracy: 90%\n",
      "Iteration: 99950  Loss: 0.3595217764377594  Accuracy: 90%\n",
      "Iteration: 100000  Loss: 0.3467274606227875  Accuracy: 90%\n",
      "Iteration: 100050  Loss: 0.3391975164413452  Accuracy: 90%\n",
      "Iteration: 100100  Loss: 0.32665663957595825  Accuracy: 90%\n",
      "Iteration: 100150  Loss: 0.36377808451652527  Accuracy: 90%\n",
      "Iteration: 100200  Loss: 0.3485661447048187  Accuracy: 90%\n",
      "Iteration: 100250  Loss: 0.3637150824069977  Accuracy: 90%\n",
      "Iteration: 100300  Loss: 0.3219864070415497  Accuracy: 90%\n",
      "Iteration: 100350  Loss: 0.32743126153945923  Accuracy: 90%\n",
      "Iteration: 100400  Loss: 0.3285338580608368  Accuracy: 90%\n",
      "Iteration: 100450  Loss: 0.4531075358390808  Accuracy: 90%\n",
      "Iteration: 100500  Loss: 0.282294362783432  Accuracy: 90%\n",
      "Iteration: 100550  Loss: 0.3008347749710083  Accuracy: 90%\n",
      "Iteration: 100600  Loss: 0.4027879536151886  Accuracy: 90%\n",
      "Iteration: 100650  Loss: 0.3923935294151306  Accuracy: 90%\n",
      "Iteration: 100700  Loss: 0.26932311058044434  Accuracy: 90%\n",
      "Iteration: 100750  Loss: 0.33218106627464294  Accuracy: 90%\n",
      "Iteration: 100800  Loss: 0.4600154161453247  Accuracy: 90%\n",
      "Iteration: 100850  Loss: 0.389771431684494  Accuracy: 90%\n",
      "Iteration: 100900  Loss: 0.28439486026763916  Accuracy: 90%\n",
      "Iteration: 100950  Loss: 0.36576706171035767  Accuracy: 90%\n",
      "Iteration: 101000  Loss: 0.3382643461227417  Accuracy: 90%\n",
      "Iteration: 101050  Loss: 0.3634839653968811  Accuracy: 90%\n",
      "Iteration: 101100  Loss: 0.33107849955558777  Accuracy: 90%\n",
      "Iteration: 101150  Loss: 0.31712016463279724  Accuracy: 90%\n",
      "Iteration: 101200  Loss: 0.3925035893917084  Accuracy: 90%\n",
      "Iteration: 101250  Loss: 0.36906468868255615  Accuracy: 90%\n",
      "Iteration: 101300  Loss: 0.33936575055122375  Accuracy: 90%\n",
      "Iteration: 101350  Loss: 0.35437461733818054  Accuracy: 90%\n",
      "Iteration: 101400  Loss: 0.330858051776886  Accuracy: 90%\n",
      "Iteration: 101450  Loss: 0.38177481293678284  Accuracy: 90%\n",
      "Iteration: 101500  Loss: 0.3295683264732361  Accuracy: 90%\n",
      "Iteration: 101550  Loss: 0.34219518303871155  Accuracy: 90%\n",
      "Iteration: 101600  Loss: 0.4380544126033783  Accuracy: 90%\n",
      "Iteration: 101650  Loss: 0.34121036529541016  Accuracy: 90%\n",
      "Iteration: 101700  Loss: 0.29951706528663635  Accuracy: 90%\n",
      "Iteration: 101750  Loss: 0.27894487977027893  Accuracy: 90%\n",
      "Iteration: 101800  Loss: 0.3068443238735199  Accuracy: 90%\n",
      "Iteration: 101850  Loss: 0.4158927798271179  Accuracy: 90%\n",
      "Iteration: 101900  Loss: 0.4225926697254181  Accuracy: 90%\n",
      "Iteration: 101950  Loss: 0.42815110087394714  Accuracy: 90%\n",
      "Iteration: 102000  Loss: 0.33458635210990906  Accuracy: 90%\n",
      "Iteration: 102050  Loss: 0.3648764193058014  Accuracy: 90%\n",
      "Iteration: 102100  Loss: 0.3488102853298187  Accuracy: 90%\n",
      "Iteration: 102150  Loss: 0.37894514203071594  Accuracy: 90%\n",
      "Iteration: 102200  Loss: 0.3356357216835022  Accuracy: 90%\n",
      "Iteration: 102250  Loss: 0.3571956157684326  Accuracy: 90%\n",
      "Iteration: 102300  Loss: 0.3313368856906891  Accuracy: 90%\n",
      "Iteration: 102350  Loss: 0.3864985704421997  Accuracy: 90%\n",
      "Iteration: 102400  Loss: 0.27824801206588745  Accuracy: 90%\n",
      "Iteration: 102450  Loss: 0.2825080156326294  Accuracy: 90%\n",
      "Iteration: 102500  Loss: 0.3625321090221405  Accuracy: 90%\n",
      "Iteration: 102550  Loss: 0.32114481925964355  Accuracy: 90%\n",
      "Iteration: 102600  Loss: 0.33479583263397217  Accuracy: 90%\n",
      "Iteration: 102650  Loss: 0.38339897990226746  Accuracy: 90%\n",
      "Iteration: 102700  Loss: 0.38038209080696106  Accuracy: 90%\n",
      "Iteration: 102750  Loss: 0.35380029678344727  Accuracy: 90%\n",
      "Iteration: 102800  Loss: 0.4248276352882385  Accuracy: 90%\n",
      "Iteration: 102850  Loss: 0.25461238622665405  Accuracy: 90%\n",
      "Iteration: 102900  Loss: 0.3352232873439789  Accuracy: 90%\n",
      "Iteration: 102950  Loss: 0.3511221706867218  Accuracy: 90%\n",
      "Iteration: 103000  Loss: 0.27359724044799805  Accuracy: 90%\n",
      "Iteration: 103050  Loss: 0.32500746846199036  Accuracy: 90%\n",
      "Iteration: 103100  Loss: 0.4508358836174011  Accuracy: 90%\n",
      "Iteration: 103150  Loss: 0.3806217908859253  Accuracy: 90%\n",
      "Iteration: 103200  Loss: 0.314275860786438  Accuracy: 90%\n",
      "Iteration: 103250  Loss: 0.38956406712532043  Accuracy: 90%\n",
      "Iteration: 103300  Loss: 0.3453037142753601  Accuracy: 90%\n",
      "Iteration: 103350  Loss: 0.3700059652328491  Accuracy: 90%\n",
      "Iteration: 103400  Loss: 0.32106220722198486  Accuracy: 90%\n",
      "Iteration: 103450  Loss: 0.3080015182495117  Accuracy: 90%\n",
      "Iteration: 103500  Loss: 0.34446072578430176  Accuracy: 90%\n",
      "Iteration: 103550  Loss: 0.354876309633255  Accuracy: 90%\n",
      "Iteration: 103600  Loss: 0.26695820689201355  Accuracy: 90%\n",
      "Iteration: 103650  Loss: 0.3276884853839874  Accuracy: 90%\n",
      "Iteration: 103700  Loss: 0.320889949798584  Accuracy: 90%\n",
      "Iteration: 103750  Loss: 0.2894478738307953  Accuracy: 90%\n",
      "Iteration: 103800  Loss: 0.3702184557914734  Accuracy: 90%\n",
      "Iteration: 103850  Loss: 0.3606901168823242  Accuracy: 90%\n",
      "Iteration: 103900  Loss: 0.3117195665836334  Accuracy: 90%\n",
      "Iteration: 103950  Loss: 0.30606216192245483  Accuracy: 90%\n",
      "Iteration: 104000  Loss: 0.3744376003742218  Accuracy: 90%\n",
      "Iteration: 104050  Loss: 0.4031909704208374  Accuracy: 90%\n",
      "Iteration: 104100  Loss: 0.32906582951545715  Accuracy: 90%\n",
      "Iteration: 104150  Loss: 0.3316385746002197  Accuracy: 90%\n",
      "Iteration: 104200  Loss: 0.305911660194397  Accuracy: 90%\n",
      "Iteration: 104250  Loss: 0.38063478469848633  Accuracy: 90%\n",
      "Iteration: 104300  Loss: 0.3188411593437195  Accuracy: 90%\n",
      "Iteration: 104350  Loss: 0.36593860387802124  Accuracy: 90%\n",
      "Iteration: 104400  Loss: 0.3090503513813019  Accuracy: 90%\n",
      "Iteration: 104450  Loss: 0.31290024518966675  Accuracy: 90%\n",
      "Iteration: 104500  Loss: 0.2709800601005554  Accuracy: 90%\n",
      "Iteration: 104550  Loss: 0.3689540922641754  Accuracy: 90%\n",
      "Iteration: 104600  Loss: 0.3339228332042694  Accuracy: 90%\n",
      "Iteration: 104650  Loss: 0.36418092250823975  Accuracy: 90%\n",
      "Iteration: 104700  Loss: 0.2890283167362213  Accuracy: 90%\n",
      "Iteration: 104750  Loss: 0.2864590883255005  Accuracy: 90%\n",
      "Iteration: 104800  Loss: 0.32202085852622986  Accuracy: 90%\n",
      "Iteration: 104850  Loss: 0.22115880250930786  Accuracy: 90%\n",
      "Iteration: 104900  Loss: 0.32197117805480957  Accuracy: 90%\n",
      "Iteration: 104950  Loss: 0.43346309661865234  Accuracy: 90%\n",
      "Iteration: 105000  Loss: 0.40058332681655884  Accuracy: 90%\n",
      "Iteration: 105050  Loss: 0.30178961157798767  Accuracy: 90%\n",
      "Iteration: 105100  Loss: 0.3611884117126465  Accuracy: 90%\n",
      "Iteration: 105150  Loss: 0.35136881470680237  Accuracy: 90%\n",
      "Iteration: 105200  Loss: 0.310129851102829  Accuracy: 90%\n",
      "Iteration: 105250  Loss: 0.38154736161231995  Accuracy: 90%\n",
      "Iteration: 105300  Loss: 0.4200305640697479  Accuracy: 90%\n",
      "Iteration: 105350  Loss: 0.2682002782821655  Accuracy: 90%\n",
      "Iteration: 105400  Loss: 0.2771473228931427  Accuracy: 90%\n",
      "Iteration: 105450  Loss: 0.36571332812309265  Accuracy: 90%\n",
      "Iteration: 105500  Loss: 0.3180847465991974  Accuracy: 90%\n",
      "Iteration: 105550  Loss: 0.33759620785713196  Accuracy: 90%\n",
      "Iteration: 105600  Loss: 0.37217193841934204  Accuracy: 90%\n"
     ]
    }
   ],
   "source": [
    "# Traning the Model\n",
    "# the inner loop runs 131 times because x_train.shape[0] (the number of samples) / 256 (batch size) = 131\n",
    "# since num_epochs = 800, and batch_size = 256, the number of losses should be around 800 * (#samples/256)\n",
    "num_epochs = 800 # number of epochs\n",
    "count = 0 # Count keeps track of when u want to append something to loss_list / iteration_list\n",
    "loss_list = [] # loss values\n",
    "iteration_list = [] # iteration_list is used to keep track of the iteration corresponding to the loss value\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # source: https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch\n",
    "        # the view function reshapes the tensor so that it now has dimensions 28 * 28\n",
    "        # the parameter -1 tells pytorch to reshape the tensor to the specific number of columns (784) and to decide\n",
    "        # the number of rows by itself\n",
    "        # i dont think images.view is needed though since images.shape is 256 * 784 already\n",
    "        # train = Variable(images.view(-1, 28*28)) \n",
    "        \n",
    "        # Defining the variables\n",
    "        train = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients\n",
    "        ## What is clearing gradients for?\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        out = linm(train)\n",
    "        \n",
    "        # Calculating softmax and cross entropy loss\n",
    "        loss = error(out, labels)\n",
    "        \n",
    "        # Calculating the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count +=1\n",
    "        \n",
    "        # Predict the test set\n",
    "        if count % 50 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for test_images, test_labels in test_loader:\n",
    "                test = Variable(test_images.view(-1,28*28)) # Creating the variables for the test dataset\n",
    "                test_out = linm(test) # output info for test\n",
    "                # torch.max(test_out.data) returns the max of the entire data structure\n",
    "                # torch.max(test_out.data, 0) returns the max for each column, and 1 instead of 0 returns max for each row\n",
    "                # torch.max(test_out.data, 1)[0] returns the explicit values, and [1] returns the index\n",
    "                predicted = torch.max(test_out.data, 1) [1] # return max values of all elements in input tensor\n",
    "                total+= len(test_labels) # total number of labels\n",
    "                correct += (predicted == test_labels).sum()\n",
    "            accuracy = 100 * correct / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "plt.plot(iteration_list, loss_list)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Artificial NN (Vanilla NN)\n",
    "class ann(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # Creating a model with 3 hidden layers\n",
    "        self.lin_1 = nn.Linear(in_dim, 392)\n",
    "        self.hidden1 = nn.ReLU()\n",
    "        \n",
    "        self.lin_2 = nn.Linear(392, 196)\n",
    "        self.hidden2 = nn.Tanh()\n",
    "        \n",
    "        self.lin_3 = nn.Linear(196, 98)\n",
    "        self.hidden3 = nn.ELU()\n",
    "        \n",
    "        self.lin_4 = nn.Linear(98, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.lin_1(x)\n",
    "        out = self.hidden1(out)\n",
    "        \n",
    "        out = self.lin_2(out)\n",
    "        out = self.hidden2(out)\n",
    "        \n",
    "        out = self.lin_3(out)\n",
    "        out = self.hidden3(out)\n",
    "        \n",
    "        out = self.lin_4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating ann\n",
    "ann_model = ann(28*28, 10)\n",
    "\n",
    "# Specifying loss function\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# Creating optimizer\n",
    "optimizer = torch.optim.SGD(ann_model.parameters(), lr = .02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 Loss: 0.37217193841934204 Accuracy: 42 %\n",
      "Iteration: 200 Loss: 0.37217193841934204 Accuracy: 57 %\n",
      "Iteration: 300 Loss: 0.37217193841934204 Accuracy: 61 %\n",
      "Iteration: 400 Loss: 0.37217193841934204 Accuracy: 73 %\n",
      "Iteration: 500 Loss: 0.37217193841934204 Accuracy: 79 %\n",
      "Iteration: 600 Loss: 0.37217193841934204 Accuracy: 82 %\n",
      "Iteration: 700 Loss: 0.37217193841934204 Accuracy: 84 %\n",
      "Iteration: 800 Loss: 0.37217193841934204 Accuracy: 85 %\n",
      "Iteration: 900 Loss: 0.37217193841934204 Accuracy: 87 %\n",
      "Iteration: 1000 Loss: 0.37217193841934204 Accuracy: 87 %\n",
      "Iteration: 1100 Loss: 0.37217193841934204 Accuracy: 88 %\n",
      "Iteration: 1200 Loss: 0.37217193841934204 Accuracy: 88 %\n",
      "Iteration: 1300 Loss: 0.37217193841934204 Accuracy: 88 %\n",
      "Iteration: 1400 Loss: 0.37217193841934204 Accuracy: 89 %\n",
      "Iteration: 1500 Loss: 0.37217193841934204 Accuracy: 89 %\n",
      "Iteration: 1600 Loss: 0.37217193841934204 Accuracy: 89 %\n",
      "Iteration: 1700 Loss: 0.37217193841934204 Accuracy: 90 %\n",
      "Iteration: 1800 Loss: 0.37217193841934204 Accuracy: 90 %\n",
      "Iteration: 1900 Loss: 0.37217193841934204 Accuracy: 90 %\n",
      "Iteration: 2000 Loss: 0.37217193841934204 Accuracy: 90 %\n",
      "Iteration: 2100 Loss: 0.37217193841934204 Accuracy: 91 %\n",
      "Iteration: 2200 Loss: 0.37217193841934204 Accuracy: 91 %\n",
      "Iteration: 2300 Loss: 0.37217193841934204 Accuracy: 91 %\n",
      "Iteration: 2400 Loss: 0.37217193841934204 Accuracy: 91 %\n",
      "Iteration: 2500 Loss: 0.37217193841934204 Accuracy: 91 %\n",
      "Iteration: 2600 Loss: 0.37217193841934204 Accuracy: 92 %\n",
      "Iteration: 2700 Loss: 0.37217193841934204 Accuracy: 92 %\n",
      "Iteration: 2800 Loss: 0.37217193841934204 Accuracy: 92 %\n",
      "Iteration: 2900 Loss: 0.37217193841934204 Accuracy: 92 %\n",
      "Iteration: 3000 Loss: 0.37217193841934204 Accuracy: 92 %\n",
      "Iteration: 3100 Loss: 0.37217193841934204 Accuracy: 92 %\n",
      "Iteration: 3200 Loss: 0.37217193841934204 Accuracy: 92 %\n",
      "Iteration: 3300 Loss: 0.37217193841934204 Accuracy: 372 %\n",
      "Iteration: 3400 Loss: 0.37217193841934204 Accuracy: 93 %\n",
      "Iteration: 3500 Loss: 0.37217193841934204 Accuracy: 93 %\n",
      "Iteration: 3600 Loss: 0.37217193841934204 Accuracy: 93 %\n",
      "Iteration: 3700 Loss: 0.37217193841934204 Accuracy: 93 %\n",
      "Iteration: 3800 Loss: 0.37217193841934204 Accuracy: 93 %\n",
      "Iteration: 3900 Loss: 0.37217193841934204 Accuracy: 93 %\n",
      "Iteration: 4000 Loss: 0.37217193841934204 Accuracy: 93 %\n",
      "Iteration: 4100 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4200 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4300 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4400 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4500 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4600 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4700 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4800 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 4900 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 5000 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 5100 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 5200 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 5300 Loss: 0.37217193841934204 Accuracy: 94 %\n",
      "Iteration: 5400 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 5500 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 5600 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 5700 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 5800 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 5900 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6000 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6100 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6200 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6300 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6400 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6500 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6600 Loss: 0.37217193841934204 Accuracy: 382 %\n",
      "Iteration: 6700 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6800 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 6900 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 7000 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 7100 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 7200 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 7300 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 7400 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 7500 Loss: 0.37217193841934204 Accuracy: 95 %\n",
      "Iteration: 7600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 7700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 7800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 7900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 8900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 9900 Loss: 0.37217193841934204 Accuracy: 385 %\n",
      "Iteration: 10000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 10900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 11900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 12900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13200 Loss: 0.37217193841934204 Accuracy: 386 %\n",
      "Iteration: 13300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 13900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14400 Loss: 0.37217193841934204 Accuracy: 96 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 14900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 15900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16200 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16500 Loss: 0.37217193841934204 Accuracy: 387 %\n",
      "Iteration: 16600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 16700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 16900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17100 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 17300 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 17800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 17900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 18000 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 18100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 18200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 18300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 18400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 18500 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 18600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 18700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 18800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 18900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 19000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 19100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 19200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 19300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 19400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 19500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 19600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 19700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 19800 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 19900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20600 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 20700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 20800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 20900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 21000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 21100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 21200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 21300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 21400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 21500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 21600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 21700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 21800 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 21900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 22500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 22900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23100 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 23200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 23900 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 24000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 24900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25400 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 25500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25700 Loss: 0.37217193841934204 Accuracy: 96 %\n",
      "Iteration: 25800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 25900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26400 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 26500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 26900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 27900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28600 Loss: 0.37217193841934204 Accuracy: 97 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 28900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29700 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 29800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 29900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 30900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 31900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 32900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33000 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 33100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 33900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 34900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 35900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36300 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 36400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 36900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 37900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 38900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39600 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 39700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 39900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 40900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 41900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 42800 Loss: 0.37217193841934204 Accuracy: 97 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 42900 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 43000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 43900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 44900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 45900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46200 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 46300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 46900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 47900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 48900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49500 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 49600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 49900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 50900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 51900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 52800 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 52900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 53900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 54900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 55900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56100 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 56200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 56900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57000 Loss: 0.37217193841934204 Accuracy: 97 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 57100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 57900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 58900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59400 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 59500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 59900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 60900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 61900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62700 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 62800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 62900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 63900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 64900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 65900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66000 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 66100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 66900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 67900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 68900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69300 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 69400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 69900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 70900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71200 Loss: 0.37217193841934204 Accuracy: 97 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 71300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 71900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72600 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 72700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 72900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 73900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 74900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 75900 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 76000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 76900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 77900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 78900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79200 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 79300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 79900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 80900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 81900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82500 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 82600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 82900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 83900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 84900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85400 Loss: 0.37217193841934204 Accuracy: 97 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 85500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 85800 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 85900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 86900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 87900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 88900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89100 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 89200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 89900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 90900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 91900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92400 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 92500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 92900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 93900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 94900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95700 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 95800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 95900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 96900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 97900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 98900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99000 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 99100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99600 Loss: 0.37217193841934204 Accuracy: 97 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 99900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 100900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 101900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102300 Loss: 0.37217193841934204 Accuracy: 388 %\n",
      "Iteration: 102400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 102900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 103900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104600 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104700 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104800 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 104900 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 105000 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 105100 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 105200 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 105300 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 105400 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 105500 Loss: 0.37217193841934204 Accuracy: 97 %\n",
      "Iteration: 105600 Loss: 0.37217193841934204 Accuracy: 388 %\n"
     ]
    }
   ],
   "source": [
    "# Training the ann model\n",
    "epochs = 800\n",
    "iteration_list = []\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for e in range(epochs):\n",
    "    for i, (train_images, train_values) in enumerate(train_loader):\n",
    "        # Define the variables\n",
    "        train = Variable(train_images.view(-1,28*28))\n",
    "        labels = Variable(train_values)\n",
    "\n",
    "        # Clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        train_out = ann_model.forward(train)\n",
    "\n",
    "        # Calculate loss\n",
    "#         print(train_values.shape, train_out.shape)\n",
    "        train_loss = error(train_out, labels)\n",
    "\n",
    "        # Calculate gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()    \n",
    "        count+=1\n",
    "        if count % 100 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for test_images, test_values in test_loader:\n",
    "                test = Variable(test_images)\n",
    "                test_out = ann_model.forward(test)\n",
    "                predicted = torch.max(test_out, 1)[1]\n",
    "                total += len(labels)\n",
    "                correct += (predicted==test_values).sum() # numbre that was correct\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracy_list.append(accuracy)\n",
    "            loss_list.append(train_loss.data)\n",
    "            iteration_list.append(count)\n",
    "            print('Iteration: {} Loss: {} Accuracy: {} %'.format(count, loss.data, accuracy))\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training went awry so drop the last index\n",
    "accuracy_list = accuracy_list[:-1]\n",
    "loss_list = loss_list[:-1]\n",
    "iteration_list = iteration_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iteration x Loss\n",
    "plt.plot(iteration_list, loss_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv NN Model\n",
    "class cnn_model(nn.Module):\n",
    "    # Observation: no in_dim, out_dim like the two previous models\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Conv 1\n",
    "        ## What do each of these parameters mean?\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.hidden1 = nn.ReLU()\n",
    "        \n",
    "        # Max pooling 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Conv2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride = 1, padding=0)\n",
    "        self.hidden2 = nn.ReLU()\n",
    "        \n",
    "        # Max Pooling 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected\n",
    "        # Dim: 512, 10\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.hidden1(out)\n",
    "        out = self.pool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.hidden2(out)\n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # flatten; -1 means let model decide number of columns i believe\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for prediction part\n",
    "batch_size = 100 # changing batch size from 256 -> 100\n",
    "num_epochs = 200\n",
    "\n",
    "# Creating the train and test sets\n",
    "trainSet = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "testSet = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainSet, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(testSet, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Creating the model\n",
    "model = cnn_model()\n",
    "\n",
    "# Error\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50 Loss: 0.592875063419342 Accuracy: 86.5\n",
      "Iteration: 100 Loss: 0.2643536627292633 Accuracy: 91.55952453613281\n",
      "Iteration: 150 Loss: 0.1777155101299286 Accuracy: 93.88095092773438\n",
      "Iteration: 200 Loss: 0.16675597429275513 Accuracy: 95.07142639160156\n",
      "Iteration: 250 Loss: 0.11144477128982544 Accuracy: 95.85714721679688\n",
      "Iteration: 300 Loss: 0.12514320015907288 Accuracy: 95.51190948486328\n",
      "Iteration: 350 Loss: 0.20498473942279816 Accuracy: 96.21428680419922\n",
      "Iteration: 400 Loss: 0.12622904777526855 Accuracy: 96.55952453613281\n",
      "Iteration: 450 Loss: 0.06578727066516876 Accuracy: 96.57142639160156\n",
      "Iteration: 500 Loss: 0.04246839880943298 Accuracy: 97.29762268066406\n",
      "Iteration: 550 Loss: 0.23893633484840393 Accuracy: 97.14286041259766\n",
      "Iteration: 600 Loss: 0.0348547101020813 Accuracy: 97.0\n",
      "Iteration: 650 Loss: 0.03944312408566475 Accuracy: 97.48809051513672\n",
      "Iteration: 700 Loss: 0.0716116800904274 Accuracy: 97.69047546386719\n",
      "Iteration: 750 Loss: 0.03769059106707573 Accuracy: 97.64286041259766\n",
      "Iteration: 800 Loss: 0.027788866311311722 Accuracy: 97.67857360839844\n",
      "Iteration: 850 Loss: 0.06218978762626648 Accuracy: 97.17857360839844\n",
      "Iteration: 900 Loss: 0.0659509003162384 Accuracy: 97.53571319580078\n",
      "Iteration: 950 Loss: 0.08419004082679749 Accuracy: 97.29762268066406\n",
      "Iteration: 1000 Loss: 0.05906913802027702 Accuracy: 96.89286041259766\n",
      "Iteration: 1050 Loss: 0.09333458542823792 Accuracy: 97.63095092773438\n",
      "Iteration: 1100 Loss: 0.03071644715964794 Accuracy: 97.79762268066406\n",
      "Iteration: 1150 Loss: 0.02578941360116005 Accuracy: 98.07142639160156\n",
      "Iteration: 1200 Loss: 0.03508678451180458 Accuracy: 98.03572082519531\n",
      "Iteration: 1250 Loss: 0.06432081013917923 Accuracy: 97.69047546386719\n",
      "Iteration: 1300 Loss: 0.06046104431152344 Accuracy: 97.95238494873047\n",
      "Iteration: 1350 Loss: 0.12083838135004044 Accuracy: 97.79762268066406\n",
      "Iteration: 1400 Loss: 0.07048109173774719 Accuracy: 97.71428680419922\n",
      "Iteration: 1450 Loss: 0.10729477554559708 Accuracy: 98.16667175292969\n",
      "Iteration: 1500 Loss: 0.04197535663843155 Accuracy: 98.20238494873047\n",
      "Iteration: 1550 Loss: 0.036562152206897736 Accuracy: 97.94047546386719\n",
      "Iteration: 1600 Loss: 0.06304527074098587 Accuracy: 98.30952453613281\n",
      "Iteration: 1650 Loss: 0.058364082127809525 Accuracy: 98.17857360839844\n",
      "Iteration: 1700 Loss: 0.06709910184144974 Accuracy: 98.19047546386719\n",
      "Iteration: 1750 Loss: 0.07001292705535889 Accuracy: 97.98809051513672\n",
      "Iteration: 1800 Loss: 0.05090557411313057 Accuracy: 98.19047546386719\n",
      "Iteration: 1850 Loss: 0.019626887515187263 Accuracy: 98.28571319580078\n",
      "Iteration: 1900 Loss: 0.047181449830532074 Accuracy: 98.25\n",
      "Iteration: 1950 Loss: 0.03393206000328064 Accuracy: 98.08333587646484\n",
      "Iteration: 2000 Loss: 0.10337043553590775 Accuracy: 98.11904907226562\n",
      "Iteration: 2050 Loss: 0.03146495670080185 Accuracy: 98.17857360839844\n",
      "Iteration: 2100 Loss: 0.06313470005989075 Accuracy: 98.19047546386719\n",
      "Iteration: 2150 Loss: 0.05554919317364693 Accuracy: 98.0952377319336\n",
      "Iteration: 2200 Loss: 0.08804576843976974 Accuracy: 98.28571319580078\n",
      "Iteration: 2250 Loss: 0.040248338133096695 Accuracy: 98.19047546386719\n",
      "Iteration: 2300 Loss: 0.014494267292320728 Accuracy: 98.08333587646484\n",
      "Iteration: 2350 Loss: 0.11965303122997284 Accuracy: 98.33333587646484\n",
      "Iteration: 2400 Loss: 0.005165704060345888 Accuracy: 98.44047546386719\n",
      "Iteration: 2450 Loss: 0.02913031540811062 Accuracy: 98.27381134033203\n",
      "Iteration: 2500 Loss: 0.07364849001169205 Accuracy: 98.38095092773438\n",
      "Iteration: 2550 Loss: 0.030766582116484642 Accuracy: 98.33333587646484\n",
      "Iteration: 2600 Loss: 0.021663179621100426 Accuracy: 98.4047622680664\n",
      "Iteration: 2650 Loss: 0.015090945176780224 Accuracy: 98.47618865966797\n",
      "Iteration: 2700 Loss: 0.005398034583777189 Accuracy: 98.54762268066406\n",
      "Iteration: 2750 Loss: 0.05371500179171562 Accuracy: 98.1547622680664\n",
      "Iteration: 2800 Loss: 0.003239894285798073 Accuracy: 98.0952377319336\n",
      "Iteration: 2850 Loss: 0.020951705053448677 Accuracy: 98.44047546386719\n",
      "Iteration: 2900 Loss: 0.0029570648912340403 Accuracy: 98.4047622680664\n",
      "Iteration: 2950 Loss: 0.009617882780730724 Accuracy: 98.20238494873047\n",
      "Iteration: 3000 Loss: 0.009632973931729794 Accuracy: 98.4047622680664\n",
      "Iteration: 3050 Loss: 0.03904036432504654 Accuracy: 98.25\n",
      "Iteration: 3100 Loss: 0.02175041474401951 Accuracy: 98.51190185546875\n",
      "Iteration: 3150 Loss: 0.04749411717057228 Accuracy: 98.42857360839844\n",
      "Iteration: 3200 Loss: 0.04130452498793602 Accuracy: 98.26190185546875\n",
      "Iteration: 3250 Loss: 0.05475793033838272 Accuracy: 98.47618865966797\n",
      "Iteration: 3300 Loss: 0.019526446238160133 Accuracy: 98.36904907226562\n",
      "Iteration: 3350 Loss: 0.0045220982283353806 Accuracy: 98.47618865966797\n",
      "Iteration: 3400 Loss: 0.008718389086425304 Accuracy: 98.51190185546875\n",
      "Iteration: 3450 Loss: 0.011105278506875038 Accuracy: 98.57142639160156\n",
      "Iteration: 3500 Loss: 0.007444323506206274 Accuracy: 98.42857360839844\n",
      "Iteration: 3550 Loss: 0.01916196011006832 Accuracy: 98.44047546386719\n",
      "Iteration: 3600 Loss: 0.041285768151283264 Accuracy: 98.42857360839844\n",
      "Iteration: 3650 Loss: 0.002401383128017187 Accuracy: 98.6547622680664\n",
      "Iteration: 3700 Loss: 0.02016298472881317 Accuracy: 98.5\n",
      "Iteration: 3750 Loss: 0.006596909370273352 Accuracy: 98.5238037109375\n",
      "Iteration: 3800 Loss: 0.0271235890686512 Accuracy: 98.39285278320312\n",
      "Iteration: 3850 Loss: 0.02225474640727043 Accuracy: 98.30952453613281\n",
      "Iteration: 3900 Loss: 0.002272537909448147 Accuracy: 98.30952453613281\n",
      "Iteration: 3950 Loss: 0.009068098850548267 Accuracy: 98.38095092773438\n",
      "Iteration: 4000 Loss: 0.03373079001903534 Accuracy: 98.48809051513672\n",
      "Iteration: 4050 Loss: 0.004241645336151123 Accuracy: 98.58333587646484\n",
      "Iteration: 4100 Loss: 0.05768365412950516 Accuracy: 98.39285278320312\n",
      "Iteration: 4150 Loss: 0.045011430978775024 Accuracy: 98.36904907226562\n",
      "Iteration: 4200 Loss: 0.021508360281586647 Accuracy: 98.55952453613281\n",
      "Iteration: 4250 Loss: 0.011313923634588718 Accuracy: 98.55952453613281\n",
      "Iteration: 4300 Loss: 0.014285269193351269 Accuracy: 98.57142639160156\n",
      "Iteration: 4350 Loss: 0.01892171986401081 Accuracy: 98.48809051513672\n",
      "Iteration: 4400 Loss: 0.013612552545964718 Accuracy: 98.33333587646484\n",
      "Iteration: 4450 Loss: 0.016994250938296318 Accuracy: 98.57142639160156\n",
      "Iteration: 4500 Loss: 0.017912300303578377 Accuracy: 98.46428680419922\n",
      "Iteration: 4550 Loss: 0.00802986603230238 Accuracy: 98.39285278320312\n",
      "Iteration: 4600 Loss: 0.030946852639317513 Accuracy: 98.54762268066406\n",
      "Iteration: 4650 Loss: 0.014159045182168484 Accuracy: 98.64285278320312\n",
      "Iteration: 4700 Loss: 0.007217958103865385 Accuracy: 98.48809051513672\n",
      "Iteration: 4750 Loss: 0.01612081006169319 Accuracy: 98.5\n",
      "Iteration: 4800 Loss: 0.04434039071202278 Accuracy: 98.64285278320312\n",
      "Iteration: 4850 Loss: 0.01961953192949295 Accuracy: 98.58333587646484\n",
      "Iteration: 4900 Loss: 0.006909314543008804 Accuracy: 98.28571319580078\n",
      "Iteration: 4950 Loss: 0.01240457221865654 Accuracy: 98.57142639160156\n",
      "Iteration: 5000 Loss: 0.011048508808016777 Accuracy: 98.60713958740234\n",
      "Iteration: 5050 Loss: 0.02754170447587967 Accuracy: 98.63095092773438\n",
      "Iteration: 5100 Loss: 0.029561452567577362 Accuracy: 98.44047546386719\n",
      "Iteration: 5150 Loss: 0.0056523471139371395 Accuracy: 98.55952453613281\n",
      "Iteration: 5200 Loss: 0.032016269862651825 Accuracy: 98.45238494873047\n",
      "Iteration: 5250 Loss: 0.030403131619095802 Accuracy: 98.69047546386719\n",
      "Iteration: 5300 Loss: 0.005896007642149925 Accuracy: 98.55952453613281\n",
      "Iteration: 5350 Loss: 0.01082872599363327 Accuracy: 98.46428680419922\n",
      "Iteration: 5400 Loss: 0.002616036916151643 Accuracy: 98.42857360839844\n",
      "Iteration: 5450 Loss: 0.009275678545236588 Accuracy: 98.71428680419922\n",
      "Iteration: 5500 Loss: 0.006289641838520765 Accuracy: 98.5952377319336\n",
      "Iteration: 5550 Loss: 0.008841258473694324 Accuracy: 98.42857360839844\n",
      "Iteration: 5600 Loss: 0.0038035279139876366 Accuracy: 98.61904907226562\n",
      "Iteration: 5650 Loss: 0.0012944424524903297 Accuracy: 98.5\n",
      "Iteration: 5700 Loss: 0.02808147855103016 Accuracy: 98.53571319580078\n",
      "Iteration: 5750 Loss: 0.007360207848250866 Accuracy: 98.66667175292969\n",
      "Iteration: 5800 Loss: 0.005051310174167156 Accuracy: 98.67857360839844\n",
      "Iteration: 5850 Loss: 0.016269385814666748 Accuracy: 98.54762268066406\n",
      "Iteration: 5900 Loss: 0.04906603693962097 Accuracy: 98.10713958740234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5950 Loss: 0.027641355991363525 Accuracy: 98.60713958740234\n",
      "Iteration: 6000 Loss: 0.02189931459724903 Accuracy: 98.61904907226562\n",
      "Iteration: 6050 Loss: 0.015017606317996979 Accuracy: 98.55952453613281\n",
      "Iteration: 6100 Loss: 0.0049400050193071365 Accuracy: 98.48809051513672\n",
      "Iteration: 6150 Loss: 0.015988923609256744 Accuracy: 98.47618865966797\n",
      "Iteration: 6200 Loss: 0.03867286071181297 Accuracy: 98.55952453613281\n",
      "Iteration: 6250 Loss: 0.007715906947851181 Accuracy: 98.78571319580078\n",
      "Iteration: 6300 Loss: 0.013117237947881222 Accuracy: 98.69047546386719\n",
      "Iteration: 6350 Loss: 0.016634177416563034 Accuracy: 98.53571319580078\n",
      "Iteration: 6400 Loss: 0.01774750091135502 Accuracy: 98.7738037109375\n",
      "Iteration: 6450 Loss: 0.001557107432745397 Accuracy: 98.67857360839844\n",
      "Iteration: 6500 Loss: 0.006327356677502394 Accuracy: 98.64285278320312\n",
      "Iteration: 6550 Loss: 0.013137435540556908 Accuracy: 98.64285278320312\n",
      "Iteration: 6600 Loss: 0.002303149551153183 Accuracy: 98.69047546386719\n",
      "Iteration: 6650 Loss: 0.005147966090589762 Accuracy: 98.71428680419922\n",
      "Iteration: 6700 Loss: 0.014647655189037323 Accuracy: 98.79762268066406\n",
      "Iteration: 6750 Loss: 0.004669456742703915 Accuracy: 98.46428680419922\n",
      "Iteration: 6800 Loss: 0.013539940118789673 Accuracy: 98.38095092773438\n",
      "Iteration: 6850 Loss: 0.0037168979179114103 Accuracy: 98.7738037109375\n",
      "Iteration: 6900 Loss: 0.010571836493909359 Accuracy: 98.41667175292969\n",
      "Iteration: 6950 Loss: 0.02002374455332756 Accuracy: 98.75\n",
      "Iteration: 7000 Loss: 0.0067720175720751286 Accuracy: 98.73809814453125\n",
      "Iteration: 7050 Loss: 0.005232598632574081 Accuracy: 98.6547622680664\n",
      "Iteration: 7100 Loss: 0.008054888807237148 Accuracy: 98.72618865966797\n",
      "Iteration: 7150 Loss: 0.005951646715402603 Accuracy: 98.7738037109375\n",
      "Iteration: 7200 Loss: 0.014947532676160336 Accuracy: 98.66667175292969\n",
      "Iteration: 7250 Loss: 0.007808675989508629 Accuracy: 98.64285278320312\n",
      "Iteration: 7300 Loss: 0.011558282189071178 Accuracy: 98.7738037109375\n",
      "Iteration: 7350 Loss: 0.0060536907985806465 Accuracy: 98.60713958740234\n",
      "Iteration: 7400 Loss: 0.004297120962291956 Accuracy: 98.8214340209961\n",
      "Iteration: 7450 Loss: 0.02175115793943405 Accuracy: 98.6547622680664\n",
      "Iteration: 7500 Loss: 0.0045389472506940365 Accuracy: 98.61904907226562\n",
      "Iteration: 7550 Loss: 0.012792123481631279 Accuracy: 98.72618865966797\n",
      "Iteration: 7600 Loss: 0.001131342607550323 Accuracy: 98.67857360839844\n",
      "Iteration: 7650 Loss: 0.007622522301971912 Accuracy: 98.66667175292969\n",
      "Iteration: 7700 Loss: 0.002593411598354578 Accuracy: 98.64285278320312\n",
      "Iteration: 7750 Loss: 0.003307523438706994 Accuracy: 98.66667175292969\n",
      "Iteration: 7800 Loss: 0.0020234438125044107 Accuracy: 98.72618865966797\n",
      "Iteration: 7850 Loss: 0.031200533732771873 Accuracy: 98.61904907226562\n",
      "Iteration: 7900 Loss: 0.009987316094338894 Accuracy: 98.63095092773438\n",
      "Iteration: 7950 Loss: 0.03857201710343361 Accuracy: 98.58333587646484\n",
      "Iteration: 8000 Loss: 0.007867368869483471 Accuracy: 98.58333587646484\n",
      "Iteration: 8050 Loss: 0.0054110693745315075 Accuracy: 98.67857360839844\n",
      "Iteration: 8100 Loss: 0.005775759927928448 Accuracy: 98.54762268066406\n",
      "Iteration: 8150 Loss: 0.0012734840856865048 Accuracy: 98.72618865966797\n",
      "Iteration: 8200 Loss: 0.004748087842017412 Accuracy: 98.6547622680664\n",
      "Iteration: 8250 Loss: 0.008772138506174088 Accuracy: 98.7738037109375\n",
      "Iteration: 8300 Loss: 0.0031269663013517857 Accuracy: 98.66667175292969\n",
      "Iteration: 8350 Loss: 0.00646241195499897 Accuracy: 98.78571319580078\n",
      "Iteration: 8400 Loss: 0.001748027978464961 Accuracy: 98.48809051513672\n",
      "Iteration: 8450 Loss: 0.004309787414968014 Accuracy: 98.76190185546875\n",
      "Iteration: 8500 Loss: 0.0018420313717797399 Accuracy: 98.72618865966797\n",
      "Iteration: 8550 Loss: 0.0011177968699485064 Accuracy: 98.76190185546875\n",
      "Iteration: 8600 Loss: 0.05284060165286064 Accuracy: 98.66667175292969\n",
      "Iteration: 8650 Loss: 0.006293664686381817 Accuracy: 98.6547622680664\n",
      "Iteration: 8700 Loss: 0.017702998593449593 Accuracy: 98.61904907226562\n",
      "Iteration: 8750 Loss: 0.0017724798526614904 Accuracy: 98.6547622680664\n",
      "Iteration: 8800 Loss: 0.0076998937875032425 Accuracy: 98.45238494873047\n",
      "Iteration: 8850 Loss: 0.001174464588984847 Accuracy: 98.70238494873047\n",
      "Iteration: 8900 Loss: 0.005344677716493607 Accuracy: 98.5952377319336\n",
      "Iteration: 8950 Loss: 0.008953861892223358 Accuracy: 98.71428680419922\n",
      "Iteration: 9000 Loss: 0.009898386895656586 Accuracy: 98.76190185546875\n",
      "Iteration: 9050 Loss: 0.0022452613338828087 Accuracy: 98.51190185546875\n",
      "Iteration: 9100 Loss: 0.0025574457831680775 Accuracy: 98.61904907226562\n",
      "Iteration: 9150 Loss: 0.0027966315392404795 Accuracy: 98.64285278320312\n",
      "Iteration: 9200 Loss: 0.003921592608094215 Accuracy: 98.60713958740234\n",
      "Iteration: 9250 Loss: 0.0031248650047928095 Accuracy: 98.72618865966797\n",
      "Iteration: 9300 Loss: 0.005880072247236967 Accuracy: 98.7738037109375\n",
      "Iteration: 9350 Loss: 0.0017861004453152418 Accuracy: 98.78571319580078\n",
      "Iteration: 9400 Loss: 0.0031735666561871767 Accuracy: 98.69047546386719\n",
      "Iteration: 9450 Loss: 0.00043893250403925776 Accuracy: 98.67857360839844\n",
      "Iteration: 9500 Loss: 0.019499048590660095 Accuracy: 98.67857360839844\n",
      "Iteration: 9550 Loss: 0.00040778861148282886 Accuracy: 98.69047546386719\n",
      "Iteration: 9600 Loss: 0.00025388365611433983 Accuracy: 98.66667175292969\n",
      "Iteration: 9650 Loss: 0.019224582239985466 Accuracy: 98.44047546386719\n",
      "Iteration: 9700 Loss: 0.0011374590685591102 Accuracy: 98.60713958740234\n",
      "Iteration: 9750 Loss: 0.0008968912879936397 Accuracy: 98.8452377319336\n",
      "Iteration: 9800 Loss: 0.0028086653910577297 Accuracy: 98.72618865966797\n",
      "Iteration: 9850 Loss: 0.00041886791586875916 Accuracy: 98.5952377319336\n",
      "Iteration: 9900 Loss: 0.005255280528217554 Accuracy: 98.73809814453125\n",
      "Iteration: 9950 Loss: 0.0021452410146594048 Accuracy: 98.66667175292969\n",
      "Iteration: 10000 Loss: 0.0014812335139140487 Accuracy: 98.60713958740234\n",
      "Iteration: 10050 Loss: 0.0037518891040235758 Accuracy: 98.75\n",
      "Iteration: 10100 Loss: 0.00792338140308857 Accuracy: 98.73809814453125\n",
      "Iteration: 10150 Loss: 0.0120570482686162 Accuracy: 98.71428680419922\n",
      "Iteration: 10200 Loss: 0.00377245107665658 Accuracy: 98.71428680419922\n",
      "Iteration: 10250 Loss: 0.0015290406299754977 Accuracy: 98.60713958740234\n",
      "Iteration: 10300 Loss: 0.0013698404654860497 Accuracy: 98.69047546386719\n",
      "Iteration: 10350 Loss: 0.04267686977982521 Accuracy: 98.73809814453125\n",
      "Iteration: 10400 Loss: 0.0027374073397368193 Accuracy: 98.60713958740234\n",
      "Iteration: 10450 Loss: 0.00200590118765831 Accuracy: 98.72618865966797\n",
      "Iteration: 10500 Loss: 0.0010801180033013225 Accuracy: 98.76190185546875\n",
      "Iteration: 10550 Loss: 0.008904896676540375 Accuracy: 98.61904907226562\n",
      "Iteration: 10600 Loss: 0.0033356789499521255 Accuracy: 98.67857360839844\n",
      "Iteration: 10650 Loss: 0.0028322008438408375 Accuracy: 98.75\n",
      "Iteration: 10700 Loss: 0.0036765094846487045 Accuracy: 98.7738037109375\n",
      "Iteration: 10750 Loss: 0.0036497872788459063 Accuracy: 98.66667175292969\n",
      "Iteration: 10800 Loss: 0.0025235917419195175 Accuracy: 98.63095092773438\n",
      "Iteration: 10850 Loss: 0.0005238045123405755 Accuracy: 98.67857360839844\n",
      "Iteration: 10900 Loss: 0.0005490535986609757 Accuracy: 98.70238494873047\n",
      "Iteration: 10950 Loss: 0.003833233145996928 Accuracy: 98.72618865966797\n",
      "Iteration: 11000 Loss: 0.0006642404478043318 Accuracy: 98.63095092773438\n",
      "Iteration: 11050 Loss: 0.0002876637445297092 Accuracy: 98.61904907226562\n",
      "Iteration: 11100 Loss: 0.0050073289312422276 Accuracy: 98.69047546386719\n",
      "Iteration: 11150 Loss: 0.0021771297324448824 Accuracy: 98.76190185546875\n",
      "Iteration: 11200 Loss: 0.001830022083595395 Accuracy: 98.6547622680664\n",
      "Iteration: 11250 Loss: 0.0016289596678689122 Accuracy: 98.8214340209961\n",
      "Iteration: 11300 Loss: 0.0012718201614916325 Accuracy: 98.6547622680664\n",
      "Iteration: 11350 Loss: 0.000572195160202682 Accuracy: 98.73809814453125\n",
      "Iteration: 11400 Loss: 0.014837468974292278 Accuracy: 98.70238494873047\n",
      "Iteration: 11450 Loss: 0.0020403768867254257 Accuracy: 98.66667175292969\n",
      "Iteration: 11500 Loss: 0.0025963494554162025 Accuracy: 98.7738037109375\n",
      "Iteration: 11550 Loss: 0.002137454692274332 Accuracy: 98.73809814453125\n",
      "Iteration: 11600 Loss: 0.0019381834426894784 Accuracy: 98.75\n",
      "Iteration: 11650 Loss: 0.009297380223870277 Accuracy: 98.6547622680664\n",
      "Iteration: 11700 Loss: 0.0047638900578022 Accuracy: 98.70238494873047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 11750 Loss: 0.0029409865383058786 Accuracy: 98.48809051513672\n",
      "Iteration: 11800 Loss: 0.0010862653143703938 Accuracy: 98.69047546386719\n",
      "Iteration: 11850 Loss: 0.0012756967917084694 Accuracy: 98.75\n",
      "Iteration: 11900 Loss: 0.005264424253255129 Accuracy: 98.67857360839844\n",
      "Iteration: 11950 Loss: 0.002867859322577715 Accuracy: 98.83333587646484\n",
      "Iteration: 12000 Loss: 0.0054250252433121204 Accuracy: 98.85713958740234\n",
      "Iteration: 12050 Loss: 0.001357360975816846 Accuracy: 98.83333587646484\n",
      "Iteration: 12100 Loss: 0.003559747012332082 Accuracy: 98.66667175292969\n",
      "Iteration: 12150 Loss: 0.00034900844912044704 Accuracy: 98.73809814453125\n",
      "Iteration: 12200 Loss: 0.0017802503425627947 Accuracy: 98.8214340209961\n",
      "Iteration: 12250 Loss: 0.0015700726071372628 Accuracy: 98.8214340209961\n",
      "Iteration: 12300 Loss: 0.0009665997349657118 Accuracy: 98.70238494873047\n",
      "Iteration: 12350 Loss: 0.0010119457729160786 Accuracy: 98.79762268066406\n",
      "Iteration: 12400 Loss: 0.0010120071237906814 Accuracy: 98.61904907226562\n",
      "Iteration: 12450 Loss: 0.002084471983835101 Accuracy: 98.8214340209961\n",
      "Iteration: 12500 Loss: 0.00020613361266441643 Accuracy: 98.75\n",
      "Iteration: 12550 Loss: 0.0018995527643710375 Accuracy: 98.76190185546875\n",
      "Iteration: 12600 Loss: 0.004892888944596052 Accuracy: 98.76190185546875\n",
      "Iteration: 12650 Loss: 0.001595190609805286 Accuracy: 98.80952453613281\n",
      "Iteration: 12700 Loss: 0.0015581020852550864 Accuracy: 98.7738037109375\n",
      "Iteration: 12750 Loss: 0.0009018986020237207 Accuracy: 98.61904907226562\n",
      "Iteration: 12800 Loss: 0.000673899136018008 Accuracy: 98.75\n",
      "Iteration: 12850 Loss: 0.0009166538948193192 Accuracy: 98.72618865966797\n",
      "Iteration: 12900 Loss: 0.00047712179366499186 Accuracy: 98.78571319580078\n",
      "Iteration: 12950 Loss: 0.0019716122187674046 Accuracy: 98.79762268066406\n",
      "Iteration: 13000 Loss: 0.005005250684916973 Accuracy: 98.5952377319336\n",
      "Iteration: 13050 Loss: 0.006143853068351746 Accuracy: 98.8452377319336\n",
      "Iteration: 13100 Loss: 0.0001454492739867419 Accuracy: 98.8214340209961\n",
      "Iteration: 13150 Loss: 9.439750283490866e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 13200 Loss: 0.00041206617606803775 Accuracy: 98.83333587646484\n",
      "Iteration: 13250 Loss: 0.0006553315906785429 Accuracy: 98.76190185546875\n",
      "Iteration: 13300 Loss: 0.00025265279691666365 Accuracy: 98.71428680419922\n",
      "Iteration: 13350 Loss: 0.0007916922331787646 Accuracy: 98.66667175292969\n",
      "Iteration: 13400 Loss: 0.000887820206116885 Accuracy: 98.76190185546875\n",
      "Iteration: 13450 Loss: 0.0035958632361143827 Accuracy: 98.79762268066406\n",
      "Iteration: 13500 Loss: 0.003501832252368331 Accuracy: 98.75\n",
      "Iteration: 13550 Loss: 0.0003580260672606528 Accuracy: 98.79762268066406\n",
      "Iteration: 13600 Loss: 0.0004054511373396963 Accuracy: 98.79762268066406\n",
      "Iteration: 13650 Loss: 0.0009296966600231826 Accuracy: 98.80952453613281\n",
      "Iteration: 13700 Loss: 0.005839594639837742 Accuracy: 98.73809814453125\n",
      "Iteration: 13750 Loss: 0.0037173228338360786 Accuracy: 98.80952453613281\n",
      "Iteration: 13800 Loss: 0.002581096487119794 Accuracy: 98.76190185546875\n",
      "Iteration: 13850 Loss: 5.510968549060635e-05 Accuracy: 98.75\n",
      "Iteration: 13900 Loss: 0.001304440083913505 Accuracy: 98.72618865966797\n",
      "Iteration: 13950 Loss: 0.010018233209848404 Accuracy: 98.72618865966797\n",
      "Iteration: 14000 Loss: 0.00048511047498323023 Accuracy: 98.76190185546875\n",
      "Iteration: 14050 Loss: 0.0025503060314804316 Accuracy: 98.73809814453125\n",
      "Iteration: 14100 Loss: 0.00047108065336942673 Accuracy: 98.76190185546875\n",
      "Iteration: 14150 Loss: 0.0030872179195284843 Accuracy: 98.7738037109375\n",
      "Iteration: 14200 Loss: 0.002612360520288348 Accuracy: 98.73809814453125\n",
      "Iteration: 14250 Loss: 0.0008242233307100832 Accuracy: 98.7738037109375\n",
      "Iteration: 14300 Loss: 0.0026597105897963047 Accuracy: 98.73809814453125\n",
      "Iteration: 14350 Loss: 0.0013784756883978844 Accuracy: 98.71428680419922\n",
      "Iteration: 14400 Loss: 0.01589033007621765 Accuracy: 98.78571319580078\n",
      "Iteration: 14450 Loss: 0.0012994625139981508 Accuracy: 98.7738037109375\n",
      "Iteration: 14500 Loss: 0.0012492825044319034 Accuracy: 98.79762268066406\n",
      "Iteration: 14550 Loss: 0.001080926856957376 Accuracy: 98.53571319580078\n",
      "Iteration: 14600 Loss: 0.0011376967886462808 Accuracy: 98.78571319580078\n",
      "Iteration: 14650 Loss: 0.00014041968097444624 Accuracy: 98.7738037109375\n",
      "Iteration: 14700 Loss: 0.00015720323426648974 Accuracy: 98.70238494873047\n",
      "Iteration: 14750 Loss: 0.0023117694072425365 Accuracy: 98.8214340209961\n",
      "Iteration: 14800 Loss: 0.00033925511525012553 Accuracy: 98.7738037109375\n",
      "Iteration: 14850 Loss: 0.0018864700105041265 Accuracy: 98.78571319580078\n",
      "Iteration: 14900 Loss: 0.0012694880133494735 Accuracy: 98.8214340209961\n",
      "Iteration: 14950 Loss: 0.0010939425555989146 Accuracy: 98.7738037109375\n",
      "Iteration: 15000 Loss: 0.002342804567888379 Accuracy: 98.78571319580078\n",
      "Iteration: 15050 Loss: 0.003008119994774461 Accuracy: 98.73809814453125\n",
      "Iteration: 15100 Loss: 0.0009287417633458972 Accuracy: 98.76190185546875\n",
      "Iteration: 15150 Loss: 0.0008112514624372125 Accuracy: 98.76190185546875\n",
      "Iteration: 15200 Loss: 0.0016981593798846006 Accuracy: 98.78571319580078\n",
      "Iteration: 15250 Loss: 0.0007267592009156942 Accuracy: 98.73809814453125\n",
      "Iteration: 15300 Loss: 0.0008689776877872646 Accuracy: 98.71428680419922\n",
      "Iteration: 15350 Loss: 0.000735726032871753 Accuracy: 98.73809814453125\n",
      "Iteration: 15400 Loss: 0.0031039961613714695 Accuracy: 98.76190185546875\n",
      "Iteration: 15450 Loss: 0.000459998962469399 Accuracy: 98.73809814453125\n",
      "Iteration: 15500 Loss: 0.0014958020765334368 Accuracy: 98.75\n",
      "Iteration: 15550 Loss: 0.00044268963392823935 Accuracy: 98.79762268066406\n",
      "Iteration: 15600 Loss: 0.0014730001566931605 Accuracy: 98.80952453613281\n",
      "Iteration: 15650 Loss: 0.0018965628696605563 Accuracy: 98.7738037109375\n",
      "Iteration: 15700 Loss: 0.001047209370881319 Accuracy: 98.85713958740234\n",
      "Iteration: 15750 Loss: 0.0016331544611603022 Accuracy: 98.75\n",
      "Iteration: 15800 Loss: 0.002600838430225849 Accuracy: 98.69047546386719\n",
      "Iteration: 15850 Loss: 7.833273411961272e-05 Accuracy: 98.8214340209961\n",
      "Iteration: 15900 Loss: 0.0001594108180142939 Accuracy: 98.8214340209961\n",
      "Iteration: 15950 Loss: 0.0009845849126577377 Accuracy: 98.79762268066406\n",
      "Iteration: 16000 Loss: 0.0007592565380036831 Accuracy: 98.67857360839844\n",
      "Iteration: 16050 Loss: 0.00045802697422914207 Accuracy: 98.63095092773438\n",
      "Iteration: 16100 Loss: 0.0017965836450457573 Accuracy: 98.79762268066406\n",
      "Iteration: 16150 Loss: 0.0001638250396354124 Accuracy: 98.7738037109375\n",
      "Iteration: 16200 Loss: 0.0005434653721749783 Accuracy: 98.8214340209961\n",
      "Iteration: 16250 Loss: 0.0005224584019742906 Accuracy: 98.73809814453125\n",
      "Iteration: 16300 Loss: 0.0012835338711738586 Accuracy: 98.76190185546875\n",
      "Iteration: 16350 Loss: 0.0013195930514484644 Accuracy: 98.79762268066406\n",
      "Iteration: 16400 Loss: 0.001456890138797462 Accuracy: 98.80952453613281\n",
      "Iteration: 16450 Loss: 0.0003284982230979949 Accuracy: 98.72618865966797\n",
      "Iteration: 16500 Loss: 0.000407189509132877 Accuracy: 98.73809814453125\n",
      "Iteration: 16550 Loss: 0.0011373148299753666 Accuracy: 98.78571319580078\n",
      "Iteration: 16600 Loss: 0.001951613579876721 Accuracy: 98.6547622680664\n",
      "Iteration: 16650 Loss: 0.0017320991028100252 Accuracy: 98.79762268066406\n",
      "Iteration: 16700 Loss: 0.0016446469817310572 Accuracy: 98.79762268066406\n",
      "Iteration: 16750 Loss: 0.00015928516222629696 Accuracy: 98.76190185546875\n",
      "Iteration: 16800 Loss: 0.002213386818766594 Accuracy: 98.80952453613281\n",
      "Iteration: 16850 Loss: 0.0009354000794701278 Accuracy: 98.8452377319336\n",
      "Iteration: 16900 Loss: 0.0012173765571787953 Accuracy: 98.73809814453125\n",
      "Iteration: 16950 Loss: 4.1600003896746784e-05 Accuracy: 98.80952453613281\n",
      "Iteration: 17000 Loss: 0.0002473797358106822 Accuracy: 98.76190185546875\n",
      "Iteration: 17050 Loss: 0.00036722488584928215 Accuracy: 98.79762268066406\n",
      "Iteration: 17100 Loss: 0.0005013226764276624 Accuracy: 98.79762268066406\n",
      "Iteration: 17150 Loss: 0.004767489153891802 Accuracy: 98.70238494873047\n",
      "Iteration: 17200 Loss: 0.0002208359946962446 Accuracy: 98.78571319580078\n",
      "Iteration: 17250 Loss: 0.0005752731231041253 Accuracy: 98.75\n",
      "Iteration: 17300 Loss: 0.0005830192822031677 Accuracy: 98.71428680419922\n",
      "Iteration: 17350 Loss: 0.0007393715786747634 Accuracy: 98.79762268066406\n",
      "Iteration: 17400 Loss: 0.0016535205068066716 Accuracy: 98.7738037109375\n",
      "Iteration: 17450 Loss: 0.0004708711931016296 Accuracy: 98.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 17500 Loss: 0.00018013427325058728 Accuracy: 98.7738037109375\n",
      "Iteration: 17550 Loss: 0.0016694676596671343 Accuracy: 98.75\n",
      "Iteration: 17600 Loss: 0.002765500918030739 Accuracy: 98.80952453613281\n",
      "Iteration: 17650 Loss: 0.00016654097998980433 Accuracy: 98.83333587646484\n",
      "Iteration: 17700 Loss: 0.00023652684467379004 Accuracy: 98.8214340209961\n",
      "Iteration: 17750 Loss: 0.00022598556824959815 Accuracy: 98.76190185546875\n",
      "Iteration: 17800 Loss: 0.0007761815213598311 Accuracy: 98.75\n",
      "Iteration: 17850 Loss: 0.0010479340562596917 Accuracy: 98.66667175292969\n",
      "Iteration: 17900 Loss: 0.00136629119515419 Accuracy: 98.80952453613281\n",
      "Iteration: 17950 Loss: 0.0024704942479729652 Accuracy: 98.75\n",
      "Iteration: 18000 Loss: 0.00022321482538245618 Accuracy: 98.78571319580078\n",
      "Iteration: 18050 Loss: 0.0007413840503431857 Accuracy: 98.79762268066406\n",
      "Iteration: 18100 Loss: 0.0005726471426896751 Accuracy: 98.80952453613281\n",
      "Iteration: 18150 Loss: 0.0010272891959175467 Accuracy: 98.75\n",
      "Iteration: 18200 Loss: 0.00028937309980392456 Accuracy: 98.7738037109375\n",
      "Iteration: 18250 Loss: 0.001177097437903285 Accuracy: 98.78571319580078\n",
      "Iteration: 18300 Loss: 0.00032083806581795216 Accuracy: 98.7738037109375\n",
      "Iteration: 18350 Loss: 0.0003789472102653235 Accuracy: 98.76190185546875\n",
      "Iteration: 18400 Loss: 0.0006501646130345762 Accuracy: 98.72618865966797\n",
      "Iteration: 18450 Loss: 0.00012302794493734837 Accuracy: 98.83333587646484\n",
      "Iteration: 18500 Loss: 0.0035869386047124863 Accuracy: 98.76190185546875\n",
      "Iteration: 18550 Loss: 0.00021675246534869075 Accuracy: 98.7738037109375\n",
      "Iteration: 18600 Loss: 0.0010186751605942845 Accuracy: 98.76190185546875\n",
      "Iteration: 18650 Loss: 0.00018584034114610404 Accuracy: 98.7738037109375\n",
      "Iteration: 18700 Loss: 0.0007518528145737946 Accuracy: 98.78571319580078\n",
      "Iteration: 18750 Loss: 0.0005894515779800713 Accuracy: 98.73809814453125\n",
      "Iteration: 18800 Loss: 0.0002970012719742954 Accuracy: 98.73809814453125\n",
      "Iteration: 18850 Loss: 0.0005069379694759846 Accuracy: 98.7738037109375\n",
      "Iteration: 18900 Loss: 0.0006162307690829039 Accuracy: 98.79762268066406\n",
      "Iteration: 18950 Loss: 0.00032758404267951846 Accuracy: 98.75\n",
      "Iteration: 19000 Loss: 0.000571483513340354 Accuracy: 98.76190185546875\n",
      "Iteration: 19050 Loss: 0.0005580948782153428 Accuracy: 98.83333587646484\n",
      "Iteration: 19100 Loss: 0.0007395228021778166 Accuracy: 98.7738037109375\n",
      "Iteration: 19150 Loss: 0.0004806546785403043 Accuracy: 98.7738037109375\n",
      "Iteration: 19200 Loss: 0.0005378656787797809 Accuracy: 98.80952453613281\n",
      "Iteration: 19250 Loss: 0.0010693484218791127 Accuracy: 98.8214340209961\n",
      "Iteration: 19300 Loss: 0.001407541916705668 Accuracy: 98.8452377319336\n",
      "Iteration: 19350 Loss: 0.00028933631256222725 Accuracy: 98.79762268066406\n",
      "Iteration: 19400 Loss: 0.0002609641815070063 Accuracy: 98.76190185546875\n",
      "Iteration: 19450 Loss: 0.00022314440866466612 Accuracy: 98.73809814453125\n",
      "Iteration: 19500 Loss: 0.00040731357876211405 Accuracy: 98.75\n",
      "Iteration: 19550 Loss: 0.0009455215185880661 Accuracy: 98.76190185546875\n",
      "Iteration: 19600 Loss: 0.00022477212769445032 Accuracy: 98.79762268066406\n",
      "Iteration: 19650 Loss: 0.0020460898522287607 Accuracy: 98.79762268066406\n",
      "Iteration: 19700 Loss: 0.00022849295055493712 Accuracy: 98.71428680419922\n",
      "Iteration: 19750 Loss: 0.0004917809274047613 Accuracy: 98.7738037109375\n",
      "Iteration: 19800 Loss: 0.0004016502934973687 Accuracy: 98.80952453613281\n",
      "Iteration: 19850 Loss: 0.0006893014651723206 Accuracy: 98.79762268066406\n",
      "Iteration: 19900 Loss: 6.231399311218411e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 19950 Loss: 0.0038589739706367254 Accuracy: 98.8214340209961\n",
      "Iteration: 20000 Loss: 0.00025882426416501403 Accuracy: 98.8214340209961\n",
      "Iteration: 20050 Loss: 9.668964048614725e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 20100 Loss: 0.0003112046397291124 Accuracy: 98.73809814453125\n",
      "Iteration: 20150 Loss: 0.0008889191085472703 Accuracy: 98.7738037109375\n",
      "Iteration: 20200 Loss: 0.0013401384931057692 Accuracy: 98.75\n",
      "Iteration: 20250 Loss: 0.0012810772750526667 Accuracy: 98.79762268066406\n",
      "Iteration: 20300 Loss: 1.2825130397686735e-05 Accuracy: 98.80952453613281\n",
      "Iteration: 20350 Loss: 0.0006663557724095881 Accuracy: 98.76190185546875\n",
      "Iteration: 20400 Loss: 0.0010134264593943954 Accuracy: 98.73809814453125\n",
      "Iteration: 20450 Loss: 0.0003858281415887177 Accuracy: 98.69047546386719\n",
      "Iteration: 20500 Loss: 0.00033585820347070694 Accuracy: 98.75\n",
      "Iteration: 20550 Loss: 0.0008249065140262246 Accuracy: 98.79762268066406\n",
      "Iteration: 20600 Loss: 0.0007859517354518175 Accuracy: 98.79762268066406\n",
      "Iteration: 20650 Loss: 9.941721509676427e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 20700 Loss: 0.001313702785409987 Accuracy: 98.79762268066406\n",
      "Iteration: 20750 Loss: 0.0001424904039595276 Accuracy: 98.8214340209961\n",
      "Iteration: 20800 Loss: 0.0005486862501129508 Accuracy: 98.76190185546875\n",
      "Iteration: 20850 Loss: 0.0009726229473017156 Accuracy: 98.75\n",
      "Iteration: 20900 Loss: 0.0006990911206230521 Accuracy: 98.80952453613281\n",
      "Iteration: 20950 Loss: 0.00016873481217771769 Accuracy: 98.79762268066406\n",
      "Iteration: 21000 Loss: 0.0005938763497397304 Accuracy: 98.73809814453125\n",
      "Iteration: 21050 Loss: 0.0006248276913538575 Accuracy: 98.71428680419922\n",
      "Iteration: 21100 Loss: 4.815814463654533e-06 Accuracy: 98.76190185546875\n",
      "Iteration: 21150 Loss: 0.0005276480223983526 Accuracy: 98.7738037109375\n",
      "Iteration: 21200 Loss: 0.0002254417195217684 Accuracy: 98.76190185546875\n",
      "Iteration: 21250 Loss: 0.00011932312190765515 Accuracy: 98.7738037109375\n",
      "Iteration: 21300 Loss: 0.00033624214120209217 Accuracy: 98.72618865966797\n",
      "Iteration: 21350 Loss: 0.0006123183411546052 Accuracy: 98.80952453613281\n",
      "Iteration: 21400 Loss: 0.0005736607708968222 Accuracy: 98.78571319580078\n",
      "Iteration: 21450 Loss: 0.0001274437236133963 Accuracy: 98.76190185546875\n",
      "Iteration: 21500 Loss: 0.0022360633593052626 Accuracy: 98.76190185546875\n",
      "Iteration: 21550 Loss: 0.00011389434075681493 Accuracy: 98.7738037109375\n",
      "Iteration: 21600 Loss: 0.0006094067357480526 Accuracy: 98.75\n",
      "Iteration: 21650 Loss: 0.001041334355250001 Accuracy: 98.7738037109375\n",
      "Iteration: 21700 Loss: 0.00046457763528451324 Accuracy: 98.73809814453125\n",
      "Iteration: 21750 Loss: 0.0005872374167665839 Accuracy: 98.69047546386719\n",
      "Iteration: 21800 Loss: 0.0008513040957041085 Accuracy: 98.79762268066406\n",
      "Iteration: 21850 Loss: 0.00025869032833725214 Accuracy: 98.7738037109375\n",
      "Iteration: 21900 Loss: 0.0003330458712298423 Accuracy: 98.80952453613281\n",
      "Iteration: 21950 Loss: 9.061829041456804e-05 Accuracy: 98.79762268066406\n",
      "Iteration: 22000 Loss: 6.885237235110253e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 22050 Loss: 0.0006151648121885955 Accuracy: 98.7738037109375\n",
      "Iteration: 22100 Loss: 0.00010841631592484191 Accuracy: 98.80952453613281\n",
      "Iteration: 22150 Loss: 0.00010885485244216397 Accuracy: 98.78571319580078\n",
      "Iteration: 22200 Loss: 0.0001684138405835256 Accuracy: 98.80952453613281\n",
      "Iteration: 22250 Loss: 0.0006163783837109804 Accuracy: 98.78571319580078\n",
      "Iteration: 22300 Loss: 0.0006820466369390488 Accuracy: 98.80952453613281\n",
      "Iteration: 22350 Loss: 0.0001570840395288542 Accuracy: 98.79762268066406\n",
      "Iteration: 22400 Loss: 0.00013462206698022783 Accuracy: 98.7738037109375\n",
      "Iteration: 22450 Loss: 4.740628719446249e-05 Accuracy: 98.80952453613281\n",
      "Iteration: 22500 Loss: 0.0003678242792375386 Accuracy: 98.76190185546875\n",
      "Iteration: 22550 Loss: 9.285435953643173e-05 Accuracy: 98.80952453613281\n",
      "Iteration: 22600 Loss: 0.0002578976273071021 Accuracy: 98.79762268066406\n",
      "Iteration: 22650 Loss: 2.5912015189533122e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 22700 Loss: 0.0004969415604136884 Accuracy: 98.7738037109375\n",
      "Iteration: 22750 Loss: 0.0009471526136621833 Accuracy: 98.8214340209961\n",
      "Iteration: 22800 Loss: 0.0010656228987500072 Accuracy: 98.78571319580078\n",
      "Iteration: 22850 Loss: 0.00027632841374725103 Accuracy: 98.73809814453125\n",
      "Iteration: 22900 Loss: 0.00019036026787944138 Accuracy: 98.75\n",
      "Iteration: 22950 Loss: 0.000307139300275594 Accuracy: 98.79762268066406\n",
      "Iteration: 23000 Loss: 0.0007500358042307198 Accuracy: 98.76190185546875\n",
      "Iteration: 23050 Loss: 0.0005382979870773852 Accuracy: 98.7738037109375\n",
      "Iteration: 23100 Loss: 0.0009229402057826519 Accuracy: 98.80952453613281\n",
      "Iteration: 23150 Loss: 0.0002513217623345554 Accuracy: 98.78571319580078\n",
      "Iteration: 23200 Loss: 0.00011748528049793094 Accuracy: 98.76190185546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 23250 Loss: 0.0002337265614187345 Accuracy: 98.80952453613281\n",
      "Iteration: 23300 Loss: 6.558885070262477e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 23350 Loss: 5.382185918278992e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 23400 Loss: 4.076713139511412e-06 Accuracy: 98.78571319580078\n",
      "Iteration: 23450 Loss: 0.00023133549257181585 Accuracy: 98.75\n",
      "Iteration: 23500 Loss: 0.00047715046093799174 Accuracy: 98.76190185546875\n",
      "Iteration: 23550 Loss: 0.0002971137291751802 Accuracy: 98.76190185546875\n",
      "Iteration: 23600 Loss: 6.73773029120639e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 23650 Loss: 2.849671545845922e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 23700 Loss: 0.0002792087325360626 Accuracy: 98.78571319580078\n",
      "Iteration: 23750 Loss: 0.00035050782025791705 Accuracy: 98.80952453613281\n",
      "Iteration: 23800 Loss: 0.00018903480668086559 Accuracy: 98.79762268066406\n",
      "Iteration: 23850 Loss: 0.0002231947291875258 Accuracy: 98.79762268066406\n",
      "Iteration: 23900 Loss: 0.0009898905409500003 Accuracy: 98.76190185546875\n",
      "Iteration: 23950 Loss: 0.00022544237435795367 Accuracy: 98.7738037109375\n",
      "Iteration: 24000 Loss: 7.369686500169337e-05 Accuracy: 98.8214340209961\n",
      "Iteration: 24050 Loss: 0.00029048690339550376 Accuracy: 98.7738037109375\n",
      "Iteration: 24100 Loss: 0.0006106593064032495 Accuracy: 98.80952453613281\n",
      "Iteration: 24150 Loss: 0.00017185449542012066 Accuracy: 98.78571319580078\n",
      "Iteration: 24200 Loss: 0.00026723791961558163 Accuracy: 98.7738037109375\n",
      "Iteration: 24250 Loss: 0.0005579973221756518 Accuracy: 98.7738037109375\n",
      "Iteration: 24300 Loss: 0.001330747501924634 Accuracy: 98.78571319580078\n",
      "Iteration: 24350 Loss: 0.00034972053254023194 Accuracy: 98.78571319580078\n",
      "Iteration: 24400 Loss: 8.652185351820663e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 24450 Loss: 0.0004626015143003315 Accuracy: 98.78571319580078\n",
      "Iteration: 24500 Loss: 0.00012476534175220877 Accuracy: 98.76190185546875\n",
      "Iteration: 24550 Loss: 0.00016953577869571745 Accuracy: 98.79762268066406\n",
      "Iteration: 24600 Loss: 0.0007034330628812313 Accuracy: 98.7738037109375\n",
      "Iteration: 24650 Loss: 0.0003187799593433738 Accuracy: 98.80952453613281\n",
      "Iteration: 24700 Loss: 0.0009807070018723607 Accuracy: 98.76190185546875\n",
      "Iteration: 24750 Loss: 0.00029801338678225875 Accuracy: 98.76190185546875\n",
      "Iteration: 24800 Loss: 0.0005614222609438002 Accuracy: 98.79762268066406\n",
      "Iteration: 24850 Loss: 0.0003913666005246341 Accuracy: 98.78571319580078\n",
      "Iteration: 24900 Loss: 0.0009362029959447682 Accuracy: 98.7738037109375\n",
      "Iteration: 24950 Loss: 0.0002746629761531949 Accuracy: 98.78571319580078\n",
      "Iteration: 25000 Loss: 0.0002628221409395337 Accuracy: 98.71428680419922\n",
      "Iteration: 25050 Loss: 0.0004484134551603347 Accuracy: 98.75\n",
      "Iteration: 25100 Loss: 0.0017839474603533745 Accuracy: 98.75\n",
      "Iteration: 25150 Loss: 0.000165665042004548 Accuracy: 98.7738037109375\n",
      "Iteration: 25200 Loss: 0.00035902525996789336 Accuracy: 98.7738037109375\n",
      "Iteration: 25250 Loss: 9.339827374788001e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 25300 Loss: 0.0007204868015833199 Accuracy: 98.80952453613281\n",
      "Iteration: 25350 Loss: 0.0001759519800543785 Accuracy: 98.76190185546875\n",
      "Iteration: 25400 Loss: 0.00010026837117038667 Accuracy: 98.78571319580078\n",
      "Iteration: 25450 Loss: 0.00018105554045177996 Accuracy: 98.7738037109375\n",
      "Iteration: 25500 Loss: 0.002154973801225424 Accuracy: 98.79762268066406\n",
      "Iteration: 25550 Loss: 0.0002989782951772213 Accuracy: 98.75\n",
      "Iteration: 25600 Loss: 0.0006384716252796352 Accuracy: 98.8214340209961\n",
      "Iteration: 25650 Loss: 0.0005392148159444332 Accuracy: 98.75\n",
      "Iteration: 25700 Loss: 0.00034859313745982945 Accuracy: 98.7738037109375\n",
      "Iteration: 25750 Loss: 0.00018255699251312762 Accuracy: 98.75\n",
      "Iteration: 25800 Loss: 0.0009095737477764487 Accuracy: 98.76190185546875\n",
      "Iteration: 25850 Loss: 0.00022860724129714072 Accuracy: 98.73809814453125\n",
      "Iteration: 25900 Loss: 0.00022471467673312873 Accuracy: 98.80952453613281\n",
      "Iteration: 25950 Loss: 0.0005104396259412169 Accuracy: 98.76190185546875\n",
      "Iteration: 26000 Loss: 0.00046373403165489435 Accuracy: 98.78571319580078\n",
      "Iteration: 26050 Loss: 0.0002458425296936184 Accuracy: 98.7738037109375\n",
      "Iteration: 26100 Loss: 0.0006617084727622569 Accuracy: 98.76190185546875\n",
      "Iteration: 26150 Loss: 0.0004537386412266642 Accuracy: 98.73809814453125\n",
      "Iteration: 26200 Loss: 0.0004575204220600426 Accuracy: 98.76190185546875\n",
      "Iteration: 26250 Loss: 0.00043367742910049856 Accuracy: 98.78571319580078\n",
      "Iteration: 26300 Loss: 0.0006098494050092995 Accuracy: 98.7738037109375\n",
      "Iteration: 26350 Loss: 0.000568036048207432 Accuracy: 98.79762268066406\n",
      "Iteration: 26400 Loss: 0.0005131175857968628 Accuracy: 98.78571319580078\n",
      "Iteration: 26450 Loss: 9.687103010946885e-05 Accuracy: 98.80952453613281\n",
      "Iteration: 26500 Loss: 0.00018163843196816742 Accuracy: 98.7738037109375\n",
      "Iteration: 26550 Loss: 0.00023042602697387338 Accuracy: 98.75\n",
      "Iteration: 26600 Loss: 0.000330400827806443 Accuracy: 98.7738037109375\n",
      "Iteration: 26650 Loss: 7.207409362308681e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 26700 Loss: 0.000527719093952328 Accuracy: 98.73809814453125\n",
      "Iteration: 26750 Loss: 8.008542499737814e-05 Accuracy: 98.75\n",
      "Iteration: 26800 Loss: 0.0003544398641679436 Accuracy: 98.79762268066406\n",
      "Iteration: 26850 Loss: 0.0006084105116315186 Accuracy: 98.76190185546875\n",
      "Iteration: 26900 Loss: 0.0001912438456201926 Accuracy: 98.7738037109375\n",
      "Iteration: 26950 Loss: 0.0005031650653108954 Accuracy: 98.76190185546875\n",
      "Iteration: 27000 Loss: 0.00018425121379550546 Accuracy: 98.80952453613281\n",
      "Iteration: 27050 Loss: 0.0002899005776271224 Accuracy: 98.79762268066406\n",
      "Iteration: 27100 Loss: 0.000496396329253912 Accuracy: 98.78571319580078\n",
      "Iteration: 27150 Loss: 6.428093911381438e-06 Accuracy: 98.7738037109375\n",
      "Iteration: 27200 Loss: 0.00024149710952769965 Accuracy: 98.72618865966797\n",
      "Iteration: 27250 Loss: 0.0002241150796180591 Accuracy: 98.78571319580078\n",
      "Iteration: 27300 Loss: 2.3564829461975023e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 27350 Loss: 0.00012307953147683293 Accuracy: 98.79762268066406\n",
      "Iteration: 27400 Loss: 0.000163684380822815 Accuracy: 98.76190185546875\n",
      "Iteration: 27450 Loss: 0.0004078277852386236 Accuracy: 98.76190185546875\n",
      "Iteration: 27500 Loss: 0.000154812732944265 Accuracy: 98.75\n",
      "Iteration: 27550 Loss: 0.00046465828199870884 Accuracy: 98.76190185546875\n",
      "Iteration: 27600 Loss: 0.0010441216873005033 Accuracy: 98.7738037109375\n",
      "Iteration: 27650 Loss: 0.00014458624355029315 Accuracy: 98.73809814453125\n",
      "Iteration: 27700 Loss: 0.00016720168059691787 Accuracy: 98.73809814453125\n",
      "Iteration: 27750 Loss: 0.00042760014184750617 Accuracy: 98.7738037109375\n",
      "Iteration: 27800 Loss: 0.00028346467297524214 Accuracy: 98.7738037109375\n",
      "Iteration: 27850 Loss: 0.00027701459475792944 Accuracy: 98.7738037109375\n",
      "Iteration: 27900 Loss: 0.00039959847345016897 Accuracy: 98.7738037109375\n",
      "Iteration: 27950 Loss: 0.0003935752029065043 Accuracy: 98.76190185546875\n",
      "Iteration: 28000 Loss: 9.455934923607856e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 28050 Loss: 0.00016371917445212603 Accuracy: 98.78571319580078\n",
      "Iteration: 28100 Loss: 0.0003575989103410393 Accuracy: 98.73809814453125\n",
      "Iteration: 28150 Loss: 0.00017104863945860416 Accuracy: 98.7738037109375\n",
      "Iteration: 28200 Loss: 9.822376887314022e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 28250 Loss: 0.0003685964911710471 Accuracy: 98.7738037109375\n",
      "Iteration: 28300 Loss: 0.00019992992747575045 Accuracy: 98.76190185546875\n",
      "Iteration: 28350 Loss: 0.0006171121494844556 Accuracy: 98.76190185546875\n",
      "Iteration: 28400 Loss: 0.0003403777373023331 Accuracy: 98.75\n",
      "Iteration: 28450 Loss: 0.00017339711484964937 Accuracy: 98.76190185546875\n",
      "Iteration: 28500 Loss: 0.0003811461792793125 Accuracy: 98.76190185546875\n",
      "Iteration: 28550 Loss: 0.00020655145635828376 Accuracy: 98.78571319580078\n",
      "Iteration: 28600 Loss: 3.955617648898624e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 28650 Loss: 0.0003179423219989985 Accuracy: 98.7738037109375\n",
      "Iteration: 28700 Loss: 0.0006083850748836994 Accuracy: 98.75\n",
      "Iteration: 28750 Loss: 0.0003244766849093139 Accuracy: 98.76190185546875\n",
      "Iteration: 28800 Loss: 0.00018668478878680617 Accuracy: 98.79762268066406\n",
      "Iteration: 28850 Loss: 7.137432839954272e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 28900 Loss: 7.74409418227151e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 28950 Loss: 0.0006459980504587293 Accuracy: 98.76190185546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 29000 Loss: 0.00037746370071545243 Accuracy: 98.75\n",
      "Iteration: 29050 Loss: 9.140421752817929e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 29100 Loss: 0.00047910542343743145 Accuracy: 98.75\n",
      "Iteration: 29150 Loss: 6.832724466221407e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 29200 Loss: 0.0005563936429098248 Accuracy: 98.7738037109375\n",
      "Iteration: 29250 Loss: 0.0005024626152589917 Accuracy: 98.73809814453125\n",
      "Iteration: 29300 Loss: 0.0001399938337272033 Accuracy: 98.7738037109375\n",
      "Iteration: 29350 Loss: 0.00014252682740334421 Accuracy: 98.73809814453125\n",
      "Iteration: 29400 Loss: 7.274517702171579e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 29450 Loss: 0.0001678870467003435 Accuracy: 98.79762268066406\n",
      "Iteration: 29500 Loss: 0.00012757693184539676 Accuracy: 98.78571319580078\n",
      "Iteration: 29550 Loss: 0.00023625158064533025 Accuracy: 98.7738037109375\n",
      "Iteration: 29600 Loss: 9.565742948325351e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 29650 Loss: 0.00017678299627732486 Accuracy: 98.75\n",
      "Iteration: 29700 Loss: 0.0004519705835264176 Accuracy: 98.76190185546875\n",
      "Iteration: 29750 Loss: 1.9898039681720547e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 29800 Loss: 0.00027912596124224365 Accuracy: 98.78571319580078\n",
      "Iteration: 29850 Loss: 0.00034423917531967163 Accuracy: 98.75\n",
      "Iteration: 29900 Loss: 0.0004781425232067704 Accuracy: 98.75\n",
      "Iteration: 29950 Loss: 0.0005098156398162246 Accuracy: 98.75\n",
      "Iteration: 30000 Loss: 0.0005475236685015261 Accuracy: 98.76190185546875\n",
      "Iteration: 30050 Loss: 0.0001830453402362764 Accuracy: 98.76190185546875\n",
      "Iteration: 30100 Loss: 0.00038412288995459676 Accuracy: 98.76190185546875\n",
      "Iteration: 30150 Loss: 0.00015988429368007928 Accuracy: 98.78571319580078\n",
      "Iteration: 30200 Loss: 0.00010557821224210784 Accuracy: 98.7738037109375\n",
      "Iteration: 30250 Loss: 0.00016357461572624743 Accuracy: 98.79762268066406\n",
      "Iteration: 30300 Loss: 4.21891309088096e-05 Accuracy: 98.75\n",
      "Iteration: 30350 Loss: 4.831860860576853e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 30400 Loss: 0.0012867935001850128 Accuracy: 98.75\n",
      "Iteration: 30450 Loss: 0.00017388768901582807 Accuracy: 98.7738037109375\n",
      "Iteration: 30500 Loss: 0.00023442672681994736 Accuracy: 98.76190185546875\n",
      "Iteration: 30550 Loss: 0.0010243570432066917 Accuracy: 98.73809814453125\n",
      "Iteration: 30600 Loss: 0.0003760225954465568 Accuracy: 98.75\n",
      "Iteration: 30650 Loss: 1.1793114026659168e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 30700 Loss: 9.179723565466702e-05 Accuracy: 98.75\n",
      "Iteration: 30750 Loss: 5.648770820698701e-05 Accuracy: 98.75\n",
      "Iteration: 30800 Loss: 1.7809601331464364e-06 Accuracy: 98.76190185546875\n",
      "Iteration: 30850 Loss: 0.0015086281346157193 Accuracy: 98.78571319580078\n",
      "Iteration: 30900 Loss: 4.543176692095585e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 30950 Loss: 0.00040309381438419223 Accuracy: 98.7738037109375\n",
      "Iteration: 31000 Loss: 0.00030231670825742185 Accuracy: 98.76190185546875\n",
      "Iteration: 31050 Loss: 0.00028918206226080656 Accuracy: 98.75\n",
      "Iteration: 31100 Loss: 0.0001482009975006804 Accuracy: 98.75\n",
      "Iteration: 31150 Loss: 0.00019048835383728147 Accuracy: 98.76190185546875\n",
      "Iteration: 31200 Loss: 0.0001349375961581245 Accuracy: 98.76190185546875\n",
      "Iteration: 31250 Loss: 0.0004858463362324983 Accuracy: 98.78571319580078\n",
      "Iteration: 31300 Loss: 0.0004249229677952826 Accuracy: 98.78571319580078\n",
      "Iteration: 31350 Loss: 0.0003421455912757665 Accuracy: 98.78571319580078\n",
      "Iteration: 31400 Loss: 0.0004221206472720951 Accuracy: 98.7738037109375\n",
      "Iteration: 31450 Loss: 0.0003311652399133891 Accuracy: 98.7738037109375\n",
      "Iteration: 31500 Loss: 5.300010889186524e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 31550 Loss: 0.00029964803252369165 Accuracy: 98.78571319580078\n",
      "Iteration: 31600 Loss: 0.00020994599617552012 Accuracy: 98.7738037109375\n",
      "Iteration: 31650 Loss: 0.000212794155231677 Accuracy: 98.79762268066406\n",
      "Iteration: 31700 Loss: 0.00017404236132279038 Accuracy: 98.73809814453125\n",
      "Iteration: 31750 Loss: 0.00027089909417554736 Accuracy: 98.7738037109375\n",
      "Iteration: 31800 Loss: 0.0002765762328635901 Accuracy: 98.75\n",
      "Iteration: 31850 Loss: 0.00011989229096798226 Accuracy: 98.73809814453125\n",
      "Iteration: 31900 Loss: 0.00030226598028093576 Accuracy: 98.7738037109375\n",
      "Iteration: 31950 Loss: 8.302970672957599e-05 Accuracy: 98.75\n",
      "Iteration: 32000 Loss: 0.0005234564305283129 Accuracy: 98.75\n",
      "Iteration: 32050 Loss: 0.0002661322650965303 Accuracy: 98.75\n",
      "Iteration: 32100 Loss: 0.0001666127354837954 Accuracy: 98.76190185546875\n",
      "Iteration: 32150 Loss: 2.462710290274117e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 32200 Loss: 0.00014994954108260572 Accuracy: 98.76190185546875\n",
      "Iteration: 32250 Loss: 0.0002813938190229237 Accuracy: 98.76190185546875\n",
      "Iteration: 32300 Loss: 1.1412477761041373e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 32350 Loss: 0.0002129901258740574 Accuracy: 98.7738037109375\n",
      "Iteration: 32400 Loss: 0.00021729616855736822 Accuracy: 98.73809814453125\n",
      "Iteration: 32450 Loss: 0.00017408441635780036 Accuracy: 98.71428680419922\n",
      "Iteration: 32500 Loss: 0.0005098333349451423 Accuracy: 98.78571319580078\n",
      "Iteration: 32550 Loss: 0.0007548641297034919 Accuracy: 98.73809814453125\n",
      "Iteration: 32600 Loss: 0.0001293785171583295 Accuracy: 98.7738037109375\n",
      "Iteration: 32650 Loss: 0.00015957342111505568 Accuracy: 98.75\n",
      "Iteration: 32700 Loss: 0.0003938463341910392 Accuracy: 98.7738037109375\n",
      "Iteration: 32750 Loss: 4.6668665163451806e-06 Accuracy: 98.7738037109375\n",
      "Iteration: 32800 Loss: 0.00039322819793596864 Accuracy: 98.76190185546875\n",
      "Iteration: 32850 Loss: 0.00026754382997751236 Accuracy: 98.7738037109375\n",
      "Iteration: 32900 Loss: 0.00026956916553899646 Accuracy: 98.7738037109375\n",
      "Iteration: 32950 Loss: 0.0001867424725787714 Accuracy: 98.76190185546875\n",
      "Iteration: 33000 Loss: 0.00038280480657704175 Accuracy: 98.76190185546875\n",
      "Iteration: 33050 Loss: 9.83188147074543e-06 Accuracy: 98.7738037109375\n",
      "Iteration: 33100 Loss: 8.278217137558386e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 33150 Loss: 0.00021270285651553422 Accuracy: 98.73809814453125\n",
      "Iteration: 33200 Loss: 1.206008528242819e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 33250 Loss: 0.00037672166945412755 Accuracy: 98.75\n",
      "Iteration: 33300 Loss: 0.00022965802054386586 Accuracy: 98.76190185546875\n",
      "Iteration: 33350 Loss: 0.0003422845620661974 Accuracy: 98.76190185546875\n",
      "Iteration: 33400 Loss: 0.0001691963116172701 Accuracy: 98.76190185546875\n",
      "Iteration: 33450 Loss: 3.9887658203952014e-05 Accuracy: 98.75\n",
      "Iteration: 33500 Loss: 0.000306467292830348 Accuracy: 98.72618865966797\n",
      "Iteration: 33550 Loss: 0.00034870131639763713 Accuracy: 98.72618865966797\n",
      "Iteration: 33600 Loss: 0.00014940992696210742 Accuracy: 98.75\n",
      "Iteration: 33650 Loss: 0.0002916497178375721 Accuracy: 98.73809814453125\n",
      "Iteration: 33700 Loss: 9.811668860493228e-05 Accuracy: 98.75\n",
      "Iteration: 33750 Loss: 4.08093401347287e-05 Accuracy: 98.75\n",
      "Iteration: 33800 Loss: 0.0005767781403847039 Accuracy: 98.76190185546875\n",
      "Iteration: 33850 Loss: 0.00021758304501418024 Accuracy: 98.75\n",
      "Iteration: 33900 Loss: 0.0001372089609503746 Accuracy: 98.78571319580078\n",
      "Iteration: 33950 Loss: 0.000334821263095364 Accuracy: 98.73809814453125\n",
      "Iteration: 34000 Loss: 0.00017527778982184827 Accuracy: 98.76190185546875\n",
      "Iteration: 34050 Loss: 0.000100719764304813 Accuracy: 98.75\n",
      "Iteration: 34100 Loss: 3.7276287912391126e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 34150 Loss: 0.00047268636990338564 Accuracy: 98.76190185546875\n",
      "Iteration: 34200 Loss: 0.00036524140159599483 Accuracy: 98.76190185546875\n",
      "Iteration: 34250 Loss: 0.00021584318892564625 Accuracy: 98.76190185546875\n",
      "Iteration: 34300 Loss: 9.231289004674181e-05 Accuracy: 98.75\n",
      "Iteration: 34350 Loss: 1.0485830898687709e-05 Accuracy: 98.75\n",
      "Iteration: 34400 Loss: 0.0003106946824118495 Accuracy: 98.73809814453125\n",
      "Iteration: 34450 Loss: 0.0001971589372260496 Accuracy: 98.72618865966797\n",
      "Iteration: 34500 Loss: 0.0001186990921269171 Accuracy: 98.76190185546875\n",
      "Iteration: 34550 Loss: 0.00016324686293955892 Accuracy: 98.76190185546875\n",
      "Iteration: 34600 Loss: 0.0004587313160300255 Accuracy: 98.7738037109375\n",
      "Iteration: 34650 Loss: 0.0003141486959066242 Accuracy: 98.73809814453125\n",
      "Iteration: 34700 Loss: 7.101498340489343e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 34750 Loss: 0.00018888298654928803 Accuracy: 98.75\n",
      "Iteration: 34800 Loss: 0.00028720658156089485 Accuracy: 98.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 34850 Loss: 0.0004233326471876353 Accuracy: 98.7738037109375\n",
      "Iteration: 34900 Loss: 0.0003254823386669159 Accuracy: 98.78571319580078\n",
      "Iteration: 34950 Loss: 0.00016206962754949927 Accuracy: 98.75\n",
      "Iteration: 35000 Loss: 0.0001834419963415712 Accuracy: 98.78571319580078\n",
      "Iteration: 35050 Loss: 2.9514496418414637e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 35100 Loss: 0.0004482779768295586 Accuracy: 98.76190185546875\n",
      "Iteration: 35150 Loss: 0.0002380798541707918 Accuracy: 98.73809814453125\n",
      "Iteration: 35200 Loss: 5.808329296996817e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 35250 Loss: 3.99676246161107e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 35300 Loss: 0.0001477629557484761 Accuracy: 98.75\n",
      "Iteration: 35350 Loss: 0.00011334984446875751 Accuracy: 98.76190185546875\n",
      "Iteration: 35400 Loss: 0.0001046623001457192 Accuracy: 98.7738037109375\n",
      "Iteration: 35450 Loss: 0.0006927458452992141 Accuracy: 98.7738037109375\n",
      "Iteration: 35500 Loss: 0.00016720885469112545 Accuracy: 98.76190185546875\n",
      "Iteration: 35550 Loss: 0.0001545230916235596 Accuracy: 98.75\n",
      "Iteration: 35600 Loss: 0.00017117940296884626 Accuracy: 98.7738037109375\n",
      "Iteration: 35650 Loss: 0.0001531561865704134 Accuracy: 98.75\n",
      "Iteration: 35700 Loss: 0.00022475168225355446 Accuracy: 98.75\n",
      "Iteration: 35750 Loss: 0.00030455022351816297 Accuracy: 98.76190185546875\n",
      "Iteration: 35800 Loss: 0.00010386558278696612 Accuracy: 98.76190185546875\n",
      "Iteration: 35850 Loss: 0.0002368799177929759 Accuracy: 98.76190185546875\n",
      "Iteration: 35900 Loss: 6.535858119605109e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 35950 Loss: 0.0001475563767598942 Accuracy: 98.7738037109375\n",
      "Iteration: 36000 Loss: 1.5921934391371906e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 36050 Loss: 2.595321711851284e-05 Accuracy: 98.75\n",
      "Iteration: 36100 Loss: 5.955389133305289e-05 Accuracy: 98.75\n",
      "Iteration: 36150 Loss: 0.00010554465552559122 Accuracy: 98.75\n",
      "Iteration: 36200 Loss: 0.0011295501608401537 Accuracy: 98.75\n",
      "Iteration: 36250 Loss: 0.0007921291398815811 Accuracy: 98.75\n",
      "Iteration: 36300 Loss: 1.202805378852645e-06 Accuracy: 98.76190185546875\n",
      "Iteration: 36350 Loss: 9.509163646725938e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 36400 Loss: 0.0008484381251037121 Accuracy: 98.7738037109375\n",
      "Iteration: 36450 Loss: 0.00040640789666213095 Accuracy: 98.75\n",
      "Iteration: 36500 Loss: 0.00030904493178240955 Accuracy: 98.75\n",
      "Iteration: 36550 Loss: 0.00025837577413767576 Accuracy: 98.78571319580078\n",
      "Iteration: 36600 Loss: 0.0001046380857587792 Accuracy: 98.76190185546875\n",
      "Iteration: 36650 Loss: 0.00013220323307905346 Accuracy: 98.72618865966797\n",
      "Iteration: 36700 Loss: 6.248037243494764e-05 Accuracy: 98.75\n",
      "Iteration: 36750 Loss: 0.00017394118185620755 Accuracy: 98.75\n",
      "Iteration: 36800 Loss: 9.420641436008736e-05 Accuracy: 98.75\n",
      "Iteration: 36850 Loss: 9.483569010626525e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 36900 Loss: 0.00014960944827180356 Accuracy: 98.73809814453125\n",
      "Iteration: 36950 Loss: 0.0006061259191483259 Accuracy: 98.73809814453125\n",
      "Iteration: 37000 Loss: 9.125674841925502e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 37050 Loss: 0.00014426845882553607 Accuracy: 98.76190185546875\n",
      "Iteration: 37100 Loss: 0.00023826137476135045 Accuracy: 98.73809814453125\n",
      "Iteration: 37150 Loss: 0.00021772865147795528 Accuracy: 98.75\n",
      "Iteration: 37200 Loss: 0.0001869587431428954 Accuracy: 98.76190185546875\n",
      "Iteration: 37250 Loss: 7.867729436839e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 37300 Loss: 0.00016088536358438432 Accuracy: 98.75\n",
      "Iteration: 37350 Loss: 0.0001657808170421049 Accuracy: 98.7738037109375\n",
      "Iteration: 37400 Loss: 0.00019901158520951867 Accuracy: 98.76190185546875\n",
      "Iteration: 37450 Loss: 0.0003087012155447155 Accuracy: 98.72618865966797\n",
      "Iteration: 37500 Loss: 0.00037827796768397093 Accuracy: 98.73809814453125\n",
      "Iteration: 37550 Loss: 0.000277332728728652 Accuracy: 98.7738037109375\n",
      "Iteration: 37600 Loss: 0.00021898711565881968 Accuracy: 98.7738037109375\n",
      "Iteration: 37650 Loss: 0.00025944100343622267 Accuracy: 98.73809814453125\n",
      "Iteration: 37700 Loss: 6.619731721002609e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 37750 Loss: 0.0003037677670363337 Accuracy: 98.75\n",
      "Iteration: 37800 Loss: 0.0004825085343327373 Accuracy: 98.75\n",
      "Iteration: 37850 Loss: 0.00021777609072159976 Accuracy: 98.76190185546875\n",
      "Iteration: 37900 Loss: 0.00021013710647821426 Accuracy: 98.78571319580078\n",
      "Iteration: 37950 Loss: 0.00016835499263834208 Accuracy: 98.7738037109375\n",
      "Iteration: 38000 Loss: 0.0006273928447626531 Accuracy: 98.72618865966797\n",
      "Iteration: 38050 Loss: 0.00015879381680861115 Accuracy: 98.72618865966797\n",
      "Iteration: 38100 Loss: 0.0003262507088948041 Accuracy: 98.7738037109375\n",
      "Iteration: 38150 Loss: 0.0002576609840616584 Accuracy: 98.76190185546875\n",
      "Iteration: 38200 Loss: 0.0004461632634047419 Accuracy: 98.7738037109375\n",
      "Iteration: 38250 Loss: 0.00039262959035113454 Accuracy: 98.75\n",
      "Iteration: 38300 Loss: 0.00011475498467916623 Accuracy: 98.75\n",
      "Iteration: 38350 Loss: 8.668578630022239e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 38400 Loss: 0.00029958688537590206 Accuracy: 98.73809814453125\n",
      "Iteration: 38450 Loss: 0.00027177605079486966 Accuracy: 98.75\n",
      "Iteration: 38500 Loss: 9.356561349704862e-05 Accuracy: 98.75\n",
      "Iteration: 38550 Loss: 2.294792648172006e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 38600 Loss: 1.5584828361170366e-05 Accuracy: 98.75\n",
      "Iteration: 38650 Loss: 0.00025607645511627197 Accuracy: 98.76190185546875\n",
      "Iteration: 38700 Loss: 7.697429100517184e-05 Accuracy: 98.75\n",
      "Iteration: 38750 Loss: 0.0001135967395384796 Accuracy: 98.76190185546875\n",
      "Iteration: 38800 Loss: 0.000290661962935701 Accuracy: 98.73809814453125\n",
      "Iteration: 38850 Loss: 0.00029117814847268164 Accuracy: 98.76190185546875\n",
      "Iteration: 38900 Loss: 0.00033483695005998015 Accuracy: 98.73809814453125\n",
      "Iteration: 38950 Loss: 0.00036637127050198615 Accuracy: 98.76190185546875\n",
      "Iteration: 39000 Loss: 0.00021949880465399474 Accuracy: 98.76190185546875\n",
      "Iteration: 39050 Loss: 1.986645656870678e-05 Accuracy: 98.75\n",
      "Iteration: 39100 Loss: 0.00015401403652504086 Accuracy: 98.72618865966797\n",
      "Iteration: 39150 Loss: 0.00016930291894823313 Accuracy: 98.73809814453125\n",
      "Iteration: 39200 Loss: 0.00026432660524733365 Accuracy: 98.75\n",
      "Iteration: 39250 Loss: 0.00023021287051960826 Accuracy: 98.76190185546875\n",
      "Iteration: 39300 Loss: 0.00013575989578384906 Accuracy: 98.78571319580078\n",
      "Iteration: 39350 Loss: 0.00030728834099136293 Accuracy: 98.73809814453125\n",
      "Iteration: 39400 Loss: 0.00028487812960520387 Accuracy: 98.7738037109375\n",
      "Iteration: 39450 Loss: 8.240115130320191e-05 Accuracy: 98.75\n",
      "Iteration: 39500 Loss: 0.0005059660761617124 Accuracy: 98.78571319580078\n",
      "Iteration: 39550 Loss: 0.00020211281662341207 Accuracy: 98.7738037109375\n",
      "Iteration: 39600 Loss: 0.0001794087147573009 Accuracy: 98.75\n",
      "Iteration: 39650 Loss: 2.994920578203164e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 39700 Loss: 0.00014096044469624758 Accuracy: 98.76190185546875\n",
      "Iteration: 39750 Loss: 4.061574145453051e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 39800 Loss: 0.00017728922830428928 Accuracy: 98.72618865966797\n",
      "Iteration: 39850 Loss: 3.8059890357544646e-05 Accuracy: 98.75\n",
      "Iteration: 39900 Loss: 0.00010070845746668056 Accuracy: 98.73809814453125\n",
      "Iteration: 39950 Loss: 0.00010729178029578179 Accuracy: 98.75\n",
      "Iteration: 40000 Loss: 0.0003112439007963985 Accuracy: 98.75\n",
      "Iteration: 40050 Loss: 4.886904207523912e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 40100 Loss: 0.00011232527322135866 Accuracy: 98.75\n",
      "Iteration: 40150 Loss: 9.18960286071524e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 40200 Loss: 5.365578908822499e-05 Accuracy: 98.75\n",
      "Iteration: 40250 Loss: 0.00019503096700645983 Accuracy: 98.75\n",
      "Iteration: 40300 Loss: 0.0001762078027240932 Accuracy: 98.78571319580078\n",
      "Iteration: 40350 Loss: 1.407844251843926e-06 Accuracy: 98.7738037109375\n",
      "Iteration: 40400 Loss: 4.1509014408802614e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 40450 Loss: 0.00030365411657840014 Accuracy: 98.78571319580078\n",
      "Iteration: 40500 Loss: 3.05684152408503e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 40550 Loss: 0.00013811548706144094 Accuracy: 98.78571319580078\n",
      "Iteration: 40600 Loss: 0.00011056065704906359 Accuracy: 98.75\n",
      "Iteration: 40650 Loss: 0.00022630322200711817 Accuracy: 98.75\n",
      "Iteration: 40700 Loss: 1.7226540876436047e-05 Accuracy: 98.75\n",
      "Iteration: 40750 Loss: 0.00015641261416021734 Accuracy: 98.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 40800 Loss: 0.00010992137686116621 Accuracy: 98.76190185546875\n",
      "Iteration: 40850 Loss: 8.712668932275847e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 40900 Loss: 6.031922907823173e-07 Accuracy: 98.75\n",
      "Iteration: 40950 Loss: 1.3947860679763835e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 41000 Loss: 0.00021051467047072947 Accuracy: 98.76190185546875\n",
      "Iteration: 41050 Loss: 0.00027826614677906036 Accuracy: 98.73809814453125\n",
      "Iteration: 41100 Loss: 0.00013224194117356092 Accuracy: 98.72618865966797\n",
      "Iteration: 41150 Loss: 0.0005344362580217421 Accuracy: 98.76190185546875\n",
      "Iteration: 41200 Loss: 3.4512697311583906e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 41250 Loss: 6.498092989204451e-05 Accuracy: 98.75\n",
      "Iteration: 41300 Loss: 0.0001756143319653347 Accuracy: 98.7738037109375\n",
      "Iteration: 41350 Loss: 8.969168993644416e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 41400 Loss: 1.299498944717925e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 41450 Loss: 0.0003176747995894402 Accuracy: 98.75\n",
      "Iteration: 41500 Loss: 1.3520322681870311e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 41550 Loss: 9.628119005355984e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 41600 Loss: 7.73458305047825e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 41650 Loss: 0.00010630235919961706 Accuracy: 98.7738037109375\n",
      "Iteration: 41700 Loss: 3.524009298416786e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 41750 Loss: 0.0001907214755192399 Accuracy: 98.75\n",
      "Iteration: 41800 Loss: 6.035151091055013e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 41850 Loss: 0.00013276381650939584 Accuracy: 98.75\n",
      "Iteration: 41900 Loss: 6.022877641953528e-05 Accuracy: 98.78571319580078\n",
      "Iteration: 41950 Loss: 0.00026898752548731863 Accuracy: 98.76190185546875\n",
      "Iteration: 42000 Loss: 6.385431333910674e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 42050 Loss: 0.00019218049419578165 Accuracy: 98.73809814453125\n",
      "Iteration: 42100 Loss: 5.2501403843052685e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 42150 Loss: 0.00022119157074484974 Accuracy: 98.72618865966797\n",
      "Iteration: 42200 Loss: 0.00021279258362483233 Accuracy: 98.7738037109375\n",
      "Iteration: 42250 Loss: 5.225754648563452e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 42300 Loss: 0.00026133147184737027 Accuracy: 98.7738037109375\n",
      "Iteration: 42350 Loss: 0.0001436741295037791 Accuracy: 98.75\n",
      "Iteration: 42400 Loss: 6.444664904847741e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 42450 Loss: 0.0002825401024892926 Accuracy: 98.7738037109375\n",
      "Iteration: 42500 Loss: 2.1715854018111713e-05 Accuracy: 98.75\n",
      "Iteration: 42550 Loss: 0.00016114488244056702 Accuracy: 98.7738037109375\n",
      "Iteration: 42600 Loss: 0.00020962301641702652 Accuracy: 98.73809814453125\n",
      "Iteration: 42650 Loss: 4.1248087654821575e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 42700 Loss: 8.349562267540023e-05 Accuracy: 98.75\n",
      "Iteration: 42750 Loss: 0.0003439043357502669 Accuracy: 98.73809814453125\n",
      "Iteration: 42800 Loss: 2.512526771170087e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 42850 Loss: 4.998984877602197e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 42900 Loss: 6.893005775054917e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 42950 Loss: 3.488402217044495e-05 Accuracy: 98.75\n",
      "Iteration: 43000 Loss: 9.889702050713822e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 43050 Loss: 0.00011871429160237312 Accuracy: 98.73809814453125\n",
      "Iteration: 43100 Loss: 0.000151850312249735 Accuracy: 98.73809814453125\n",
      "Iteration: 43150 Loss: 8.179175347322598e-05 Accuracy: 98.75\n",
      "Iteration: 43200 Loss: 5.247944500297308e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 43250 Loss: 4.4177682866575196e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 43300 Loss: 7.424925570376217e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 43350 Loss: 7.643795106559992e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 43400 Loss: 1.0924874914053362e-05 Accuracy: 98.75\n",
      "Iteration: 43450 Loss: 0.00017099078104365617 Accuracy: 98.73809814453125\n",
      "Iteration: 43500 Loss: 0.00010800399468280375 Accuracy: 98.7738037109375\n",
      "Iteration: 43550 Loss: 3.565664519555867e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 43600 Loss: 0.00022013460693415254 Accuracy: 98.73809814453125\n",
      "Iteration: 43650 Loss: 0.0002663748455233872 Accuracy: 98.72618865966797\n",
      "Iteration: 43700 Loss: 0.00018423417350277305 Accuracy: 98.7738037109375\n",
      "Iteration: 43750 Loss: 0.00013840895553585142 Accuracy: 98.78571319580078\n",
      "Iteration: 43800 Loss: 0.0006658511119894683 Accuracy: 98.72618865966797\n",
      "Iteration: 43850 Loss: 0.0008348140981979668 Accuracy: 98.76190185546875\n",
      "Iteration: 43900 Loss: 1.6435205907328054e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 43950 Loss: 7.196921069407836e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 44000 Loss: 3.0002665880601853e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 44050 Loss: 0.0002643469488248229 Accuracy: 98.73809814453125\n",
      "Iteration: 44100 Loss: 0.00017675805429462343 Accuracy: 98.75\n",
      "Iteration: 44150 Loss: 3.1083753128768876e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 44200 Loss: 3.770486364373937e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 44250 Loss: 0.00013586042041424662 Accuracy: 98.7738037109375\n",
      "Iteration: 44300 Loss: 0.00020456960191950202 Accuracy: 98.75\n",
      "Iteration: 44350 Loss: 6.91642562742345e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 44400 Loss: 0.0001703825982986018 Accuracy: 98.75\n",
      "Iteration: 44450 Loss: 7.48279198887758e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 44500 Loss: 0.0003077595611102879 Accuracy: 98.73809814453125\n",
      "Iteration: 44550 Loss: 0.00016979489009827375 Accuracy: 98.75\n",
      "Iteration: 44600 Loss: 0.00013547785056289285 Accuracy: 98.75\n",
      "Iteration: 44650 Loss: 0.00015981470642145723 Accuracy: 98.76190185546875\n",
      "Iteration: 44700 Loss: 0.0003334651992190629 Accuracy: 98.7738037109375\n",
      "Iteration: 44750 Loss: 0.00021564947383012623 Accuracy: 98.72618865966797\n",
      "Iteration: 44800 Loss: 0.00014960215776227415 Accuracy: 98.75\n",
      "Iteration: 44850 Loss: 3.678047505673021e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 44900 Loss: 0.00014963852299842983 Accuracy: 98.75\n",
      "Iteration: 44950 Loss: 0.00020650507940445095 Accuracy: 98.75\n",
      "Iteration: 45000 Loss: 6.859833229100332e-05 Accuracy: 98.75\n",
      "Iteration: 45050 Loss: 6.636673060711473e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 45100 Loss: 0.0002181938907597214 Accuracy: 98.75\n",
      "Iteration: 45150 Loss: 3.476363781373948e-05 Accuracy: 98.75\n",
      "Iteration: 45200 Loss: 0.00016051586135290563 Accuracy: 98.72618865966797\n",
      "Iteration: 45250 Loss: 0.00034174518077634275 Accuracy: 98.75\n",
      "Iteration: 45300 Loss: 4.34868925367482e-05 Accuracy: 98.75\n",
      "Iteration: 45350 Loss: 4.110375448362902e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 45400 Loss: 6.18275735178031e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 45450 Loss: 0.0001745058543747291 Accuracy: 98.75\n",
      "Iteration: 45500 Loss: 0.00018708157585933805 Accuracy: 98.75\n",
      "Iteration: 45550 Loss: 0.0001517338096164167 Accuracy: 98.76190185546875\n",
      "Iteration: 45600 Loss: 0.000162855998496525 Accuracy: 98.73809814453125\n",
      "Iteration: 45650 Loss: 0.00011323498620186001 Accuracy: 98.76190185546875\n",
      "Iteration: 45700 Loss: 0.00015770284517202526 Accuracy: 98.72618865966797\n",
      "Iteration: 45750 Loss: 2.3666018023504876e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 45800 Loss: 0.00012493281974457204 Accuracy: 98.75\n",
      "Iteration: 45850 Loss: 7.537349301856011e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 45900 Loss: 3.535682481015101e-05 Accuracy: 98.75\n",
      "Iteration: 45950 Loss: 0.0001688169431872666 Accuracy: 98.75\n",
      "Iteration: 46000 Loss: 0.00023166384198702872 Accuracy: 98.75\n",
      "Iteration: 46050 Loss: 0.00015248167619574815 Accuracy: 98.75\n",
      "Iteration: 46100 Loss: 7.696969259995967e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 46150 Loss: 0.00012997335579711944 Accuracy: 98.75\n",
      "Iteration: 46200 Loss: 4.151479879510589e-05 Accuracy: 98.75\n",
      "Iteration: 46250 Loss: 0.00012126327783335 Accuracy: 98.75\n",
      "Iteration: 46300 Loss: 0.00010754594404716045 Accuracy: 98.75\n",
      "Iteration: 46350 Loss: 0.00016157286881934851 Accuracy: 98.73809814453125\n",
      "Iteration: 46400 Loss: 6.622770888498053e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 46450 Loss: 0.0001330372178927064 Accuracy: 98.72618865966797\n",
      "Iteration: 46500 Loss: 8.717059972696006e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 46550 Loss: 0.0001121826862799935 Accuracy: 98.76190185546875\n",
      "Iteration: 46600 Loss: 0.00012020311260130256 Accuracy: 98.76190185546875\n",
      "Iteration: 46650 Loss: 6.52725066174753e-05 Accuracy: 98.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 46700 Loss: 0.00011333376460243016 Accuracy: 98.75\n",
      "Iteration: 46750 Loss: 6.272709288168699e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 46800 Loss: 0.00015651952708140016 Accuracy: 98.76190185546875\n",
      "Iteration: 46850 Loss: 0.00018278272182215005 Accuracy: 98.75\n",
      "Iteration: 46900 Loss: 0.00022124749375507236 Accuracy: 98.73809814453125\n",
      "Iteration: 46950 Loss: 4.6533918066415936e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 47000 Loss: 1.547965621284675e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 47050 Loss: 4.413334681885317e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 47100 Loss: 5.645429700962268e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 47150 Loss: 1.5908010027487762e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 47200 Loss: 4.16475677411654e-06 Accuracy: 98.78571319580078\n",
      "Iteration: 47250 Loss: 0.00011150266800541431 Accuracy: 98.7738037109375\n",
      "Iteration: 47300 Loss: 0.00025259001995436847 Accuracy: 98.75\n",
      "Iteration: 47350 Loss: 0.00010844060307135805 Accuracy: 98.75\n",
      "Iteration: 47400 Loss: 1.0498609299247619e-05 Accuracy: 98.75\n",
      "Iteration: 47450 Loss: 0.00010647867020452395 Accuracy: 98.75\n",
      "Iteration: 47500 Loss: 8.519792754668742e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 47550 Loss: 7.772963726893067e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 47600 Loss: 0.00010089781426358968 Accuracy: 98.72618865966797\n",
      "Iteration: 47650 Loss: 2.0498033336480148e-05 Accuracy: 98.75\n",
      "Iteration: 47700 Loss: 0.0006988186505623162 Accuracy: 98.75\n",
      "Iteration: 47750 Loss: 1.8571790860733017e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 47800 Loss: 0.00016134438919834793 Accuracy: 98.76190185546875\n",
      "Iteration: 47850 Loss: 0.00020992316422052681 Accuracy: 98.73809814453125\n",
      "Iteration: 47900 Loss: 0.00030684872763231397 Accuracy: 98.75\n",
      "Iteration: 47950 Loss: 0.0003094557032454759 Accuracy: 98.72618865966797\n",
      "Iteration: 48000 Loss: 9.703727118903771e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 48050 Loss: 5.4173033277038485e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 48100 Loss: 0.00021351933537516743 Accuracy: 98.75\n",
      "Iteration: 48150 Loss: 7.851654663681984e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 48200 Loss: 9.043219324667007e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 48250 Loss: 2.7278083507553674e-05 Accuracy: 98.75\n",
      "Iteration: 48300 Loss: 0.00016160501400008798 Accuracy: 98.75\n",
      "Iteration: 48350 Loss: 2.8660992029472254e-05 Accuracy: 98.75\n",
      "Iteration: 48400 Loss: 1.8733982869889587e-05 Accuracy: 98.79762268066406\n",
      "Iteration: 48450 Loss: 4.744229227071628e-05 Accuracy: 98.75\n",
      "Iteration: 48500 Loss: 8.711466216482222e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 48550 Loss: 3.102029222645797e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 48600 Loss: 7.405958604067564e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 48650 Loss: 0.00010659412509994581 Accuracy: 98.76190185546875\n",
      "Iteration: 48700 Loss: 0.00022570531291421503 Accuracy: 98.75\n",
      "Iteration: 48750 Loss: 0.00018573671695776284 Accuracy: 98.75\n",
      "Iteration: 48800 Loss: 0.00012277750647626817 Accuracy: 98.73809814453125\n",
      "Iteration: 48850 Loss: 0.0004185129364486784 Accuracy: 98.75\n",
      "Iteration: 48900 Loss: 0.00020018160284962505 Accuracy: 98.73809814453125\n",
      "Iteration: 48950 Loss: 9.261276863981038e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 49000 Loss: 2.7416004741098732e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 49050 Loss: 0.0003036644193343818 Accuracy: 98.75\n",
      "Iteration: 49100 Loss: 0.0001679299894021824 Accuracy: 98.7738037109375\n",
      "Iteration: 49150 Loss: 4.226955206831917e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 49200 Loss: 1.601742951606866e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 49250 Loss: 0.00014305597869679332 Accuracy: 98.7738037109375\n",
      "Iteration: 49300 Loss: 8.270367834484205e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 49350 Loss: 3.131756602670066e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 49400 Loss: 1.9500665075611323e-05 Accuracy: 98.75\n",
      "Iteration: 49450 Loss: 0.00018394214566797018 Accuracy: 98.76190185546875\n",
      "Iteration: 49500 Loss: 0.00012567255180329084 Accuracy: 98.76190185546875\n",
      "Iteration: 49550 Loss: 1.4215122064342722e-05 Accuracy: 98.75\n",
      "Iteration: 49600 Loss: 8.903972047846764e-05 Accuracy: 98.75\n",
      "Iteration: 49650 Loss: 0.00012740034435410053 Accuracy: 98.72618865966797\n",
      "Iteration: 49700 Loss: 8.543697913410142e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 49750 Loss: 3.941911199945025e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 49800 Loss: 9.687945566838607e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 49850 Loss: 7.154605555115268e-05 Accuracy: 98.75\n",
      "Iteration: 49900 Loss: 9.817136742640287e-05 Accuracy: 98.75\n",
      "Iteration: 49950 Loss: 9.450363722862676e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 50000 Loss: 5.39028987986967e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 50050 Loss: 1.99971764232032e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 50100 Loss: 6.683950050501153e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 50150 Loss: 0.000271154596703127 Accuracy: 98.73809814453125\n",
      "Iteration: 50200 Loss: 8.387536945519969e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 50250 Loss: 4.003831054433249e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 50300 Loss: 0.00011714803258655593 Accuracy: 98.76190185546875\n",
      "Iteration: 50350 Loss: 0.000249853590503335 Accuracy: 98.73809814453125\n",
      "Iteration: 50400 Loss: 0.0001389199314871803 Accuracy: 98.75\n",
      "Iteration: 50450 Loss: 0.00010907436808338389 Accuracy: 98.73809814453125\n",
      "Iteration: 50500 Loss: 0.00012166865053586662 Accuracy: 98.75\n",
      "Iteration: 50550 Loss: 0.0001562660763738677 Accuracy: 98.76190185546875\n",
      "Iteration: 50600 Loss: 3.333544736960903e-05 Accuracy: 98.75\n",
      "Iteration: 50650 Loss: 9.151418635156006e-05 Accuracy: 98.75\n",
      "Iteration: 50700 Loss: 3.727633156813681e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 50750 Loss: 0.00020446589041966945 Accuracy: 98.75\n",
      "Iteration: 50800 Loss: 0.00043461870518513024 Accuracy: 98.73809814453125\n",
      "Iteration: 50850 Loss: 0.00016031581617426127 Accuracy: 98.73809814453125\n",
      "Iteration: 50900 Loss: 0.00027915925602428615 Accuracy: 98.73809814453125\n",
      "Iteration: 50950 Loss: 7.542240200564265e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 51000 Loss: 9.241141378879547e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 51050 Loss: 2.9596172680612653e-05 Accuracy: 98.75\n",
      "Iteration: 51100 Loss: 1.5104592421266716e-05 Accuracy: 98.75\n",
      "Iteration: 51150 Loss: 0.0001525258703622967 Accuracy: 98.75\n",
      "Iteration: 51200 Loss: 4.766726851812564e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 51250 Loss: 5.7582119552535005e-06 Accuracy: 98.75\n",
      "Iteration: 51300 Loss: 4.48369282821659e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 51350 Loss: 9.052723908098415e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 51400 Loss: 5.125209645484574e-05 Accuracy: 98.75\n",
      "Iteration: 51450 Loss: 0.00011655291746137664 Accuracy: 98.76190185546875\n",
      "Iteration: 51500 Loss: 2.3986185624380596e-05 Accuracy: 98.75\n",
      "Iteration: 51550 Loss: 0.00014563289005309343 Accuracy: 98.75\n",
      "Iteration: 51600 Loss: 0.00011101811833214015 Accuracy: 98.72618865966797\n",
      "Iteration: 51650 Loss: 3.990177356172353e-05 Accuracy: 98.75\n",
      "Iteration: 51700 Loss: 3.7380511912488146e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 51750 Loss: 6.956716242711991e-05 Accuracy: 98.75\n",
      "Iteration: 51800 Loss: 2.9647662813658826e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 51850 Loss: 0.00011628623906290159 Accuracy: 98.73809814453125\n",
      "Iteration: 51900 Loss: 0.00024748590658418834 Accuracy: 98.75\n",
      "Iteration: 51950 Loss: 6.0716833104379475e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 52000 Loss: 2.1472442313097417e-05 Accuracy: 98.75\n",
      "Iteration: 52050 Loss: 8.115668606478721e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 52100 Loss: 3.2557742088101804e-05 Accuracy: 98.75\n",
      "Iteration: 52150 Loss: 0.00016935111489146948 Accuracy: 98.75\n",
      "Iteration: 52200 Loss: 8.148098277160898e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 52250 Loss: 9.866543405223638e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 52300 Loss: 6.648813268839149e-06 Accuracy: 98.75\n",
      "Iteration: 52350 Loss: 0.00014071946498006582 Accuracy: 98.73809814453125\n",
      "Iteration: 52400 Loss: 0.00010799890151247382 Accuracy: 98.73809814453125\n",
      "Iteration: 52450 Loss: 0.00014346301031764597 Accuracy: 98.72618865966797\n",
      "Iteration: 52500 Loss: 3.717432628036477e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 52550 Loss: 0.00014208572974894196 Accuracy: 98.76190185546875\n",
      "Iteration: 52600 Loss: 3.865962935378775e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 52650 Loss: 0.00010534556349739432 Accuracy: 98.73809814453125\n",
      "Iteration: 52700 Loss: 2.540105560910888e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 52750 Loss: 0.0001248975604539737 Accuracy: 98.75\n",
      "Iteration: 52800 Loss: 0.00017685563943814486 Accuracy: 98.76190185546875\n",
      "Iteration: 52850 Loss: 7.701185677433386e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 52900 Loss: 5.628061626339331e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 52950 Loss: 5.252549090073444e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 53000 Loss: 0.00014520963304676116 Accuracy: 98.73809814453125\n",
      "Iteration: 53050 Loss: 0.00014241121243685484 Accuracy: 98.75\n",
      "Iteration: 53100 Loss: 8.321626228280365e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 53150 Loss: 3.802409264608286e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 53200 Loss: 6.295990897342563e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 53250 Loss: 4.9562913773115724e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 53300 Loss: 5.049832179793157e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 53350 Loss: 0.00026617819094099104 Accuracy: 98.75\n",
      "Iteration: 53400 Loss: 0.00016281261923722923 Accuracy: 98.73809814453125\n",
      "Iteration: 53450 Loss: 7.411783735733479e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 53500 Loss: 4.505190372583456e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 53550 Loss: 1.2076258826709818e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 53600 Loss: 0.00023927335860207677 Accuracy: 98.73809814453125\n",
      "Iteration: 53650 Loss: 7.850829570088536e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 53700 Loss: 0.00010670025221770629 Accuracy: 98.7738037109375\n",
      "Iteration: 53750 Loss: 1.0418610827400698e-06 Accuracy: 98.76190185546875\n",
      "Iteration: 53800 Loss: 0.0001480829087086022 Accuracy: 98.7738037109375\n",
      "Iteration: 53850 Loss: 0.00021342484978958964 Accuracy: 98.73809814453125\n",
      "Iteration: 53900 Loss: 8.624772453913465e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 53950 Loss: 1.879525007097982e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 54000 Loss: 0.00010820876195793971 Accuracy: 98.75\n",
      "Iteration: 54050 Loss: 9.25960557651706e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 54100 Loss: 0.0003444296889938414 Accuracy: 98.73809814453125\n",
      "Iteration: 54150 Loss: 2.5328512492706068e-05 Accuracy: 98.75\n",
      "Iteration: 54200 Loss: 7.471739081665874e-05 Accuracy: 98.75\n",
      "Iteration: 54250 Loss: 0.0005012743058614433 Accuracy: 98.75\n",
      "Iteration: 54300 Loss: 0.0001820504548959434 Accuracy: 98.75\n",
      "Iteration: 54350 Loss: 0.00011112508946098387 Accuracy: 98.75\n",
      "Iteration: 54400 Loss: 6.456187111325562e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 54450 Loss: 9.10575981833972e-05 Accuracy: 98.75\n",
      "Iteration: 54500 Loss: 7.087222184054554e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 54550 Loss: 8.54010067996569e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 54600 Loss: 0.0004274057282600552 Accuracy: 98.73809814453125\n",
      "Iteration: 54650 Loss: 1.9051445633522235e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 54700 Loss: 0.00010209703759755939 Accuracy: 98.75\n",
      "Iteration: 54750 Loss: 0.0001143979825428687 Accuracy: 98.76190185546875\n",
      "Iteration: 54800 Loss: 9.797795792110264e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 54850 Loss: 6.580198714800645e-07 Accuracy: 98.75\n",
      "Iteration: 54900 Loss: 7.48959428165108e-05 Accuracy: 98.75\n",
      "Iteration: 54950 Loss: 1.38995192173752e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 55000 Loss: 1.9327340851305053e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 55050 Loss: 0.00022208329755812883 Accuracy: 98.75\n",
      "Iteration: 55100 Loss: 0.0001402810012223199 Accuracy: 98.73809814453125\n",
      "Iteration: 55150 Loss: 0.00010457060125190765 Accuracy: 98.73809814453125\n",
      "Iteration: 55200 Loss: 6.33939853287302e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 55250 Loss: 0.00012108823284506798 Accuracy: 98.76190185546875\n",
      "Iteration: 55300 Loss: 0.0001111398232751526 Accuracy: 98.75\n",
      "Iteration: 55350 Loss: 6.94975460646674e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 55400 Loss: 8.504547440679744e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 55450 Loss: 5.522566061699763e-05 Accuracy: 98.75\n",
      "Iteration: 55500 Loss: 7.796950376359746e-05 Accuracy: 98.75\n",
      "Iteration: 55550 Loss: 0.0001809435780160129 Accuracy: 98.72618865966797\n",
      "Iteration: 55600 Loss: 2.196145214838907e-05 Accuracy: 98.75\n",
      "Iteration: 55650 Loss: 2.5255047148675658e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 55700 Loss: 5.119754860061221e-05 Accuracy: 98.75\n",
      "Iteration: 55750 Loss: 0.00018717627972364426 Accuracy: 98.76190185546875\n",
      "Iteration: 55800 Loss: 0.0001595187495695427 Accuracy: 98.72618865966797\n",
      "Iteration: 55850 Loss: 3.321766052977182e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 55900 Loss: 0.00013488391414284706 Accuracy: 98.72618865966797\n",
      "Iteration: 55950 Loss: 0.00021778792142868042 Accuracy: 98.73809814453125\n",
      "Iteration: 56000 Loss: 1.3013321222388186e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 56050 Loss: 0.00016222377598751336 Accuracy: 98.72618865966797\n",
      "Iteration: 56100 Loss: 0.00013219620450399816 Accuracy: 98.73809814453125\n",
      "Iteration: 56150 Loss: 4.470130443223752e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 56200 Loss: 1.036975208990043e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 56250 Loss: 3.91586399928201e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 56300 Loss: 3.350038969074376e-05 Accuracy: 98.75\n",
      "Iteration: 56350 Loss: 3.5298500733915716e-05 Accuracy: 98.75\n",
      "Iteration: 56400 Loss: 6.918173312442377e-05 Accuracy: 98.75\n",
      "Iteration: 56450 Loss: 2.890821087930817e-05 Accuracy: 98.75\n",
      "Iteration: 56500 Loss: 2.2457448721979745e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 56550 Loss: 6.285496783675626e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 56600 Loss: 2.865988244593609e-05 Accuracy: 98.75\n",
      "Iteration: 56650 Loss: 7.325171463890001e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 56700 Loss: 8.171323861461133e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 56750 Loss: 0.0002684169157873839 Accuracy: 98.75\n",
      "Iteration: 56800 Loss: 3.2309300877386704e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 56850 Loss: 4.163034100201912e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 56900 Loss: 3.6797042412217706e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 56950 Loss: 0.0003543776401784271 Accuracy: 98.76190185546875\n",
      "Iteration: 57000 Loss: 0.0002915924706030637 Accuracy: 98.73809814453125\n",
      "Iteration: 57050 Loss: 2.5193055989802815e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 57100 Loss: 6.929302617209032e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 57150 Loss: 6.518701411550865e-05 Accuracy: 98.75\n",
      "Iteration: 57200 Loss: 5.2218238124623895e-06 Accuracy: 98.75\n",
      "Iteration: 57250 Loss: 0.00016800707089714706 Accuracy: 98.73809814453125\n",
      "Iteration: 57300 Loss: 0.00016199728997889906 Accuracy: 98.73809814453125\n",
      "Iteration: 57350 Loss: 1.6861849871929735e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 57400 Loss: 3.839899000013247e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 57450 Loss: 3.132345955236815e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 57500 Loss: 0.00025401622406207025 Accuracy: 98.75\n",
      "Iteration: 57550 Loss: 0.000132643966935575 Accuracy: 98.73809814453125\n",
      "Iteration: 57600 Loss: 0.00013161878450773656 Accuracy: 98.75\n",
      "Iteration: 57650 Loss: 1.9035769582842477e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 57700 Loss: 7.161836401792243e-05 Accuracy: 98.75\n",
      "Iteration: 57750 Loss: 9.987686644308269e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 57800 Loss: 4.541447924566455e-05 Accuracy: 98.75\n",
      "Iteration: 57850 Loss: 2.6687826903071254e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 57900 Loss: 0.0001190217153634876 Accuracy: 98.75\n",
      "Iteration: 57950 Loss: 5.514552321983501e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 58000 Loss: 0.00022987271950114518 Accuracy: 98.75\n",
      "Iteration: 58050 Loss: 7.254385764099425e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 58100 Loss: 6.720351666444913e-05 Accuracy: 98.75\n",
      "Iteration: 58150 Loss: 9.861509897746146e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 58200 Loss: 4.551529491436668e-05 Accuracy: 98.75\n",
      "Iteration: 58250 Loss: 2.3261593014467508e-05 Accuracy: 98.7738037109375\n",
      "Iteration: 58300 Loss: 5.85294037591666e-05 Accuracy: 98.75\n",
      "Iteration: 58350 Loss: 4.651677227229811e-05 Accuracy: 98.75\n",
      "Iteration: 58400 Loss: 0.00012014094681944698 Accuracy: 98.75\n",
      "Iteration: 58450 Loss: 3.922740143025294e-05 Accuracy: 98.75\n",
      "Iteration: 58500 Loss: 9.136922744801268e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 58550 Loss: 5.562070873565972e-05 Accuracy: 98.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 58600 Loss: 8.603967580711469e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 58650 Loss: 3.154603473376483e-05 Accuracy: 98.75\n",
      "Iteration: 58700 Loss: 9.53743074205704e-05 Accuracy: 98.75\n",
      "Iteration: 58750 Loss: 4.749344225274399e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 58800 Loss: 6.798026151955128e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 58850 Loss: 0.00011451734462752938 Accuracy: 98.73809814453125\n",
      "Iteration: 58900 Loss: 8.353417797479779e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 58950 Loss: 0.0002046041627181694 Accuracy: 98.73809814453125\n",
      "Iteration: 59000 Loss: 0.00014607346383854747 Accuracy: 98.73809814453125\n",
      "Iteration: 59050 Loss: 3.320812902529724e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 59100 Loss: 0.00015259243082255125 Accuracy: 98.73809814453125\n",
      "Iteration: 59150 Loss: 0.00032651916262693703 Accuracy: 98.75\n",
      "Iteration: 59200 Loss: 4.263336813892238e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 59250 Loss: 0.00014049797027837485 Accuracy: 98.73809814453125\n",
      "Iteration: 59300 Loss: 4.5564756874227896e-05 Accuracy: 98.75\n",
      "Iteration: 59350 Loss: 2.062329622276593e-05 Accuracy: 98.75\n",
      "Iteration: 59400 Loss: 7.204418943729252e-05 Accuracy: 98.75\n",
      "Iteration: 59450 Loss: 2.144151585525833e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 59500 Loss: 1.7953219867195003e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 59550 Loss: 2.1458445189637132e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 59600 Loss: 7.198924868134782e-05 Accuracy: 98.75\n",
      "Iteration: 59650 Loss: 9.833020158112049e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 59700 Loss: 0.0001743433385854587 Accuracy: 98.72618865966797\n",
      "Iteration: 59750 Loss: 0.00010972505697282031 Accuracy: 98.75\n",
      "Iteration: 59800 Loss: 0.0001603512791916728 Accuracy: 98.73809814453125\n",
      "Iteration: 59850 Loss: 7.476720202248544e-05 Accuracy: 98.75\n",
      "Iteration: 59900 Loss: 0.00013488053809851408 Accuracy: 98.75\n",
      "Iteration: 59950 Loss: 5.7975659728981555e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60000 Loss: 9.324753773398697e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60050 Loss: 7.754849502816796e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60100 Loss: 1.4062115951674059e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 60150 Loss: 8.876568608684465e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 60200 Loss: 4.959472062182613e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60250 Loss: 0.00011266162618994713 Accuracy: 98.73809814453125\n",
      "Iteration: 60300 Loss: 7.473181176465005e-05 Accuracy: 98.75\n",
      "Iteration: 60350 Loss: 1.1849483598780353e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60400 Loss: 0.00019426795188337564 Accuracy: 98.73809814453125\n",
      "Iteration: 60450 Loss: 4.57595415355172e-05 Accuracy: 98.75\n",
      "Iteration: 60500 Loss: 0.00011393072054488584 Accuracy: 98.73809814453125\n",
      "Iteration: 60550 Loss: 0.0002465620927978307 Accuracy: 98.75\n",
      "Iteration: 60600 Loss: 4.397295924718492e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 60650 Loss: 8.228717342717573e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60700 Loss: 1.3411441614152864e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 60750 Loss: 0.00022358892601914704 Accuracy: 98.73809814453125\n",
      "Iteration: 60800 Loss: 3.725707210833207e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60850 Loss: 4.835842992179096e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 60900 Loss: 6.124078936409205e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 60950 Loss: 1.9075221644015983e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 61000 Loss: 7.870581612223759e-05 Accuracy: 98.75\n",
      "Iteration: 61050 Loss: 7.241708226501942e-05 Accuracy: 98.75\n",
      "Iteration: 61100 Loss: 7.15581263648346e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 61150 Loss: 0.00029172823997214437 Accuracy: 98.73809814453125\n",
      "Iteration: 61200 Loss: 4.6235454647103325e-05 Accuracy: 98.75\n",
      "Iteration: 61250 Loss: 6.563142051163595e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 61300 Loss: 7.304921018658206e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 61350 Loss: 0.00022561840887647122 Accuracy: 98.75\n",
      "Iteration: 61400 Loss: 2.259887332911603e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 61450 Loss: 0.00016540945216547698 Accuracy: 98.73809814453125\n",
      "Iteration: 61500 Loss: 0.00018151527910958976 Accuracy: 98.75\n",
      "Iteration: 61550 Loss: 2.9918635391368298e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 61600 Loss: 0.00015763341798447073 Accuracy: 98.75\n",
      "Iteration: 61650 Loss: 0.00021832050697412342 Accuracy: 98.73809814453125\n",
      "Iteration: 61700 Loss: 8.591135701863095e-05 Accuracy: 98.75\n",
      "Iteration: 61750 Loss: 7.827210356481373e-05 Accuracy: 98.75\n",
      "Iteration: 61800 Loss: 8.445255843980704e-06 Accuracy: 98.75\n",
      "Iteration: 61850 Loss: 0.0002565805916674435 Accuracy: 98.73809814453125\n",
      "Iteration: 61900 Loss: 4.4949829316465184e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 61950 Loss: 7.934940367704257e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 62000 Loss: 0.0002429051382932812 Accuracy: 98.73809814453125\n",
      "Iteration: 62050 Loss: 2.7410536858951673e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 62100 Loss: 0.00014898415247444063 Accuracy: 98.72618865966797\n",
      "Iteration: 62150 Loss: 7.859272955101915e-06 Accuracy: 98.76190185546875\n",
      "Iteration: 62200 Loss: 3.179188752255868e-06 Accuracy: 98.75\n",
      "Iteration: 62250 Loss: 3.7245990824885666e-05 Accuracy: 98.75\n",
      "Iteration: 62300 Loss: 7.476986502297223e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 62350 Loss: 0.00013277468679007143 Accuracy: 98.75\n",
      "Iteration: 62400 Loss: 3.609713530750014e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 62450 Loss: 1.2131177754781675e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 62500 Loss: 2.7199699616176076e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 62550 Loss: 1.982473986572586e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 62600 Loss: 0.00010070722782984376 Accuracy: 98.72618865966797\n",
      "Iteration: 62650 Loss: 9.915381815517321e-05 Accuracy: 98.75\n",
      "Iteration: 62700 Loss: 0.00027011873316951096 Accuracy: 98.73809814453125\n",
      "Iteration: 62750 Loss: 7.880678458604962e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 62800 Loss: 9.019665594678372e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 62850 Loss: 0.00010595133790047839 Accuracy: 98.73809814453125\n",
      "Iteration: 62900 Loss: 2.314778794243466e-05 Accuracy: 98.75\n",
      "Iteration: 62950 Loss: 7.200521213235334e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63000 Loss: 2.2362387426255737e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 63050 Loss: 4.075145989190787e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63100 Loss: 2.5457942683715373e-05 Accuracy: 98.75\n",
      "Iteration: 63150 Loss: 0.0001378762099193409 Accuracy: 98.73809814453125\n",
      "Iteration: 63200 Loss: 4.3438212742330506e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63250 Loss: 1.001350119622657e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 63300 Loss: 1.6113515812321566e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63350 Loss: 6.995999137870967e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63400 Loss: 0.00018450617790222168 Accuracy: 98.75\n",
      "Iteration: 63450 Loss: 7.927525439299643e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63500 Loss: 3.563685459084809e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 63550 Loss: 1.2969679801244638e-06 Accuracy: 98.75\n",
      "Iteration: 63600 Loss: 2.340542596357409e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63650 Loss: 0.00020064582349732518 Accuracy: 98.73809814453125\n",
      "Iteration: 63700 Loss: 6.364348519127816e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63750 Loss: 1.7309448594460264e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 63800 Loss: 1.8369606550550088e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 63850 Loss: 2.6533878553891554e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 63900 Loss: 0.0002036654914263636 Accuracy: 98.73809814453125\n",
      "Iteration: 63950 Loss: 0.0003438981366343796 Accuracy: 98.72618865966797\n",
      "Iteration: 64000 Loss: 0.00012624183727893978 Accuracy: 98.73809814453125\n",
      "Iteration: 64050 Loss: 3.8135258364491165e-05 Accuracy: 98.75\n",
      "Iteration: 64100 Loss: 0.00024065375328063965 Accuracy: 98.73809814453125\n",
      "Iteration: 64150 Loss: 0.0001499481440987438 Accuracy: 98.73809814453125\n",
      "Iteration: 64200 Loss: 5.637732465402223e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 64250 Loss: 4.5992415834916756e-05 Accuracy: 98.75\n",
      "Iteration: 64300 Loss: 0.00012571537808980793 Accuracy: 98.75\n",
      "Iteration: 64350 Loss: 8.640367741463706e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 64400 Loss: 9.753069025464356e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 64450 Loss: 0.00018987526709679514 Accuracy: 98.76190185546875\n",
      "Iteration: 64500 Loss: 1.587923543411307e-05 Accuracy: 98.75\n",
      "Iteration: 64550 Loss: 4.387756416690536e-05 Accuracy: 98.75\n",
      "Iteration: 64600 Loss: 4.6215900511015207e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 64650 Loss: 8.969707414507866e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 64700 Loss: 0.00012891928781755269 Accuracy: 98.73809814453125\n",
      "Iteration: 64750 Loss: 7.473826372006442e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 64800 Loss: 7.33753913664259e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 64850 Loss: 6.160324846860021e-05 Accuracy: 98.75\n",
      "Iteration: 64900 Loss: 0.00011508090392453596 Accuracy: 98.73809814453125\n",
      "Iteration: 64950 Loss: 0.0001074651227099821 Accuracy: 98.75\n",
      "Iteration: 65000 Loss: 5.671407052432187e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 65050 Loss: 6.46000262349844e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 65100 Loss: 8.665135828778148e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 65150 Loss: 2.62563225987833e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 65200 Loss: 0.00014821755758021027 Accuracy: 98.75\n",
      "Iteration: 65250 Loss: 1.3093631423544139e-05 Accuracy: 98.75\n",
      "Iteration: 65300 Loss: 5.578907803283073e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 65350 Loss: 0.00014675794227514416 Accuracy: 98.75\n",
      "Iteration: 65400 Loss: 5.190084993955679e-05 Accuracy: 98.75\n",
      "Iteration: 65450 Loss: 0.00013168211444281042 Accuracy: 98.73809814453125\n",
      "Iteration: 65500 Loss: 5.458887972054072e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 65550 Loss: 9.357745148008689e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 65600 Loss: 4.8776470066513866e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 65650 Loss: 0.0002643228799570352 Accuracy: 98.75\n",
      "Iteration: 65700 Loss: 0.00017275853315368295 Accuracy: 98.73809814453125\n",
      "Iteration: 65750 Loss: 8.605582843301818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 65800 Loss: 5.631048679788364e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 65850 Loss: 0.0003108101955149323 Accuracy: 98.75\n",
      "Iteration: 65900 Loss: 0.00010894326260313392 Accuracy: 98.73809814453125\n",
      "Iteration: 65950 Loss: 2.3526416043750942e-05 Accuracy: 98.75\n",
      "Iteration: 66000 Loss: 8.690825052326545e-05 Accuracy: 98.75\n",
      "Iteration: 66050 Loss: 8.05439685791498e-06 Accuracy: 98.75\n",
      "Iteration: 66100 Loss: 1.0686993846320547e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 66150 Loss: 7.834071584511548e-05 Accuracy: 98.75\n",
      "Iteration: 66200 Loss: 7.053396984701976e-05 Accuracy: 98.75\n",
      "Iteration: 66250 Loss: 7.237482350319624e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 66300 Loss: 0.0001331567473243922 Accuracy: 98.73809814453125\n",
      "Iteration: 66350 Loss: 0.00011003066174453124 Accuracy: 98.72618865966797\n",
      "Iteration: 66400 Loss: 0.00010384046618128195 Accuracy: 98.73809814453125\n",
      "Iteration: 66450 Loss: 0.00014712543634232134 Accuracy: 98.75\n",
      "Iteration: 66500 Loss: 0.00043072845437563956 Accuracy: 98.72618865966797\n",
      "Iteration: 66550 Loss: 1.5154048014665022e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 66600 Loss: 5.046395745011978e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 66650 Loss: 0.00010690439376048744 Accuracy: 98.73809814453125\n",
      "Iteration: 66700 Loss: 4.532670936896466e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 66750 Loss: 0.0001502490194980055 Accuracy: 98.72618865966797\n",
      "Iteration: 66800 Loss: 8.993423398351297e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 66850 Loss: 0.00015512901882175356 Accuracy: 98.73809814453125\n",
      "Iteration: 66900 Loss: 3.6216690205037594e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 66950 Loss: 8.950765186455101e-05 Accuracy: 98.75\n",
      "Iteration: 67000 Loss: 0.00010576836939435452 Accuracy: 98.73809814453125\n",
      "Iteration: 67050 Loss: 8.01586065790616e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 67100 Loss: 5.624655750580132e-05 Accuracy: 98.75\n",
      "Iteration: 67150 Loss: 6.998139269853709e-06 Accuracy: 98.75\n",
      "Iteration: 67200 Loss: 7.184972673712764e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 67250 Loss: 3.564347252904554e-07 Accuracy: 98.75\n",
      "Iteration: 67300 Loss: 6.904242763994262e-05 Accuracy: 98.75\n",
      "Iteration: 67350 Loss: 1.89130223589018e-05 Accuracy: 98.75\n",
      "Iteration: 67400 Loss: 6.593301077373326e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 67450 Loss: 5.619157673208974e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 67500 Loss: 4.4495263864519075e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 67550 Loss: 3.7026409700047225e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 67600 Loss: 8.307202369906008e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 67650 Loss: 7.39392198738642e-05 Accuracy: 98.75\n",
      "Iteration: 67700 Loss: 9.988484089262784e-05 Accuracy: 98.75\n",
      "Iteration: 67750 Loss: 0.00021968867804389447 Accuracy: 98.73809814453125\n",
      "Iteration: 67800 Loss: 0.00010228829341940582 Accuracy: 98.72618865966797\n",
      "Iteration: 67850 Loss: 8.177551353583112e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 67900 Loss: 1.0382965456301463e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 67950 Loss: 7.182583067333326e-05 Accuracy: 98.75\n",
      "Iteration: 68000 Loss: 6.939787999726832e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 68050 Loss: 5.764880188507959e-05 Accuracy: 98.75\n",
      "Iteration: 68100 Loss: 2.442314325890038e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 68150 Loss: 4.182581324130297e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 68200 Loss: 5.436652645585127e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 68250 Loss: 8.465372957289219e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 68300 Loss: 1.674177110544406e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 68350 Loss: 0.00019070651615038514 Accuracy: 98.73809814453125\n",
      "Iteration: 68400 Loss: 0.00012277513451408595 Accuracy: 98.73809814453125\n",
      "Iteration: 68450 Loss: 6.236560238903621e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 68500 Loss: 0.0003719002415891737 Accuracy: 98.73809814453125\n",
      "Iteration: 68550 Loss: 2.4993252736749128e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 68600 Loss: 8.675162825966254e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 68650 Loss: 6.453442620113492e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 68700 Loss: 0.00011326087405905128 Accuracy: 98.73809814453125\n",
      "Iteration: 68750 Loss: 4.856743908021599e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 68800 Loss: 0.00046772678615525365 Accuracy: 98.73809814453125\n",
      "Iteration: 68850 Loss: 0.0002919031830970198 Accuracy: 98.73809814453125\n",
      "Iteration: 68900 Loss: 7.023142097750679e-05 Accuracy: 98.75\n",
      "Iteration: 68950 Loss: 4.2836680222535506e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69000 Loss: 0.00010293064406141639 Accuracy: 98.73809814453125\n",
      "Iteration: 69050 Loss: 3.5792552807834e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 69100 Loss: 4.5201973989605904e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69150 Loss: 4.996767165721394e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 69200 Loss: 0.00013017142191529274 Accuracy: 98.75\n",
      "Iteration: 69250 Loss: 3.5130869946442544e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69300 Loss: 5.864433114766143e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 69350 Loss: 0.00012086085189366713 Accuracy: 98.73809814453125\n",
      "Iteration: 69400 Loss: 8.751371933612972e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69450 Loss: 2.6386544050183147e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69500 Loss: 5.375207547331229e-05 Accuracy: 98.75\n",
      "Iteration: 69550 Loss: 0.00010719827696448192 Accuracy: 98.76190185546875\n",
      "Iteration: 69600 Loss: 4.2615647544153035e-05 Accuracy: 98.75\n",
      "Iteration: 69650 Loss: 5.622276876238175e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69700 Loss: 9.686560224508867e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69750 Loss: 2.8716051019728184e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 69800 Loss: 9.247486741514876e-05 Accuracy: 98.75\n",
      "Iteration: 69850 Loss: 3.401250432943925e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 69900 Loss: 5.358505586627871e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 69950 Loss: 3.514096897561103e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70000 Loss: 8.564861491322517e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70050 Loss: 0.00010036501771537587 Accuracy: 98.75\n",
      "Iteration: 70100 Loss: 9.76316659944132e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70150 Loss: 0.00010803360783029348 Accuracy: 98.72618865966797\n",
      "Iteration: 70200 Loss: 9.421609138371423e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 70250 Loss: 0.0001308226928813383 Accuracy: 98.73809814453125\n",
      "Iteration: 70300 Loss: 0.00010901150380959734 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 70350 Loss: 3.36338380293455e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70400 Loss: 0.0001078387358575128 Accuracy: 98.75\n",
      "Iteration: 70450 Loss: 5.376409899326973e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70500 Loss: 0.00010626795119605958 Accuracy: 98.73809814453125\n",
      "Iteration: 70550 Loss: 8.340906060766429e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 70600 Loss: 0.0001358610752504319 Accuracy: 98.75\n",
      "Iteration: 70650 Loss: 4.041237843921408e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70700 Loss: 4.8695514124119654e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70750 Loss: 5.5406810133717954e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 70800 Loss: 5.9831319958902895e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 70850 Loss: 1.9501210772432387e-05 Accuracy: 98.75\n",
      "Iteration: 70900 Loss: 0.0001615192013559863 Accuracy: 98.75\n",
      "Iteration: 70950 Loss: 5.755616439273581e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71000 Loss: 6.0579018281714525e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 71050 Loss: 1.1403317330405116e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71100 Loss: 1.5884073945926502e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71150 Loss: 2.620344457682222e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71200 Loss: 1.8896142137236893e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 71250 Loss: 2.2454185454989783e-05 Accuracy: 98.75\n",
      "Iteration: 71300 Loss: 0.00013429459067992866 Accuracy: 98.75\n",
      "Iteration: 71350 Loss: 7.003990322118625e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71400 Loss: 3.7567297113128006e-05 Accuracy: 98.75\n",
      "Iteration: 71450 Loss: 8.000715024536476e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 71500 Loss: 1.0871918675547931e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71550 Loss: 4.81185270473361e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71600 Loss: 0.00014673650730401278 Accuracy: 98.75\n",
      "Iteration: 71650 Loss: 4.610828182194382e-05 Accuracy: 98.75\n",
      "Iteration: 71700 Loss: 5.106504977447912e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 71750 Loss: 8.482867997372523e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 71800 Loss: 4.987643114873208e-05 Accuracy: 98.75\n",
      "Iteration: 71850 Loss: 8.200444426620379e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71900 Loss: 7.185641879914328e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 71950 Loss: 0.00012030731159029528 Accuracy: 98.72618865966797\n",
      "Iteration: 72000 Loss: 6.228189158719033e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 72050 Loss: 4.5364628022070974e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 72100 Loss: 0.00017510812904220074 Accuracy: 98.73809814453125\n",
      "Iteration: 72150 Loss: 3.191683572367765e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 72200 Loss: 6.778218812542036e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 72250 Loss: 1.9028417227673344e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 72300 Loss: 4.3359021219657734e-05 Accuracy: 98.75\n",
      "Iteration: 72350 Loss: 3.239058059989475e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 72400 Loss: 2.7029278498957865e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 72450 Loss: 0.00012816692469641566 Accuracy: 98.73809814453125\n",
      "Iteration: 72500 Loss: 9.195021993946284e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 72550 Loss: 1.305952810071176e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 72600 Loss: 2.2822892788099125e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 72650 Loss: 3.8468424463644624e-05 Accuracy: 98.75\n",
      "Iteration: 72700 Loss: 4.8245688958559185e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 72750 Loss: 8.589555363869295e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 72800 Loss: 0.0002446098951622844 Accuracy: 98.76190185546875\n",
      "Iteration: 72850 Loss: 3.3475240343250334e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 72900 Loss: 2.9492801331798546e-05 Accuracy: 98.75\n",
      "Iteration: 72950 Loss: 4.410629117046483e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 73000 Loss: 2.4505490728188306e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 73050 Loss: 1.491328021074878e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 73100 Loss: 2.773042069748044e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 73150 Loss: 0.0002677679294720292 Accuracy: 98.73809814453125\n",
      "Iteration: 73200 Loss: 1.804201201593969e-05 Accuracy: 98.75\n",
      "Iteration: 73250 Loss: 1.5744804841233417e-05 Accuracy: 98.75\n",
      "Iteration: 73300 Loss: 0.00011286103836027905 Accuracy: 98.73809814453125\n",
      "Iteration: 73350 Loss: 5.070575207355432e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 73400 Loss: 0.00012111318937968463 Accuracy: 98.75\n",
      "Iteration: 73450 Loss: 8.869689918356016e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 73500 Loss: 5.763668741565198e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 73550 Loss: 3.077510336879641e-05 Accuracy: 98.75\n",
      "Iteration: 73600 Loss: 8.643874753033742e-05 Accuracy: 98.75\n",
      "Iteration: 73650 Loss: 8.790165884420276e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 73700 Loss: 3.0453758881776594e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 73750 Loss: 0.0001089425859390758 Accuracy: 98.73809814453125\n",
      "Iteration: 73800 Loss: 4.376915603643283e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 73850 Loss: 5.620082811219618e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 73900 Loss: 5.772977237938903e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 73950 Loss: 0.00012221644283272326 Accuracy: 98.73809814453125\n",
      "Iteration: 74000 Loss: 3.116659718216397e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 74050 Loss: 3.178941915393807e-05 Accuracy: 98.75\n",
      "Iteration: 74100 Loss: 4.372735565993935e-05 Accuracy: 98.75\n",
      "Iteration: 74150 Loss: 7.256990647874773e-05 Accuracy: 98.75\n",
      "Iteration: 74200 Loss: 4.8654757847543806e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 74250 Loss: 0.00018321372044738382 Accuracy: 98.76190185546875\n",
      "Iteration: 74300 Loss: 0.00015779260138515383 Accuracy: 98.75\n",
      "Iteration: 74350 Loss: 7.176384679041803e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 74400 Loss: 0.00013932459114585072 Accuracy: 98.73809814453125\n",
      "Iteration: 74450 Loss: 3.6877776437904686e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 74500 Loss: 5.087088356958702e-05 Accuracy: 98.75\n",
      "Iteration: 74550 Loss: 1.2054381841153372e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 74600 Loss: 0.00011727290257113054 Accuracy: 98.73809814453125\n",
      "Iteration: 74650 Loss: 6.853146624052897e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 74700 Loss: 4.416762021719478e-05 Accuracy: 98.75\n",
      "Iteration: 74750 Loss: 2.075284101010766e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 74800 Loss: 6.587642565136775e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 74850 Loss: 3.7790661735925823e-05 Accuracy: 98.75\n",
      "Iteration: 74900 Loss: 5.543563383980654e-05 Accuracy: 98.75\n",
      "Iteration: 74950 Loss: 2.1768626538687386e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75000 Loss: 2.439594391034916e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 75050 Loss: 0.00011909047316294163 Accuracy: 98.73809814453125\n",
      "Iteration: 75100 Loss: 4.11284199799411e-05 Accuracy: 98.75\n",
      "Iteration: 75150 Loss: 2.270882760058157e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 75200 Loss: 0.000170405168319121 Accuracy: 98.72618865966797\n",
      "Iteration: 75250 Loss: 5.91927018831484e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75300 Loss: 5.375293403631076e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 75350 Loss: 5.4872725741006434e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 75400 Loss: 1.9078306650044397e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75450 Loss: 5.817295823362656e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75500 Loss: 0.00022188345610629767 Accuracy: 98.73809814453125\n",
      "Iteration: 75550 Loss: 3.453624958638102e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75600 Loss: 1.748502290865872e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 75650 Loss: 6.041184678906575e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 75700 Loss: 8.872024045558646e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75750 Loss: 5.790203431388363e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75800 Loss: 2.5931089112418704e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75850 Loss: 5.274915019981563e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 75900 Loss: 4.6924073103582487e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 75950 Loss: 1.7099990145652555e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 76000 Loss: 5.5369582696584985e-05 Accuracy: 98.75\n",
      "Iteration: 76050 Loss: 6.858928099973127e-05 Accuracy: 98.75\n",
      "Iteration: 76100 Loss: 0.00010176443174714223 Accuracy: 98.75\n",
      "Iteration: 76150 Loss: 1.8547629224485718e-05 Accuracy: 98.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 76200 Loss: 6.716557982144877e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 76250 Loss: 7.505859230150236e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 76300 Loss: 2.2723665097146295e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 76350 Loss: 8.075637742877007e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 76400 Loss: 8.036100189201534e-05 Accuracy: 98.75\n",
      "Iteration: 76450 Loss: 4.27457234764006e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 76500 Loss: 5.2124796638963744e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 76550 Loss: 1.974996121134609e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 76600 Loss: 1.7757780369720422e-05 Accuracy: 98.75\n",
      "Iteration: 76650 Loss: 4.039062696392648e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 76700 Loss: 1.66216505022021e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 76750 Loss: 0.00023916989448480308 Accuracy: 98.73809814453125\n",
      "Iteration: 76800 Loss: 9.128322562901303e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 76850 Loss: 4.675573291024193e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 76900 Loss: 2.8528440452646464e-05 Accuracy: 98.75\n",
      "Iteration: 76950 Loss: 3.4417178540024906e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 77000 Loss: 0.0001546987477922812 Accuracy: 98.75\n",
      "Iteration: 77050 Loss: 7.345687481574714e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77100 Loss: 7.09274536347948e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77150 Loss: 3.807226676144637e-05 Accuracy: 98.75\n",
      "Iteration: 77200 Loss: 6.598295294679701e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77250 Loss: 3.0076702387304977e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 77300 Loss: 2.1303094399627298e-05 Accuracy: 98.75\n",
      "Iteration: 77350 Loss: 5.6142089306376874e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77400 Loss: 4.635829827748239e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77450 Loss: 7.777006248943508e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77500 Loss: 2.5095474484260194e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77550 Loss: 2.5853905754047446e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77600 Loss: 8.973121293820441e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 77650 Loss: 6.049122748663649e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77700 Loss: 2.898065395129379e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77750 Loss: 0.00015410096966661513 Accuracy: 98.72618865966797\n",
      "Iteration: 77800 Loss: 0.00012415144010446966 Accuracy: 98.73809814453125\n",
      "Iteration: 77850 Loss: 6.508047954412177e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77900 Loss: 9.285972191719338e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 77950 Loss: 8.657597209094092e-05 Accuracy: 98.75\n",
      "Iteration: 78000 Loss: 0.00012271423474885523 Accuracy: 98.73809814453125\n",
      "Iteration: 78050 Loss: 6.128466338850558e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 78100 Loss: 0.00015855618403293192 Accuracy: 98.73809814453125\n",
      "Iteration: 78150 Loss: 5.894999048905447e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 78200 Loss: 2.633316398714669e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 78250 Loss: 3.1865154596744105e-05 Accuracy: 98.75\n",
      "Iteration: 78300 Loss: 3.2965035643428564e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 78350 Loss: 2.3342226995737292e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 78400 Loss: 0.0001601533149369061 Accuracy: 98.72618865966797\n",
      "Iteration: 78450 Loss: 4.8911806516116485e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 78500 Loss: 8.807297854218632e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 78550 Loss: 4.554662882583216e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 78600 Loss: 2.1389127141446806e-05 Accuracy: 98.75\n",
      "Iteration: 78650 Loss: 3.730844764504582e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 78700 Loss: 4.8997506382875144e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 78750 Loss: 7.425059447996318e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 78800 Loss: 5.150403376319446e-05 Accuracy: 98.75\n",
      "Iteration: 78850 Loss: 0.00011000672384398058 Accuracy: 98.72618865966797\n",
      "Iteration: 78900 Loss: 3.596856549847871e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 78950 Loss: 0.00011373018787708133 Accuracy: 98.72618865966797\n",
      "Iteration: 79000 Loss: 6.785105506423861e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 79050 Loss: 8.257569425040856e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 79100 Loss: 2.6087614969583228e-05 Accuracy: 98.75\n",
      "Iteration: 79150 Loss: 4.958243152941577e-05 Accuracy: 98.75\n",
      "Iteration: 79200 Loss: 0.00015132322732824832 Accuracy: 98.73809814453125\n",
      "Iteration: 79250 Loss: 2.6384715965832584e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 79300 Loss: 2.4369368475163355e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 79350 Loss: 5.936976594966836e-05 Accuracy: 98.75\n",
      "Iteration: 79400 Loss: 6.799126276746392e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 79450 Loss: 4.8256482841679826e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 79500 Loss: 7.777744758641347e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 79550 Loss: 2.531485188228544e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 79600 Loss: 4.315911792218685e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 79650 Loss: 0.00015336765500251204 Accuracy: 98.75\n",
      "Iteration: 79700 Loss: 2.7884105293196626e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 79750 Loss: 0.00010154982010135427 Accuracy: 98.72618865966797\n",
      "Iteration: 79800 Loss: 2.3639326172997244e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 79850 Loss: 1.9685367078636773e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 79900 Loss: 0.00011345783423166722 Accuracy: 98.75\n",
      "Iteration: 79950 Loss: 6.810430204495788e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 80000 Loss: 1.3010762813792098e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 80050 Loss: 2.5906461814884096e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 80100 Loss: 3.500226739561185e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 80150 Loss: 2.1967431166558526e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 80200 Loss: 5.439767483039759e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 80250 Loss: 8.064217581704725e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 80300 Loss: 3.5857479815604165e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 80350 Loss: 5.159396823728457e-05 Accuracy: 98.75\n",
      "Iteration: 80400 Loss: 0.00012448179768398404 Accuracy: 98.75\n",
      "Iteration: 80450 Loss: 5.189160947338678e-05 Accuracy: 98.75\n",
      "Iteration: 80500 Loss: 2.2668515157420188e-05 Accuracy: 98.75\n",
      "Iteration: 80550 Loss: 2.1959349396638572e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 80600 Loss: 5.813324969494715e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 80650 Loss: 0.00017523732094559819 Accuracy: 98.73809814453125\n",
      "Iteration: 80700 Loss: 0.00010171754547627643 Accuracy: 98.73809814453125\n",
      "Iteration: 80750 Loss: 7.327868661377579e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 80800 Loss: 6.799620314268395e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 80850 Loss: 2.925788430729881e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 80900 Loss: 8.731051639188081e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 80950 Loss: 8.186627383111045e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81000 Loss: 1.2867036275565624e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81050 Loss: 7.381629256997257e-05 Accuracy: 98.75\n",
      "Iteration: 81100 Loss: 5.5363689170917496e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81150 Loss: 3.8145528378663585e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81200 Loss: 7.632622146047652e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81250 Loss: 7.720066059846431e-05 Accuracy: 98.75\n",
      "Iteration: 81300 Loss: 4.52812819276005e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81350 Loss: 1.664520095800981e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81400 Loss: 7.691642531426623e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81450 Loss: 3.7534864532062784e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81500 Loss: 2.0851301087532192e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81550 Loss: 2.1866195311304182e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81600 Loss: 8.290939877042547e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81650 Loss: 2.6872130547417328e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 81700 Loss: 3.535718497005291e-05 Accuracy: 98.75\n",
      "Iteration: 81750 Loss: 4.766969141201116e-05 Accuracy: 98.75\n",
      "Iteration: 81800 Loss: 5.7556921092327684e-05 Accuracy: 98.75\n",
      "Iteration: 81850 Loss: 5.79166880925186e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 81900 Loss: 9.937344293575734e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 81950 Loss: 2.3478241928387433e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 82000 Loss: 7.201710104709491e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 82050 Loss: 9.018332639243454e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 82100 Loss: 7.194223871920258e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 82150 Loss: 7.562152313767001e-05 Accuracy: 98.75\n",
      "Iteration: 82200 Loss: 5.55965380044654e-05 Accuracy: 98.75\n",
      "Iteration: 82250 Loss: 8.580111898481846e-05 Accuracy: 98.75\n",
      "Iteration: 82300 Loss: 0.00012431811774149537 Accuracy: 98.73809814453125\n",
      "Iteration: 82350 Loss: 2.702376605157042e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 82400 Loss: 1.9949087800341658e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 82450 Loss: 8.704607353138272e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 82500 Loss: 0.00027204633806832135 Accuracy: 98.72618865966797\n",
      "Iteration: 82550 Loss: 3.668282806756906e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 82600 Loss: 1.885287019831594e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 82650 Loss: 6.253369792830199e-05 Accuracy: 98.75\n",
      "Iteration: 82700 Loss: 3.837708936771378e-05 Accuracy: 98.75\n",
      "Iteration: 82750 Loss: 4.681781865656376e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 82800 Loss: 0.00013689686602447182 Accuracy: 98.75\n",
      "Iteration: 82850 Loss: 9.298593067796901e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 82900 Loss: 1.3020737242186442e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 82950 Loss: 3.8972586480667815e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83000 Loss: 9.186189708998427e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83050 Loss: 2.9891549274907447e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83100 Loss: 7.926016405690461e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83150 Loss: 2.5043730147444876e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 83200 Loss: 0.00014399229257833213 Accuracy: 98.73809814453125\n",
      "Iteration: 83250 Loss: 3.426882904022932e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83300 Loss: 0.0001134824997279793 Accuracy: 98.75\n",
      "Iteration: 83350 Loss: 8.324649388669059e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83400 Loss: 0.00010822186595760286 Accuracy: 98.73809814453125\n",
      "Iteration: 83450 Loss: 5.515032898983918e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83500 Loss: 4.91215760121122e-05 Accuracy: 98.75\n",
      "Iteration: 83550 Loss: 3.293521513114683e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 83600 Loss: 0.00011356590403011069 Accuracy: 98.75\n",
      "Iteration: 83650 Loss: 0.00018207199173048139 Accuracy: 98.72618865966797\n",
      "Iteration: 83700 Loss: 4.038553925056476e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 83750 Loss: 2.1455680325743742e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 83800 Loss: 0.00019783801690209657 Accuracy: 98.73809814453125\n",
      "Iteration: 83850 Loss: 4.7884979721857235e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83900 Loss: 9.525307541480288e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 83950 Loss: 1.8093647668138146e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84000 Loss: 1.4680827007396147e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84050 Loss: 9.883885468298104e-06 Accuracy: 98.75\n",
      "Iteration: 84100 Loss: 2.6059547963086516e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84150 Loss: 1.2848830920120236e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84200 Loss: 0.00016442753258161247 Accuracy: 98.72618865966797\n",
      "Iteration: 84250 Loss: 1.8712431483436376e-05 Accuracy: 98.75\n",
      "Iteration: 84300 Loss: 5.5281179811572656e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84350 Loss: 4.2312240111641586e-05 Accuracy: 98.75\n",
      "Iteration: 84400 Loss: 2.579746796982363e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84450 Loss: 6.0479687817860395e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84500 Loss: 2.149298961739987e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 84550 Loss: 2.4709184799576178e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 84600 Loss: 0.00017230796220246702 Accuracy: 98.72618865966797\n",
      "Iteration: 84650 Loss: 3.9975322579266503e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 84700 Loss: 6.250714068301022e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 84750 Loss: 2.2364787582773715e-05 Accuracy: 98.75\n",
      "Iteration: 84800 Loss: 9.33615865505999e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 84850 Loss: 4.555682153295493e-06 Accuracy: 98.75\n",
      "Iteration: 84900 Loss: 5.8262365200789645e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 84950 Loss: 8.621290180599317e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85000 Loss: 0.00011830526636913419 Accuracy: 98.73809814453125\n",
      "Iteration: 85050 Loss: 9.012080408865586e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 85100 Loss: 4.5955228415550664e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 85150 Loss: 6.101710823713802e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 85200 Loss: 1.5586832887493074e-05 Accuracy: 98.75\n",
      "Iteration: 85250 Loss: 3.564099461073056e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85300 Loss: 4.0036989958025515e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85350 Loss: 4.011826604255475e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85400 Loss: 2.9254884793772362e-05 Accuracy: 98.75\n",
      "Iteration: 85450 Loss: 6.033132012817077e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 85500 Loss: 6.88462023390457e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85550 Loss: 0.00013853967539034784 Accuracy: 98.73809814453125\n",
      "Iteration: 85600 Loss: 9.300545207224786e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85650 Loss: 1.4758532415726222e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85700 Loss: 3.9458172977901995e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 85750 Loss: 5.1129605708410963e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85800 Loss: 7.583632395835593e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85850 Loss: 5.345435783965513e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 85900 Loss: 4.405581421451643e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 85950 Loss: 5.109602716402151e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 86000 Loss: 8.850558515405282e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86050 Loss: 3.0972511012805626e-05 Accuracy: 98.75\n",
      "Iteration: 86100 Loss: 1.594007517269347e-05 Accuracy: 98.75\n",
      "Iteration: 86150 Loss: 5.059643808635883e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86200 Loss: 4.3775220547104254e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86250 Loss: 1.7680931705399416e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86300 Loss: 2.090942143695429e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 86350 Loss: 3.995482256868854e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 86400 Loss: 2.5443216145504266e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 86450 Loss: 4.4465883547673e-05 Accuracy: 98.75\n",
      "Iteration: 86500 Loss: 5.1871651521651074e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86550 Loss: 1.782127469596162e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 86600 Loss: 7.014974107732996e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86650 Loss: 7.965119038999546e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 86700 Loss: 1.621370938664768e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86750 Loss: 5.6154698540922254e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86800 Loss: 2.9229087886051275e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86850 Loss: 3.622337681008503e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 86900 Loss: 0.00019478410831652582 Accuracy: 98.73809814453125\n",
      "Iteration: 86950 Loss: 7.957150955917314e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 87000 Loss: 0.00010824427590705454 Accuracy: 98.72618865966797\n",
      "Iteration: 87050 Loss: 3.4968154068337753e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 87100 Loss: 8.77434213180095e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 87150 Loss: 2.45660721702734e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 87200 Loss: 2.7607085939962417e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 87250 Loss: 4.240799626131775e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 87300 Loss: 5.844010956934653e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 87350 Loss: 0.0001346212375210598 Accuracy: 98.72618865966797\n",
      "Iteration: 87400 Loss: 4.211958002997562e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 87450 Loss: 0.00011460912355687469 Accuracy: 98.72618865966797\n",
      "Iteration: 87500 Loss: 9.55666746449424e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 87550 Loss: 3.621256473707035e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 87600 Loss: 1.8799717508954927e-05 Accuracy: 98.75\n",
      "Iteration: 87650 Loss: 3.717846630024724e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 87700 Loss: 2.0971705453121103e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 87750 Loss: 6.079963714000769e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 87800 Loss: 4.397898374008946e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 87850 Loss: 2.9367622119025327e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 87900 Loss: 1.9656497443065746e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 87950 Loss: 0.00026303596678189933 Accuracy: 98.72618865966797\n",
      "Iteration: 88000 Loss: 1.52951088239206e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 88050 Loss: 7.271642243722454e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 88100 Loss: 6.579933688044548e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 88150 Loss: 9.8572185379453e-05 Accuracy: 98.75\n",
      "Iteration: 88200 Loss: 0.00015818493557162583 Accuracy: 98.73809814453125\n",
      "Iteration: 88250 Loss: 0.0001460908242734149 Accuracy: 98.72618865966797\n",
      "Iteration: 88300 Loss: 0.00010684728476917371 Accuracy: 98.73809814453125\n",
      "Iteration: 88350 Loss: 3.225316322641447e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 88400 Loss: 3.797233148361556e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 88450 Loss: 0.0001277799892704934 Accuracy: 98.73809814453125\n",
      "Iteration: 88500 Loss: 2.8304273655521683e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 88550 Loss: 3.89317974622827e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 88600 Loss: 4.7603476559743285e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 88650 Loss: 1.7450500308768824e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 88700 Loss: 3.63389372068923e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 88750 Loss: 0.00015734077896922827 Accuracy: 98.75\n",
      "Iteration: 88800 Loss: 4.7208304749801755e-05 Accuracy: 98.76190185546875\n",
      "Iteration: 88850 Loss: 4.199765680823475e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 88900 Loss: 2.601193955342751e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 88950 Loss: 2.5178753276122734e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 89000 Loss: 5.882609457330545e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 89050 Loss: 4.618446473614313e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89100 Loss: 6.74333714414388e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 89150 Loss: 4.294227983336896e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 89200 Loss: 2.5159270080621354e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89250 Loss: 0.00010944657697109506 Accuracy: 98.73809814453125\n",
      "Iteration: 89300 Loss: 8.954940130934119e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89350 Loss: 6.777157977921888e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89400 Loss: 4.750787411467172e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89450 Loss: 2.147407030861359e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89500 Loss: 5.8302655816078186e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 89550 Loss: 6.087550355005078e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 89600 Loss: 1.2342517948127352e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89650 Loss: 1.8162314518122002e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89700 Loss: 3.841096622636542e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 89750 Loss: 6.450715591199696e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 89800 Loss: 7.782641478115693e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89850 Loss: 2.7440371923148632e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 89900 Loss: 7.376874691544799e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 89950 Loss: 9.865053289104253e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 90000 Loss: 2.4860819394234568e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 90050 Loss: 7.086874393280596e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90100 Loss: 3.479349106783047e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90150 Loss: 3.667518467409536e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90200 Loss: 2.4841305275913328e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 90250 Loss: 0.00022336369147524238 Accuracy: 98.72618865966797\n",
      "Iteration: 90300 Loss: 7.072815060382709e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90350 Loss: 4.048464688821696e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90400 Loss: 0.0001378447050228715 Accuracy: 98.72618865966797\n",
      "Iteration: 90450 Loss: 9.726819553179666e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 90500 Loss: 7.278547855094075e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90550 Loss: 6.70704830554314e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 90600 Loss: 0.00011232191900489852 Accuracy: 98.73809814453125\n",
      "Iteration: 90650 Loss: 8.189592222151987e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 90700 Loss: 2.341727667953819e-05 Accuracy: 98.75\n",
      "Iteration: 90750 Loss: 3.0828145099803805e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90800 Loss: 5.948196485405788e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 90850 Loss: 9.059629064722685e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 90900 Loss: 2.4754963305895217e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 90950 Loss: 2.480913099134341e-05 Accuracy: 98.75\n",
      "Iteration: 91000 Loss: 9.728752775117755e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 91050 Loss: 4.991575769963674e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91100 Loss: 3.572471905499697e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91150 Loss: 4.417203672346659e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91200 Loss: 4.607070150086656e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91250 Loss: 3.8785339711466804e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 91300 Loss: 1.3260582818475086e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91350 Loss: 9.354603389510885e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91400 Loss: 5.6171807045757305e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 91450 Loss: 2.752090586000122e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 91500 Loss: 5.734322257922031e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 91550 Loss: 6.737475632689893e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 91600 Loss: 8.22138026705943e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 91650 Loss: 4.208656901028007e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91700 Loss: 8.439911312052573e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 91750 Loss: 3.6042070860276e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 91800 Loss: 3.609118357417174e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 91850 Loss: 9.948667138814926e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 91900 Loss: 0.00012339795648586005 Accuracy: 98.72618865966797\n",
      "Iteration: 91950 Loss: 2.369354660913814e-05 Accuracy: 98.75\n",
      "Iteration: 92000 Loss: 8.393537427764386e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92050 Loss: 2.2813539544586092e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 92100 Loss: 4.447311221156269e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 92150 Loss: 2.5644701963756233e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92200 Loss: 7.843205821700394e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92250 Loss: 4.799635644303635e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92300 Loss: 4.587490184349008e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 92350 Loss: 5.665396747644991e-05 Accuracy: 98.75\n",
      "Iteration: 92400 Loss: 5.636077548842877e-05 Accuracy: 98.75\n",
      "Iteration: 92450 Loss: 5.461620457936078e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92500 Loss: 5.3340409067459404e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92550 Loss: 1.474165947001893e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92600 Loss: 5.1214814448030666e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92650 Loss: 4.156354771112092e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92700 Loss: 6.288292934186757e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92750 Loss: 2.617433710838668e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92800 Loss: 2.4893814043025486e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 92850 Loss: 6.240780930966139e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 92900 Loss: 1.833852002164349e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 92950 Loss: 5.9106074331793934e-05 Accuracy: 98.75\n",
      "Iteration: 93000 Loss: 5.064821016276255e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93050 Loss: 2.570062679296825e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93100 Loss: 7.887712854426354e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 93150 Loss: 7.09946034476161e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 93200 Loss: 0.0001317339629167691 Accuracy: 98.72618865966797\n",
      "Iteration: 93250 Loss: 5.4524010920431465e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93300 Loss: 6.333708006422967e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93350 Loss: 3.730152820935473e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 93400 Loss: 1.3314913758222247e-06 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 93450 Loss: 2.6023995815194212e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93500 Loss: 4.62159478047397e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 93550 Loss: 4.186769365333021e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 93600 Loss: 0.00010430035763420165 Accuracy: 98.72618865966797\n",
      "Iteration: 93650 Loss: 8.490819163853303e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93700 Loss: 2.192357169406023e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93750 Loss: 0.00023861810041125864 Accuracy: 98.72618865966797\n",
      "Iteration: 93800 Loss: 1.8098649888997898e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 93850 Loss: 0.00012480784789659083 Accuracy: 98.72618865966797\n",
      "Iteration: 93900 Loss: 5.5959633755264804e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 93950 Loss: 1.8273174646310508e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94000 Loss: 9.895655966829509e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94050 Loss: 4.917203477816656e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94100 Loss: 3.0402035918086767e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 94150 Loss: 5.032543049310334e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94200 Loss: 5.252219125395641e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 94250 Loss: 1.224239667863003e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 94300 Loss: 6.000247594784014e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 94350 Loss: 4.174381319899112e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 94400 Loss: 3.604419180192053e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 94450 Loss: 7.579997327411547e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 94500 Loss: 1.6006799341994338e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94550 Loss: 8.997365512186661e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94600 Loss: 5.551540743908845e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94650 Loss: 8.949507173383608e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94700 Loss: 3.576277407546513e-08 Accuracy: 98.72618865966797\n",
      "Iteration: 94750 Loss: 2.4412283892161213e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 94800 Loss: 5.08375796925975e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 94850 Loss: 9.55198411247693e-05 Accuracy: 98.75\n",
      "Iteration: 94900 Loss: 2.5316696337540634e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 94950 Loss: 4.563818220049143e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95000 Loss: 4.353058830020018e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 95050 Loss: 1.5123660887184087e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95100 Loss: 4.689297202276066e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95150 Loss: 8.866428106557578e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95200 Loss: 1.352835533907637e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 95250 Loss: 2.9496515708160587e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95300 Loss: 4.506412005866878e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95350 Loss: 8.612880628788844e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 95400 Loss: 3.553299393388443e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 95450 Loss: 1.2330803656368516e-05 Accuracy: 98.75\n",
      "Iteration: 95500 Loss: 3.507220753817819e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95550 Loss: 6.418708653654903e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 95600 Loss: 0.00016230472829192877 Accuracy: 98.73809814453125\n",
      "Iteration: 95650 Loss: 5.5369295296259224e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 95700 Loss: 8.179086580639705e-05 Accuracy: 98.75\n",
      "Iteration: 95750 Loss: 1.646922464715317e-05 Accuracy: 98.75\n",
      "Iteration: 95800 Loss: 7.124376861611381e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95850 Loss: 2.015467907767743e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95900 Loss: 2.8964937882847153e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 95950 Loss: 7.689651829423383e-05 Accuracy: 98.75\n",
      "Iteration: 96000 Loss: 3.6833720514550805e-05 Accuracy: 98.75\n",
      "Iteration: 96050 Loss: 6.948520604055375e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 96100 Loss: 3.347472375025973e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 96150 Loss: 6.390049384208396e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 96200 Loss: 2.5408146029803902e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 96250 Loss: 6.324380956357345e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 96300 Loss: 1.3392605978879146e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 96350 Loss: 2.9476575491571566e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 96400 Loss: 6.462809778895462e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 96450 Loss: 3.4179935028078035e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 96500 Loss: 4.7468918637605384e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 96550 Loss: 7.325790647882968e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 96600 Loss: 0.00011218922008993104 Accuracy: 98.73809814453125\n",
      "Iteration: 96650 Loss: 6.745244900230318e-05 Accuracy: 98.75\n",
      "Iteration: 96700 Loss: 7.080898853928375e-07 Accuracy: 98.75\n",
      "Iteration: 96750 Loss: 5.305002923705615e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 96800 Loss: 4.622999404091388e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 96850 Loss: 0.00011817298218375072 Accuracy: 98.72618865966797\n",
      "Iteration: 96900 Loss: 2.3236059860209934e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 96950 Loss: 3.2587413443252444e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 97000 Loss: 2.4404313080594875e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 97050 Loss: 6.257103814277798e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97100 Loss: 8.828784302750137e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 97150 Loss: 2.233008672192227e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97200 Loss: 9.277556273445953e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 97250 Loss: 4.320646985433996e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97300 Loss: 4.6937973820604384e-05 Accuracy: 98.75\n",
      "Iteration: 97350 Loss: 3.120939072687179e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 97400 Loss: 6.39983409200795e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97450 Loss: 3.479248334770091e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 97500 Loss: 1.3089016874801018e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 97550 Loss: 9.624832455301657e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97600 Loss: 4.3308435124345124e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97650 Loss: 7.115795597201213e-05 Accuracy: 98.75\n",
      "Iteration: 97700 Loss: 9.654832683736458e-05 Accuracy: 98.75\n",
      "Iteration: 97750 Loss: 3.107612792518921e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 97800 Loss: 4.8969235649565235e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 97850 Loss: 4.317410275689326e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97900 Loss: 7.558002835139632e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 97950 Loss: 4.23219935328234e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98000 Loss: 6.200099596753716e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 98050 Loss: 5.395510743255727e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98100 Loss: 3.580588600016199e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 98150 Loss: 4.635350705939345e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98200 Loss: 0.00010406908404547721 Accuracy: 98.72618865966797\n",
      "Iteration: 98250 Loss: 3.1876526918495074e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98300 Loss: 3.88012922485359e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 98350 Loss: 3.4397155104670674e-05 Accuracy: 98.75\n",
      "Iteration: 98400 Loss: 8.502013952238485e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98450 Loss: 2.7315240004099905e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 98500 Loss: 1.611027619219385e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98550 Loss: 3.846747495117597e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98600 Loss: 1.0358270628785249e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 98650 Loss: 5.254305506241508e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98700 Loss: 2.8705448130494915e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98750 Loss: 9.398911788593978e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 98800 Loss: 0.00013088159903418273 Accuracy: 98.72618865966797\n",
      "Iteration: 98850 Loss: 0.00011420096416259184 Accuracy: 98.75\n",
      "Iteration: 98900 Loss: 3.1309304176829755e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 98950 Loss: 3.681429734569974e-05 Accuracy: 98.75\n",
      "Iteration: 99000 Loss: 4.5871267502661794e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 99050 Loss: 2.6858639103011228e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99100 Loss: 9.238480060957954e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 99150 Loss: 5.0381502660457045e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99200 Loss: 2.753954140644055e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 99250 Loss: 1.3189384844736196e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99300 Loss: 4.982638711226173e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 99350 Loss: 0.0001169170718640089 Accuracy: 98.72618865966797\n",
      "Iteration: 99400 Loss: 7.453556463588029e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99450 Loss: 1.9021783373318613e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99500 Loss: 3.503485277178697e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99550 Loss: 2.99105049634818e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99600 Loss: 8.353820885531604e-05 Accuracy: 98.75\n",
      "Iteration: 99650 Loss: 5.1971321227028966e-05 Accuracy: 98.75\n",
      "Iteration: 99700 Loss: 0.00014609152276534587 Accuracy: 98.73809814453125\n",
      "Iteration: 99750 Loss: 4.363287371234037e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99800 Loss: 3.3974334655795246e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99850 Loss: 1.445730322302552e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 99900 Loss: 2.3860677174525335e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 99950 Loss: 4.6942412154749036e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 100000 Loss: 9.04935150174424e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100050 Loss: 2.01802104129456e-05 Accuracy: 98.75\n",
      "Iteration: 100100 Loss: 8.901186083676293e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100150 Loss: 6.222961383173242e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100200 Loss: 6.066758942324668e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100250 Loss: 3.2154199288925156e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100300 Loss: 3.41500599461142e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 100350 Loss: 7.843547791708261e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 100400 Loss: 0.00013028417015448213 Accuracy: 98.75\n",
      "Iteration: 100450 Loss: 1.0695832315832376e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100500 Loss: 5.757382677984424e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100550 Loss: 6.100619430071674e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 100600 Loss: 2.173971188312862e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 100650 Loss: 2.499056063243188e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 100700 Loss: 1.1961710697505623e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100750 Loss: 9.716783824842423e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 100800 Loss: 1.3001727893424686e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 100850 Loss: 3.428706622798927e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 100900 Loss: 2.749979557847837e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 100950 Loss: 5.2822535508312285e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101000 Loss: 3.7676676583942026e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 101050 Loss: 4.638847894966602e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 101100 Loss: 3.678067878354341e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101150 Loss: 7.389970414806157e-05 Accuracy: 98.75\n",
      "Iteration: 101200 Loss: 5.389456782722846e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 101250 Loss: 4.356528734206222e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 101300 Loss: 2.1679683413822204e-05 Accuracy: 98.75\n",
      "Iteration: 101350 Loss: 9.167489770334214e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 101400 Loss: 1.1052502486563753e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101450 Loss: 2.6941916075884365e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101500 Loss: 2.6686169803724624e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101550 Loss: 2.1508005374926142e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 101600 Loss: 5.299356780597009e-05 Accuracy: 98.75\n",
      "Iteration: 101650 Loss: 7.789195660734549e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 101700 Loss: 0.00016887765377759933 Accuracy: 98.73809814453125\n",
      "Iteration: 101750 Loss: 1.7988539184443653e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101800 Loss: 6.0784768720623106e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101850 Loss: 9.722699360281695e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 101900 Loss: 2.922051862697117e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 101950 Loss: 3.644551907200366e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 102000 Loss: 6.488696817541495e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102050 Loss: 1.1084912330261432e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102100 Loss: 1.2916633750137407e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102150 Loss: 1.1205517012058408e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 102200 Loss: 5.0326725613558665e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102250 Loss: 5.252609116723761e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102300 Loss: 2.1541909518418834e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 102350 Loss: 5.444904672913253e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 102400 Loss: 3.395181192900054e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102450 Loss: 4.790172533830628e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 102500 Loss: 2.2353064196067862e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102550 Loss: 6.557421147590503e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102600 Loss: 4.861362322117202e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102650 Loss: 6.580762419616804e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102700 Loss: 4.97804103360977e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 102750 Loss: 5.499634426087141e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 102800 Loss: 4.363573680166155e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 102850 Loss: 5.383104507927783e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 102900 Loss: 7.317443760257447e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 102950 Loss: 3.6770888982573524e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103000 Loss: 4.4962147512706e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103050 Loss: 3.720467793755233e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103100 Loss: 4.228315447107889e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103150 Loss: 3.2066836865851656e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103200 Loss: 9.174676961265504e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103250 Loss: 2.7463000151328743e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103300 Loss: 6.39363715890795e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103350 Loss: 3.7440684536704794e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103400 Loss: 0.00011929247557418421 Accuracy: 98.72618865966797\n",
      "Iteration: 103450 Loss: 5.1615530537674204e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103500 Loss: 9.74573049461469e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 103550 Loss: 8.620866719866171e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103600 Loss: 2.9043205358902924e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103650 Loss: 1.928063647937961e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 103700 Loss: 8.52378798299469e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103750 Loss: 1.7323616702924483e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103800 Loss: 2.8020171157550067e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103850 Loss: 9.038821735884994e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 103900 Loss: 2.1457539389757585e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 103950 Loss: 6.956749712117016e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104000 Loss: 5.354396489565261e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104050 Loss: 5.838426295667887e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 104100 Loss: 4.739637006423436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104150 Loss: 6.031277735019103e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 104200 Loss: 4.452961093193153e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 104250 Loss: 4.864658330916427e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 104300 Loss: 0.0001044907039613463 Accuracy: 98.73809814453125\n",
      "Iteration: 104350 Loss: 5.3728213970316574e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 104400 Loss: 5.7377554185222834e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 104450 Loss: 3.0797884392086416e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104500 Loss: 2.348404422036765e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 104550 Loss: 1.458324732084293e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104600 Loss: 3.705771450768225e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104650 Loss: 3.084867785219103e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 104700 Loss: 1.1977836038568057e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 104750 Loss: 2.0204917746013962e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 104800 Loss: 7.592685506097041e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 104850 Loss: 9.232410957338288e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104900 Loss: 3.3809628803282976e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 104950 Loss: 1.4830377949692775e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105000 Loss: 3.405396273592487e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105050 Loss: 3.0168755984050222e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105100 Loss: 3.2207528420258313e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 105150 Loss: 7.591088797198609e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 105200 Loss: 2.9520180760300718e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105250 Loss: 2.188272810599301e-05 Accuracy: 98.75\n",
      "Iteration: 105300 Loss: 4.9482467147754505e-05 Accuracy: 98.75\n",
      "Iteration: 105350 Loss: 8.475547474517953e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 105400 Loss: 6.198345363372937e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105450 Loss: 8.173556852852926e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105500 Loss: 6.04808701609727e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105550 Loss: 2.6261715902364813e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 105600 Loss: 8.426671229244675e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 105650 Loss: 3.698257569340058e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 105700 Loss: 6.0190530348336324e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105750 Loss: 3.8323931221384555e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105800 Loss: 2.7891706849914044e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105850 Loss: 9.408733603777364e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 105900 Loss: 1.1049287422792986e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 105950 Loss: 3.078828740399331e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106000 Loss: 6.717480573570356e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 106050 Loss: 0.00021197878231760114 Accuracy: 98.72618865966797\n",
      "Iteration: 106100 Loss: 2.7809921448351815e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 106150 Loss: 2.691271401999984e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106200 Loss: 2.120607859978918e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 106250 Loss: 1.4778059266973287e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106300 Loss: 4.348681613919325e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106350 Loss: 4.879312473349273e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 106400 Loss: 5.485786095960066e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106450 Loss: 4.221843846607953e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106500 Loss: 3.265082705183886e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106550 Loss: 8.45603153720731e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 106600 Loss: 4.7491808800259605e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106650 Loss: 3.7443685869220644e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 106700 Loss: 1.801112716748321e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 106750 Loss: 5.691319165634923e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106800 Loss: 4.430038097780198e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106850 Loss: 1.8997128790942952e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106900 Loss: 7.562412065453827e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 106950 Loss: 4.3533264033612795e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 107000 Loss: 2.3133512513595633e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107050 Loss: 1.587348015164025e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107100 Loss: 5.7487581216264516e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107150 Loss: 3.965231735492125e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107200 Loss: 3.859656135318801e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107250 Loss: 3.175143137923442e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107300 Loss: 7.027704123174772e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 107350 Loss: 3.9488255424657837e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 107400 Loss: 3.8508700527017936e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107450 Loss: 4.0666254790266976e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107500 Loss: 3.721876419149339e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107550 Loss: 1.8825012375600636e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107600 Loss: 6.325104186544195e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107650 Loss: 4.4848842662759125e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107700 Loss: 5.234034688328393e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107750 Loss: 6.643866800004616e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107800 Loss: 4.002597779617645e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 107850 Loss: 2.6056402475660434e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 107900 Loss: 2.182432945119217e-05 Accuracy: 98.75\n",
      "Iteration: 107950 Loss: 0.00013920482888352126 Accuracy: 98.73809814453125\n",
      "Iteration: 108000 Loss: 3.155560261802748e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108050 Loss: 9.699744259705767e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108100 Loss: 3.452281089266762e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108150 Loss: 5.509639231604524e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108200 Loss: 1.7017537174979225e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108250 Loss: 7.035404269117862e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108300 Loss: 5.349233833840117e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108350 Loss: 1.4844983525108546e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108400 Loss: 1.2763720405928325e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108450 Loss: 4.844945215154439e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108500 Loss: 5.5081811296986416e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108550 Loss: 6.6416728259355295e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 108600 Loss: 5.465012873173691e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108650 Loss: 7.856440788600594e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108700 Loss: 5.8686920965556055e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 108750 Loss: 2.342229890928138e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108800 Loss: 5.392702587414533e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108850 Loss: 4.3153697333764285e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 108900 Loss: 3.3603710107854567e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 108950 Loss: 3.104253119090572e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109000 Loss: 1.9337294361321256e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 109050 Loss: 4.375104253995232e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 109100 Loss: 5.959503323538229e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109150 Loss: 0.00010337704588891938 Accuracy: 98.72618865966797\n",
      "Iteration: 109200 Loss: 4.660578997572884e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109250 Loss: 0.0001087383643607609 Accuracy: 98.72618865966797\n",
      "Iteration: 109300 Loss: 3.766956433537416e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 109350 Loss: 2.359222162340302e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109400 Loss: 3.340596231282689e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109450 Loss: 3.29357608279679e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109500 Loss: 2.5461868062848225e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 109550 Loss: 1.871447966550477e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109600 Loss: 1.9256811356171966e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109650 Loss: 3.108717646682635e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109700 Loss: 7.900305718067102e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 109750 Loss: 4.7518951760139316e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 109800 Loss: 1.8194039512309246e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 109850 Loss: 6.1521161114797e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 109900 Loss: 2.8615499104489572e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 109950 Loss: 1.4098996871325653e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 110000 Loss: 2.637337274791207e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 110050 Loss: 4.945452747051604e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110100 Loss: 4.6380908315768465e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110150 Loss: 3.4788285120157525e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110200 Loss: 8.60934960655868e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110250 Loss: 4.854843609791715e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 110300 Loss: 4.844826980843209e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 110350 Loss: 0.000112859474029392 Accuracy: 98.73809814453125\n",
      "Iteration: 110400 Loss: 3.781993291340768e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 110450 Loss: 1.1613837159529794e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110500 Loss: 3.863102392642759e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110550 Loss: 7.466160604963079e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110600 Loss: 3.0511102522723377e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110650 Loss: 4.22798257204704e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 110700 Loss: 0.00010580460366327316 Accuracy: 98.72618865966797\n",
      "Iteration: 110750 Loss: 2.8912645575474016e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110800 Loss: 8.565129974158481e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110850 Loss: 1.8144244677387178e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 110900 Loss: 7.788781658746302e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 110950 Loss: 1.3243824241726543e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 111000 Loss: 2.040810250036884e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111050 Loss: 3.7848552892683074e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111100 Loss: 1.1543532309588045e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111150 Loss: 2.9250191801111214e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111200 Loss: 3.938277222914621e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111250 Loss: 2.472823871357832e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111300 Loss: 3.9271639252547175e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111350 Loss: 3.79520351998508e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111400 Loss: 4.584863199852407e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111450 Loss: 3.578365431167185e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111500 Loss: 1.1771081517508719e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111550 Loss: 0.00010979283251799643 Accuracy: 98.72618865966797\n",
      "Iteration: 111600 Loss: 5.2652023441623896e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111650 Loss: 5.847722150065238e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 111700 Loss: 1.5007989304649527e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 111750 Loss: 4.09229178330861e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111800 Loss: 6.4102619035111275e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 111850 Loss: 2.2696764062857255e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111900 Loss: 3.7387846532510594e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 111950 Loss: 3.714728882187046e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112000 Loss: 6.0874608607264236e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112050 Loss: 3.1814419344300404e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112100 Loss: 7.602429832331836e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112150 Loss: 3.0698804039275274e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112200 Loss: 2.5952154828701168e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 112250 Loss: 1.6600159142399207e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112300 Loss: 4.5317905460251495e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 112350 Loss: 4.7770998207852244e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112400 Loss: 1.5151221077758237e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 112450 Loss: 0.00010275444219587371 Accuracy: 98.73809814453125\n",
      "Iteration: 112500 Loss: 1.9513285224093124e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112550 Loss: 4.065116081619635e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112600 Loss: 1.942213930306025e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112650 Loss: 3.993406062363647e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112700 Loss: 2.507465978851542e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 112750 Loss: 2.6819132472155616e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112800 Loss: 5.44737031304976e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 112850 Loss: 1.0994328476954252e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112900 Loss: 2.6293737391824834e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 112950 Loss: 5.3715430112788454e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113000 Loss: 8.764200174482539e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113050 Loss: 3.474620098131709e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 113100 Loss: 2.2617830836679786e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 113150 Loss: 3.655335240182467e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 113200 Loss: 6.6748530116456095e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 113250 Loss: 1.8473750969860703e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113300 Loss: 4.2518702684901655e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113350 Loss: 2.864108500943985e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 113400 Loss: 6.630488496739417e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113450 Loss: 5.388753561419435e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 113500 Loss: 6.439040589611977e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113550 Loss: 0.00014584686141461134 Accuracy: 98.73809814453125\n",
      "Iteration: 113600 Loss: 4.2737257899716496e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 113650 Loss: 4.184825957054272e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113700 Loss: 1.9828148651868105e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113750 Loss: 4.515362888923846e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113800 Loss: 1.1731608537957072e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113850 Loss: 2.4257378754555248e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 113900 Loss: 3.3007239835569635e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 113950 Loss: 0.00011883803381351754 Accuracy: 98.73809814453125\n",
      "Iteration: 114000 Loss: 3.337017187732272e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114050 Loss: 4.504710523178801e-05 Accuracy: 98.75\n",
      "Iteration: 114100 Loss: 1.2487741514632944e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114150 Loss: 1.2186334970465396e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114200 Loss: 3.221287261112593e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114250 Loss: 5.36048028152436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114300 Loss: 5.135578248882666e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 114350 Loss: 4.330987576395273e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 114400 Loss: 1.6050056729000062e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114450 Loss: 4.002890636911616e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114500 Loss: 8.01753867563093e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 114550 Loss: 1.8767033907352015e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114600 Loss: 9.580949154042173e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 114650 Loss: 3.2860159990377724e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 114700 Loss: 8.641410386189818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114750 Loss: 3.040704359591473e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114800 Loss: 5.412692644313211e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 114850 Loss: 0.00014917350199539214 Accuracy: 98.72618865966797\n",
      "Iteration: 114900 Loss: 3.9963950257515535e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 114950 Loss: 1.8117056242772378e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115000 Loss: 4.312378223403357e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 115050 Loss: 6.561645932379179e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 115100 Loss: 7.766178896417841e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 115150 Loss: 1.7606938854441978e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115200 Loss: 8.459704076813068e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 115250 Loss: 7.043014647933887e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 115300 Loss: 5.008614243706688e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115350 Loss: 6.392750219674781e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115400 Loss: 4.034914127259981e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 115450 Loss: 2.4866480089258403e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115500 Loss: 4.719476055470295e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115550 Loss: 5.709792458219454e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115600 Loss: 3.937713336199522e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115650 Loss: 2.666439104359597e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115700 Loss: 4.085667751496658e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115750 Loss: 8.925001020543277e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115800 Loss: 3.214921161998063e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 115850 Loss: 3.795458542299457e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 115900 Loss: 4.3623360397759825e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 115950 Loss: 4.764406548929401e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116000 Loss: 1.5260775398928672e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116050 Loss: 5.958131077932194e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116100 Loss: 7.050506246741861e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 116150 Loss: 1.027726466418244e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116200 Loss: 1.1112995707662776e-05 Accuracy: 98.75\n",
      "Iteration: 116250 Loss: 9.344032150693238e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116300 Loss: 1.2091265489289071e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116350 Loss: 4.475835885386914e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116400 Loss: 7.482946239179e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116450 Loss: 2.398038122919388e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116500 Loss: 4.1404986404813826e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116550 Loss: 1.4598373127228115e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116600 Loss: 3.3186144719365984e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116650 Loss: 8.463652193313465e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116700 Loss: 4.2251424019923434e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116750 Loss: 4.1944338590838015e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116800 Loss: 3.4077158488798887e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 116850 Loss: 4.483486191020347e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 116900 Loss: 3.1738050893181935e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 116950 Loss: 2.4913568267947994e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117000 Loss: 2.3679847799940035e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117050 Loss: 2.8426469725673087e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117100 Loss: 7.996868589543737e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 117150 Loss: 2.8693721105810255e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117200 Loss: 1.7111897250288166e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 117250 Loss: 3.914885746780783e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117300 Loss: 6.888895586598665e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117350 Loss: 3.3621858165133744e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117400 Loss: 2.0067340301466174e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117450 Loss: 7.2449061008228455e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 117500 Loss: 8.107162284431979e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 117550 Loss: 1.2463543498597573e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 117600 Loss: 7.186437142081559e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117650 Loss: 1.7302030755672604e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117700 Loss: 2.7379064704291523e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117750 Loss: 4.558760338113643e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 117800 Loss: 2.472297455824446e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117850 Loss: 7.091581210261211e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117900 Loss: 2.0852543457294814e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 117950 Loss: 2.154418507416267e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 118000 Loss: 7.713206287007779e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 118050 Loss: 1.4115124940872192e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118100 Loss: 4.796123903361149e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118150 Loss: 1.1799280400737189e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118200 Loss: 1.6891522136575077e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 118250 Loss: 4.2883315472863615e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118300 Loss: 3.7133610021555796e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118350 Loss: 1.6408705050707795e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118400 Loss: 0.0001798866142053157 Accuracy: 98.75\n",
      "Iteration: 118450 Loss: 1.924091520777438e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118500 Loss: 7.11767133907415e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118550 Loss: 2.339354068681132e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118600 Loss: 3.683969771373086e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118650 Loss: 2.166327612940222e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118700 Loss: 1.2738035366055556e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118750 Loss: 0.00012126976798754185 Accuracy: 98.73809814453125\n",
      "Iteration: 118800 Loss: 0.00010816234134836122 Accuracy: 98.72618865966797\n",
      "Iteration: 118850 Loss: 2.1360961909522302e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118900 Loss: 1.583722587383818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 118950 Loss: 5.066474841441959e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119000 Loss: 9.079603842110373e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 119050 Loss: 2.099379344144836e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119100 Loss: 2.7822849006042816e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 119150 Loss: 1.7731194020598195e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119200 Loss: 1.8051765437121503e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119250 Loss: 5.7050056057050824e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119300 Loss: 2.1330803065211512e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119350 Loss: 4.695545794675127e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119400 Loss: 3.5515269701136276e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119450 Loss: 2.628337279020343e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119500 Loss: 6.900310836499557e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119550 Loss: 8.546847675461322e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 119600 Loss: 3.4410775697324425e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119650 Loss: 1.906647958094254e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119700 Loss: 1.405572947987821e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 119750 Loss: 5.385553595260717e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119800 Loss: 2.21634309127694e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119850 Loss: 1.0060894965135958e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119900 Loss: 2.6332572815590538e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 119950 Loss: 8.481121039949358e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120000 Loss: 1.6523368685739115e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 120050 Loss: 5.6843320635380223e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120100 Loss: 3.873948116961401e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 120150 Loss: 2.6781908673001453e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120200 Loss: 3.34593714796938e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120250 Loss: 1.656456879572943e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120300 Loss: 2.142164157703519e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120350 Loss: 6.611588469240814e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 120400 Loss: 1.9337036064825952e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 120450 Loss: 3.1880190363153815e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120500 Loss: 8.367835835088044e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120550 Loss: 2.2945405362406746e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 120600 Loss: 2.8437376386136748e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120650 Loss: 3.2055802876129746e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120700 Loss: 6.852308433735743e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120750 Loss: 5.317488557921024e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 120800 Loss: 4.751019150717184e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120850 Loss: 3.125907460344024e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120900 Loss: 3.0852152121951804e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 120950 Loss: 1.3463806681102142e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121000 Loss: 1.2032516679028049e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121050 Loss: 5.364173193811439e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121100 Loss: 4.8092879296746105e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 121150 Loss: 7.81276830821298e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121200 Loss: 9.306695574196056e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 121250 Loss: 6.297099753282964e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121300 Loss: 3.292492692708038e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121350 Loss: 5.9949384194624145e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 121400 Loss: 5.89189694437664e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 121450 Loss: 4.47054480900988e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121500 Loss: 3.935995482606813e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121550 Loss: 3.478110011201352e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121600 Loss: 2.216966822743416e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121650 Loss: 4.2222301999572664e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121700 Loss: 9.355907423014287e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 121750 Loss: 3.0089309802860953e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121800 Loss: 2.2988830096437596e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 121850 Loss: 5.7658831792650744e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 121900 Loss: 3.350237602717243e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 121950 Loss: 7.62232521083206e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122000 Loss: 3.6384371924214065e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 122050 Loss: 1.461612555431202e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122100 Loss: 5.071998748462647e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122150 Loss: 3.4354710805928335e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122200 Loss: 3.765694418689236e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122250 Loss: 2.6768469979288056e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122300 Loss: 7.364924385910854e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122350 Loss: 1.8948134311358444e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122400 Loss: 1.182035430247197e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 122450 Loss: 2.2296786482911557e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122500 Loss: 3.5464996472001076e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122550 Loss: 5.9863217757083476e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122600 Loss: 3.332848791615106e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122650 Loss: 4.110360168851912e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122700 Loss: 2.434932866890449e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122750 Loss: 2.5049223040696234e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 122800 Loss: 1.2232401786604896e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122850 Loss: 4.009168696939014e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122900 Loss: 1.5081878700584639e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 122950 Loss: 2.1242927687126212e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123000 Loss: 6.16708566667512e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123050 Loss: 1.5377524960058508e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 123100 Loss: 2.9314227504073642e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 123150 Loss: 2.1107354768901132e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123200 Loss: 6.139821198303252e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123250 Loss: 3.16700106850476e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 123300 Loss: 6.969350215513259e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123350 Loss: 4.389965397422202e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123400 Loss: 1.1753367061828612e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 123450 Loss: 4.092957533430308e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123500 Loss: 8.095866178337019e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 123550 Loss: 3.263338658143766e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123600 Loss: 4.630668263416737e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123650 Loss: 1.7554746591486037e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123700 Loss: 2.5292340069427155e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123750 Loss: 2.490411134203896e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123800 Loss: 5.6329936342081055e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 123850 Loss: 0.00012995439465157688 Accuracy: 98.72618865966797\n",
      "Iteration: 123900 Loss: 8.436956704827026e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 123950 Loss: 4.8628112381265964e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 124000 Loss: 4.3850424845004454e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124050 Loss: 2.857099025277421e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124100 Loss: 8.466257895634044e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 124150 Loss: 2.5178685973514803e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124200 Loss: 5.66408334634616e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 124250 Loss: 5.3916824981570244e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124300 Loss: 1.8990247554029338e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124350 Loss: 0.00010390875831944868 Accuracy: 98.72618865966797\n",
      "Iteration: 124400 Loss: 2.134671012754552e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124450 Loss: 3.6901357816532254e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 124500 Loss: 1.7992084394791164e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 124550 Loss: 2.6738294764072634e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124600 Loss: 6.666561603196897e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 124650 Loss: 7.033030851744115e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124700 Loss: 3.01893669529818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124750 Loss: 4.057889964315109e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124800 Loss: 6.616631435463205e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124850 Loss: 2.5750447093741968e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124900 Loss: 2.157707785954699e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 124950 Loss: 7.364941120613366e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125000 Loss: 5.478084858623333e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125050 Loss: 3.160702544846572e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125100 Loss: 4.1574836359359324e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125150 Loss: 7.081212243065238e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125200 Loss: 5.49995711480733e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125250 Loss: 4.71762650704477e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125300 Loss: 2.61922505160328e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125350 Loss: 6.565025250893086e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125400 Loss: 3.039826594886108e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 125450 Loss: 3.5349337849766016e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125500 Loss: 6.287697033258155e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125550 Loss: 1.8265278413309716e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125600 Loss: 1.6882057025213726e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 125650 Loss: 7.50296821934171e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125700 Loss: 7.835496035113465e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 125750 Loss: 2.6947724109049886e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 125800 Loss: 5.009429150959477e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125850 Loss: 2.04013613256393e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125900 Loss: 1.746983798511792e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 125950 Loss: 8.186530067177955e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 126000 Loss: 4.1743718611542135e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126050 Loss: 1.9573155896068783e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 126100 Loss: 4.161948891123757e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126150 Loss: 1.775681448634714e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126200 Loss: 2.6589057597448118e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126250 Loss: 2.8649397791014053e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126300 Loss: 3.362687129992992e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126350 Loss: 2.3126871383283287e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126400 Loss: 6.421004218282178e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126450 Loss: 3.872712477459572e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126500 Loss: 0.00012618201435543597 Accuracy: 98.72618865966797\n",
      "Iteration: 126550 Loss: 2.9454409741447307e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 126600 Loss: 6.413364985746739e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 126650 Loss: 3.2771196856629103e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126700 Loss: 7.488403207389638e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126750 Loss: 7.3912442530854605e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 126800 Loss: 1.4749871297681239e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126850 Loss: 8.475347567582503e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126900 Loss: 8.921172411646694e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 126950 Loss: 1.4492679838440381e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 127000 Loss: 1.86239012691658e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 127050 Loss: 4.387821536511183e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 127100 Loss: 9.183212387142703e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127150 Loss: 2.714144102355931e-06 Accuracy: 98.70238494873047\n",
      "Iteration: 127200 Loss: 8.943558896135073e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 127250 Loss: 1.304375928157242e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127300 Loss: 3.232908056816086e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127350 Loss: 5.845510895596817e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127400 Loss: 0.00014804943930357695 Accuracy: 98.72618865966797\n",
      "Iteration: 127450 Loss: 3.316594666102901e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 127500 Loss: 5.1168655772926286e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127550 Loss: 2.905850124079734e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127600 Loss: 4.0314553189091384e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 127650 Loss: 9.118493471760303e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127700 Loss: 3.881128122884547e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 127750 Loss: 2.7768306608777493e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127800 Loss: 2.2959839043323882e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127850 Loss: 8.078956852841657e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 127900 Loss: 4.7789410018594936e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 127950 Loss: 4.095623353350675e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 128000 Loss: 2.1025918613304384e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128050 Loss: 1.5402420103782788e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128100 Loss: 8.738626092963386e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 128150 Loss: 1.3029173715040088e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 128200 Loss: 6.822739669587463e-05 Accuracy: 98.75\n",
      "Iteration: 128250 Loss: 4.5481938286684453e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128300 Loss: 2.222671355411876e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128350 Loss: 9.526335634291172e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128400 Loss: 4.463943696464412e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128450 Loss: 2.1727602870669216e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128500 Loss: 1.167294703918742e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128550 Loss: 1.2436000361049082e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128600 Loss: 4.067676127306186e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128650 Loss: 5.6673219660297036e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128700 Loss: 9.477440471528098e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 128750 Loss: 3.0342529498739168e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128800 Loss: 9.712620339996647e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 128850 Loss: 4.784852353623137e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128900 Loss: 4.141467798035592e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 128950 Loss: 6.409984780475497e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129000 Loss: 4.6737241063965484e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129050 Loss: 2.2439540771301836e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 129100 Loss: 2.2182035536388867e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129150 Loss: 2.7672238502418622e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 129200 Loss: 2.033626878983341e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129250 Loss: 4.249269295542035e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 129300 Loss: 6.207617116160691e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129350 Loss: 1.3244741239759605e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129400 Loss: 1.740422158036381e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129450 Loss: 5.594092363025993e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129500 Loss: 7.11726097506471e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129550 Loss: 4.588936280924827e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129600 Loss: 4.2976007534889504e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129650 Loss: 1.8144100977224298e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129700 Loss: 0.00011987524339929223 Accuracy: 98.72618865966797\n",
      "Iteration: 129750 Loss: 1.8580311007099226e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129800 Loss: 4.5545155444415286e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129850 Loss: 6.463853424065746e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 129900 Loss: 1.2585051990754437e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 129950 Loss: 3.445132961132913e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 130000 Loss: 9.003254490380641e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 130050 Loss: 9.733605111250654e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 130100 Loss: 5.0815553549909964e-05 Accuracy: 98.75\n",
      "Iteration: 130150 Loss: 1.6436735677416436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130200 Loss: 1.644240546738729e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 130250 Loss: 4.868311225436628e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 130300 Loss: 1.2858234185841866e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130350 Loss: 2.3111608243198134e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130400 Loss: 2.298391700605862e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130450 Loss: 3.701161767821759e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130500 Loss: 7.991116945049725e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 130550 Loss: 2.0398325432324782e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130600 Loss: 1.541329584142659e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130650 Loss: 3.520050449878909e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130700 Loss: 5.1533956138882786e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130750 Loss: 2.5606312192394398e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130800 Loss: 2.604125620564446e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130850 Loss: 2.9481872843462043e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 130900 Loss: 2.578473140602e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 130950 Loss: 9.480477274337318e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 131000 Loss: 1.989393240364734e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131050 Loss: 4.605160211212933e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131100 Loss: 3.6217490560375154e-05 Accuracy: 98.75\n",
      "Iteration: 131150 Loss: 5.146906551090069e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131200 Loss: 2.737579598033335e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131250 Loss: 1.654673360462766e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131300 Loss: 6.489087536465377e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 131350 Loss: 4.032023571198806e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131400 Loss: 2.078852594422642e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131450 Loss: 4.989226454199525e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 131500 Loss: 4.371882459963672e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131550 Loss: 6.456308619817719e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131600 Loss: 6.49823559797369e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131650 Loss: 2.5837924113147892e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131700 Loss: 8.234381675720215e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131750 Loss: 2.5961351639125496e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131800 Loss: 8.347844413947314e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 131850 Loss: 4.4680546125164255e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131900 Loss: 2.841312925738748e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 131950 Loss: 1.4035231288289651e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132000 Loss: 4.3739270040532574e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132050 Loss: 4.774786430061795e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132100 Loss: 4.0108403482008725e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 132150 Loss: 3.378745532245375e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132200 Loss: 1.6541594959562644e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 132250 Loss: 3.5482946259435266e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132300 Loss: 7.196396472863853e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 132350 Loss: 1.5745017662993632e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132400 Loss: 4.373105548438616e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 132450 Loss: 6.388467681972543e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 132500 Loss: 9.12118503038073e-06 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 132550 Loss: 4.255617022863589e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 132600 Loss: 6.644850145676173e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 132650 Loss: 3.0513430829159915e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 132700 Loss: 1.861575583461672e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132750 Loss: 3.682378155644983e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132800 Loss: 8.01058490651485e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 132850 Loss: 5.56830782443285e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132900 Loss: 1.2868275007349439e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 132950 Loss: 1.6749116184655577e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133000 Loss: 2.2826510758022778e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133050 Loss: 3.714837657753378e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133100 Loss: 5.2266474085627124e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133150 Loss: 1.0482256584509742e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133200 Loss: 2.070082882710267e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 133250 Loss: 1.1962833923462313e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133300 Loss: 9.206938557326794e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 133350 Loss: 3.398171975277364e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133400 Loss: 1.59640985657461e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133450 Loss: 2.56508919846965e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133500 Loss: 1.5770862091812887e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 133550 Loss: 4.994491609977558e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133600 Loss: 1.8836350136552937e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133650 Loss: 2.8202393878018484e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133700 Loss: 3.626702891779132e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 133750 Loss: 3.285567800048739e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 133800 Loss: 4.363716288935393e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 133850 Loss: 9.140307156485505e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 133900 Loss: 2.8406093406374566e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 133950 Loss: 1.234684714290779e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 134000 Loss: 1.2860981769335922e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134050 Loss: 1.711705408524722e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134100 Loss: 6.489959196187556e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134150 Loss: 1.6948293705354445e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134200 Loss: 2.3839815185056068e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 134250 Loss: 6.756837683496997e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134300 Loss: 3.065120108658448e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134350 Loss: 6.834334726590896e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 134400 Loss: 1.4715080396854319e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134450 Loss: 4.548675406113034e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 134500 Loss: 2.6380695999250747e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134550 Loss: 3.5938778637500945e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 134600 Loss: 2.2054906366975047e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134650 Loss: 6.347634189296514e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134700 Loss: 9.917911256707157e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 134750 Loss: 4.8794685426400974e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134800 Loss: 4.589769378071651e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134850 Loss: 8.465113205602393e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134900 Loss: 2.6222500309813768e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 134950 Loss: 1.768317451933399e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135000 Loss: 2.10273378797865e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 135050 Loss: 1.8546206774772145e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135100 Loss: 4.758692011819221e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 135150 Loss: 1.6494452211190946e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135200 Loss: 2.3422066078637727e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135250 Loss: 5.0846236263168976e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135300 Loss: 4.530805199465249e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 135350 Loss: 3.826935062534176e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 135400 Loss: 3.207308691344224e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 135450 Loss: 3.732912955456413e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 135500 Loss: 2.0384713650400954e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 135550 Loss: 4.110511508770287e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135600 Loss: 3.126593219349161e-05 Accuracy: 98.75\n",
      "Iteration: 135650 Loss: 2.1817821107106283e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 135700 Loss: 2.916573066613637e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135750 Loss: 2.441921242279932e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135800 Loss: 2.4895689421100542e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135850 Loss: 2.25124185817549e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135900 Loss: 1.4799084965488873e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 135950 Loss: 1.4190540241543204e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136000 Loss: 6.763139936083462e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 136050 Loss: 5.5523018090752885e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136100 Loss: 2.2489921320811845e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136150 Loss: 9.049224900081754e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 136200 Loss: 3.501155151752755e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136250 Loss: 3.27137386193499e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136300 Loss: 2.618498183437623e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136350 Loss: 2.202704308729153e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136400 Loss: 3.4673205391300144e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 136450 Loss: 2.3034834157442674e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136500 Loss: 4.442731005838141e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 136550 Loss: 2.614798904687632e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136600 Loss: 3.586463935789652e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 136650 Loss: 2.095559102599509e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136700 Loss: 3.374858351889998e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136750 Loss: 2.960900565085467e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136800 Loss: 8.136717951856554e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 136850 Loss: 2.3374618649540935e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 136900 Loss: 1.122794310504105e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 136950 Loss: 8.347142284037545e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137000 Loss: 7.525987894041464e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 137050 Loss: 2.4648064936627634e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137100 Loss: 4.7891910071484745e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137150 Loss: 3.42259336321149e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137200 Loss: 4.645855005946942e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137250 Loss: 4.727988562081009e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137300 Loss: 3.500562161207199e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137350 Loss: 2.37690910580568e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137400 Loss: 2.1821988411829807e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137450 Loss: 4.1780531319091097e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137500 Loss: 1.7175278117065318e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137550 Loss: 5.161121589480899e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 137600 Loss: 1.982395588129293e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 137650 Loss: 2.9800306947436184e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137700 Loss: 0.00012007594341412187 Accuracy: 98.72618865966797\n",
      "Iteration: 137750 Loss: 2.750900421233382e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137800 Loss: 3.523354280332569e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 137850 Loss: 8.754343434702605e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 137900 Loss: 0.0001067057964974083 Accuracy: 98.72618865966797\n",
      "Iteration: 137950 Loss: 1.9543382222764194e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138000 Loss: 5.784231689176522e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 138050 Loss: 2.0257557480363175e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 138100 Loss: 2.8830534574808553e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 138150 Loss: 3.303991616121493e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 138200 Loss: 1.9704662918229587e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 138250 Loss: 2.2549862478626892e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 138300 Loss: 5.322022479958832e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138350 Loss: 6.485632184194401e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138400 Loss: 9.098089321923908e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 138450 Loss: 0.00011210072989342734 Accuracy: 98.72618865966797\n",
      "Iteration: 138500 Loss: 4.940251074003754e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 138550 Loss: 5.9233963838778436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138600 Loss: 2.5835837732302025e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138650 Loss: 2.0009609215776436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138700 Loss: 1.0283280062139966e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138750 Loss: 3.255191768403165e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138800 Loss: 2.5393383111804724e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138850 Loss: 1.2776171388395596e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138900 Loss: 1.8069875295623206e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 138950 Loss: 1.782093931979034e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139000 Loss: 5.083182986709289e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139050 Loss: 1.8887141777668148e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 139100 Loss: 3.637835834524594e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139150 Loss: 7.596474461024627e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 139200 Loss: 3.3003885619109496e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 139250 Loss: 2.5909641408361495e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139300 Loss: 2.8377202397678047e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139350 Loss: 2.941060120065231e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139400 Loss: 5.12432525283657e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139450 Loss: 1.0669410585251171e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139500 Loss: 4.9977210437646136e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139550 Loss: 3.046774691028986e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139600 Loss: 2.9816286769346334e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 139650 Loss: 1.8265607650391757e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139700 Loss: 2.0479426439123927e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 139750 Loss: 1.956740925379563e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 139800 Loss: 9.649361345509533e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 139850 Loss: 1.9106131730950437e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 139900 Loss: 1.7546892195241526e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 139950 Loss: 1.1098791219410487e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140000 Loss: 1.5113673725863919e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140050 Loss: 9.768118616193533e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 140100 Loss: 1.8291279047844e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140150 Loss: 3.114268838544376e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140200 Loss: 1.7581489373696968e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 140250 Loss: 2.2252386770560406e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140300 Loss: 5.3937386837787926e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140350 Loss: 8.488733146805316e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140400 Loss: 6.411070353351533e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140450 Loss: 5.015815077058505e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 140500 Loss: 1.7882475731312297e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140550 Loss: 1.1643786820059177e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140600 Loss: 1.4313552128442097e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140650 Loss: 7.453435682691634e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 140700 Loss: 3.9663096686126664e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 140750 Loss: 2.721152122830972e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140800 Loss: 1.0402977750345599e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140850 Loss: 1.8174616343458183e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140900 Loss: 1.2091530152247287e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 140950 Loss: 2.2041117517801467e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 141000 Loss: 2.2719479602528736e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141050 Loss: 5.809591675642878e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141100 Loss: 1.7704789570416324e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141150 Loss: 4.1720490116858855e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141200 Loss: 1.8886610632762313e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 141250 Loss: 8.475630579596327e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 141300 Loss: 1.3801119166600984e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141350 Loss: 2.7183627025806345e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141400 Loss: 3.877902418025769e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141450 Loss: 1.604666067578364e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141500 Loss: 1.3180918358557392e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 141550 Loss: 8.071381307672709e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141600 Loss: 3.5728538932744414e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141650 Loss: 2.8866568754892796e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141700 Loss: 7.205135625554249e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141750 Loss: 5.1419236115179956e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141800 Loss: 2.3282895199372433e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 141850 Loss: 1.3845452485838905e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 141900 Loss: 2.3292026526178233e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 141950 Loss: 2.0441710148588754e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142000 Loss: 4.242048453306779e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 142050 Loss: 1.9700695702340454e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142100 Loss: 6.019229658704717e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 142150 Loss: 1.3091978871671017e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142200 Loss: 2.8538383048726246e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142250 Loss: 2.9157665267121047e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142300 Loss: 0.00012046951451338828 Accuracy: 98.73809814453125\n",
      "Iteration: 142350 Loss: 6.219129136297852e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 142400 Loss: 5.040219912189059e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142450 Loss: 1.7575141100678593e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142500 Loss: 3.302812183392234e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142550 Loss: 1.4864557442706428e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 142600 Loss: 3.4080958357662894e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 142650 Loss: 7.527785055572167e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 142700 Loss: 4.896287282463163e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142750 Loss: 6.669346475973725e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 142800 Loss: 4.68684220322757e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 142850 Loss: 4.6015997213544324e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 142900 Loss: 2.9335183171497192e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 142950 Loss: 2.0045030396431684e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143000 Loss: 8.225460078392643e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 143050 Loss: 3.7337584217311814e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143100 Loss: 2.43849353864789e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143150 Loss: 4.162158347753575e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 143200 Loss: 9.970684004656505e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 143250 Loss: 1.5424051525769755e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143300 Loss: 1.2689341929217335e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143350 Loss: 1.3355765986489132e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143400 Loss: 2.877904989873059e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143450 Loss: 2.3600730401085457e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 143500 Loss: 4.8310874262824655e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143550 Loss: 1.3834826859238092e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 143600 Loss: 2.606020098028239e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143650 Loss: 1.1945325240958482e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143700 Loss: 6.410690730263013e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 143750 Loss: 3.3354663173668087e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143800 Loss: 3.337491216370836e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143850 Loss: 2.1457415641634725e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 143900 Loss: 4.516840272117406e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 143950 Loss: 2.9323458875296637e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144000 Loss: 0.00011139781418023631 Accuracy: 98.73809814453125\n",
      "Iteration: 144050 Loss: 1.184960001410218e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144100 Loss: 1.5178196917986497e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144150 Loss: 6.780165676900651e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 144200 Loss: 1.8061698938254267e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144250 Loss: 3.210020850019646e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 144300 Loss: 0.0001258826960111037 Accuracy: 98.72618865966797\n",
      "Iteration: 144350 Loss: 7.431451194861438e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 144400 Loss: 1.1396108675398864e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144450 Loss: 1.941830305440817e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 144500 Loss: 4.600798638421111e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144550 Loss: 1.8892782463808544e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144600 Loss: 4.6252942411229014e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144650 Loss: 3.365320299053565e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144700 Loss: 3.579610392989707e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 144750 Loss: 5.015837359678699e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 144800 Loss: 4.135266499361023e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 144850 Loss: 2.4153199774445966e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 144900 Loss: 7.417290908051655e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 144950 Loss: 3.367260069353506e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145000 Loss: 9.932085959007964e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145050 Loss: 3.1948064133757725e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145100 Loss: 1.727628114167601e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 145150 Loss: 1.77744932443602e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145200 Loss: 2.5878980522975326e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 145250 Loss: 4.614896897692233e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145300 Loss: 5.137204789207317e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145350 Loss: 2.8619999284273945e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145400 Loss: 2.566241892054677e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145450 Loss: 0.00013771162775810808 Accuracy: 98.72618865966797\n",
      "Iteration: 145500 Loss: 6.785822279198328e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 145550 Loss: 2.8701510018436238e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145600 Loss: 3.304938582004979e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145650 Loss: 5.1659817472682334e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 145700 Loss: 4.4128086301498115e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 145750 Loss: 3.311308682896197e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 145800 Loss: 4.4908451855008025e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 145850 Loss: 6.283179391175508e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 145900 Loss: 9.938008588505909e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 145950 Loss: 3.7528268876485527e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146000 Loss: 2.378300814598333e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 146050 Loss: 1.9373685063328594e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146100 Loss: 2.66400402324507e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146150 Loss: 3.1839479106565705e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 146200 Loss: 2.3936954676173627e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146250 Loss: 2.4293098249472678e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 146300 Loss: 1.5030019312689546e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146350 Loss: 3.200818537152372e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146400 Loss: 2.9049078875686973e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146450 Loss: 1.7810967619880103e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146500 Loss: 8.956721285358071e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146550 Loss: 1.1830836228909902e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 146600 Loss: 2.7893420337932184e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146650 Loss: 1.605085526534822e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146700 Loss: 1.2330524441495072e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146750 Loss: 5.1370727305766195e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146800 Loss: 1.9832667021546513e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146850 Loss: 4.278010601410642e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146900 Loss: 3.391354039194994e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 146950 Loss: 4.8323032387997955e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147000 Loss: 1.3022477105550934e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 147050 Loss: 2.7206782760913484e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 147100 Loss: 1.2028507626382634e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 147150 Loss: 9.113308624364436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147200 Loss: 2.152154775103554e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147250 Loss: 1.1994574379059486e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147300 Loss: 1.7439891735193669e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 147350 Loss: 3.836289397440851e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147400 Loss: 5.316236183716683e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 147450 Loss: 2.8491520424722694e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147500 Loss: 2.7528822101885453e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147550 Loss: 3.637498593889177e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147600 Loss: 3.153046054649167e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147650 Loss: 5.735857484978624e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147700 Loss: 4.442361750989221e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147750 Loss: 6.987936649238691e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147800 Loss: 2.580961700004991e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 147850 Loss: 2.7735188268707134e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147900 Loss: 4.706601976067759e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 147950 Loss: 3.4092347050318494e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 148000 Loss: 4.0400238503934816e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148050 Loss: 2.8977963665965945e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148100 Loss: 1.1643599464150611e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148150 Loss: 4.9770049372455105e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 148200 Loss: 5.7546487369108945e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 148250 Loss: 1.7312995623797178e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148300 Loss: 1.718765815894585e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148350 Loss: 3.280110831838101e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 148400 Loss: 3.387979813851416e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148450 Loss: 1.2568252714117989e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148500 Loss: 5.3826497605768964e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148550 Loss: 1.480877836002037e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148600 Loss: 1.0435069270897657e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148650 Loss: 0.00010564865806372836 Accuracy: 98.72618865966797\n",
      "Iteration: 148700 Loss: 1.6208434317377396e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148750 Loss: 3.0120238079689443e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148800 Loss: 5.684179632226005e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148850 Loss: 1.3355044757190626e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148900 Loss: 4.0386243199463934e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 148950 Loss: 5.2242554374970496e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149000 Loss: 1.7974190996028483e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149050 Loss: 3.4911347029265016e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 149100 Loss: 1.3137494534021243e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 149150 Loss: 2.5917261154972948e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 149200 Loss: 3.228042260161601e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 149250 Loss: 8.831291779642925e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 149300 Loss: 1.4357444342749659e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 149350 Loss: 4.229561091051437e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149400 Loss: 5.724373841076158e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 149450 Loss: 2.700072764127981e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149500 Loss: 2.9956265279906802e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149550 Loss: 3.1944323382049333e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 149600 Loss: 3.240772275603376e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149650 Loss: 1.9255312508903444e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149700 Loss: 2.3687527573201805e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149750 Loss: 3.2292080049955985e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 149800 Loss: 2.441883407300338e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 149850 Loss: 1.5605715816491283e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149900 Loss: 4.8053188947960734e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 149950 Loss: 9.752433834364638e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 150000 Loss: 2.6595165763865225e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150050 Loss: 1.069848076440394e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150100 Loss: 3.806272434303537e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150150 Loss: 1.4889109479554463e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150200 Loss: 8.809434075374156e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150250 Loss: 2.308259718120098e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150300 Loss: 3.548417589627206e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 150350 Loss: 3.7661517126252875e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150400 Loss: 3.7318613976822235e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 150450 Loss: 1.4348217519000173e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150500 Loss: 1.0521372132643592e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150550 Loss: 2.463187774992548e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150600 Loss: 5.96960990151274e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 150650 Loss: 1.9504601368680596e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 150700 Loss: 2.9787763651256682e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 150750 Loss: 3.569206819520332e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150800 Loss: 3.5775719879893586e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 150850 Loss: 8.126567990984768e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 150900 Loss: 4.118917422601953e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 150950 Loss: 8.85063418536447e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151000 Loss: 1.8952083337353542e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151050 Loss: 2.4088552891043946e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151100 Loss: 3.0997955036582425e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151150 Loss: 5.077924652141519e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151200 Loss: 2.6522977350396104e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151250 Loss: 2.2085710952524096e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151300 Loss: 2.2059190087020397e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 151350 Loss: 3.4226755815325305e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151400 Loss: 1.784627056622412e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151450 Loss: 8.270510079455562e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 151500 Loss: 1.983878610190004e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151550 Loss: 2.6407658879179507e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151600 Loss: 6.462017336161807e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151650 Loss: 1.8000348063651472e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151700 Loss: 2.2498414182337e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 151750 Loss: 2.66958013526164e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 151800 Loss: 3.2560994441155344e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151850 Loss: 6.142941856523976e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 151900 Loss: 1.3889406545786187e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 151950 Loss: 1.2664222595049068e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152000 Loss: 2.849477095878683e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 152050 Loss: 1.5794336150065647e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 152100 Loss: 4.663975050789304e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 152150 Loss: 1.2953525583725423e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152200 Loss: 9.560454827806097e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 152250 Loss: 0.00014368236588779837 Accuracy: 98.72618865966797\n",
      "Iteration: 152300 Loss: 3.3094052923843265e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152350 Loss: 2.5974924938054755e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152400 Loss: 2.5020230168593116e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 152450 Loss: 1.440366486349376e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 152500 Loss: 1.5867623005760834e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152550 Loss: 3.659648064058274e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152600 Loss: 2.635975215525832e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152650 Loss: 1.31151573441457e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 152700 Loss: 1.157175483967876e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 152750 Loss: 3.218781785108149e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152800 Loss: 4.376622200652491e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 152850 Loss: 2.473999120411463e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152900 Loss: 1.4280672075983603e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 152950 Loss: 2.5562932933098637e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153000 Loss: 2.151996159227565e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153050 Loss: 8.238828013418242e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153100 Loss: 2.668898332558456e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 153150 Loss: 2.08241431209899e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 153200 Loss: 4.5654082896362524e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 153250 Loss: 1.0418602869322058e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 153300 Loss: 8.107958819891792e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 153350 Loss: 1.070539110514801e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153400 Loss: 2.271346966153942e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 153450 Loss: 1.4481694961432368e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 153500 Loss: 2.2897831513546407e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153550 Loss: 1.541159872431308e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153600 Loss: 3.20407998515293e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 153650 Loss: 3.444306639721617e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 153700 Loss: 1.881743264675606e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 153750 Loss: 3.823976840067189e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 153800 Loss: 5.0182734412373975e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153850 Loss: 0.00011782409274019301 Accuracy: 98.72618865966797\n",
      "Iteration: 153900 Loss: 3.662162271211855e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 153950 Loss: 1.3510663848137483e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154000 Loss: 1.187977977679111e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154050 Loss: 3.2861797080840915e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154100 Loss: 2.0862664314336143e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154150 Loss: 2.2505086235469207e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154200 Loss: 3.8549893361050636e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154250 Loss: 1.2925854207423981e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154300 Loss: 6.703350209136261e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 154350 Loss: 2.512035280233249e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154400 Loss: 4.3632480810629204e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154450 Loss: 1.8154356666855165e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 154500 Loss: 2.2197418729774654e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154550 Loss: 2.1750298401457258e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 154600 Loss: 3.105650466750376e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 154650 Loss: 1.2305136806389783e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154700 Loss: 1.6465981389046647e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 154750 Loss: 8.742965292185545e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 154800 Loss: 9.204611160384957e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 154850 Loss: 5.2266300372139085e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 154900 Loss: 5.325381789589301e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 154950 Loss: 5.747654995502671e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 155000 Loss: 1.4876712839395623e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 155050 Loss: 1.1850996088469401e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 155100 Loss: 2.166196281905286e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 155150 Loss: 1.7035265045706183e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 155200 Loss: 7.100619313860079e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 155250 Loss: 8.79773506312631e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 155300 Loss: 3.171711796312593e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155350 Loss: 1.5464505850104615e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155400 Loss: 8.374626304430421e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 155450 Loss: 9.058599971467629e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 155500 Loss: 1.739127947075758e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155550 Loss: 2.732083885348402e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155600 Loss: 3.0629664252046496e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155650 Loss: 3.2626303436700255e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155700 Loss: 4.7220339183695614e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155750 Loss: 2.8900478355353698e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155800 Loss: 2.3176646209321916e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 155850 Loss: 1.3815874808642548e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 155900 Loss: 2.8712882340187207e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 155950 Loss: 3.804547304753214e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156000 Loss: 1.3495432540366892e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156050 Loss: 4.564377013593912e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 156100 Loss: 1.716026963549666e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 156150 Loss: 1.3918020158598665e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 156200 Loss: 1.1214962796657346e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156250 Loss: 3.690149242174812e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156300 Loss: 1.2051651765432325e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 156350 Loss: 8.31017314339988e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 156400 Loss: 2.3010610675555654e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156450 Loss: 1.7624995962250978e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156500 Loss: 1.9069189875153825e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156550 Loss: 3.562516940291971e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 156600 Loss: 2.0775345547008328e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156650 Loss: 5.149022035766393e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156700 Loss: 1.846580380515661e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156750 Loss: 8.245656317740213e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 156800 Loss: 5.3985263548383955e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 156850 Loss: 2.8549591661430895e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 156900 Loss: 1.7438939039493562e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 156950 Loss: 1.6944668459473178e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157000 Loss: 1.9881685147993267e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157050 Loss: 0.00010365719208493829 Accuracy: 98.72618865966797\n",
      "Iteration: 157100 Loss: 9.077810318558477e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 157150 Loss: 1.8972572433995083e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157200 Loss: 5.630066152662039e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157250 Loss: 2.3929211238282733e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157300 Loss: 1.9930874259443954e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157350 Loss: 2.5629194624343654e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 157400 Loss: 4.303989771869965e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157450 Loss: 3.587697574403137e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157500 Loss: 1.9092176444246434e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157550 Loss: 1.3179615962144453e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157600 Loss: 3.987537638749927e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157650 Loss: 1.3467599274008535e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 157700 Loss: 1.5024186723167077e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157750 Loss: 4.0632457967149094e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157800 Loss: 3.189815106452443e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157850 Loss: 8.342658111359924e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 157900 Loss: 4.455388079804834e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 157950 Loss: 4.6290668251458555e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158000 Loss: 3.643697709776461e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158050 Loss: 3.621054702307447e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 158100 Loss: 2.3767286620568484e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158150 Loss: 8.070614421740174e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158200 Loss: 2.185155790357385e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158250 Loss: 5.288264583214186e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158300 Loss: 4.025739326607436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158350 Loss: 4.6777095121797174e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158400 Loss: 1.1088281098636799e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158450 Loss: 2.8963193472009152e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158500 Loss: 1.4888018085912336e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158550 Loss: 5.396950655267574e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158600 Loss: 4.5302320359041914e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158650 Loss: 2.264770955662243e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158700 Loss: 2.224932177341543e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158750 Loss: 2.8179003493278287e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158800 Loss: 4.645232911570929e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158850 Loss: 3.928968726540916e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 158900 Loss: 5.107376637170091e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 158950 Loss: 9.35726347961463e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 159000 Loss: 4.321827145759016e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159050 Loss: 7.836486474843696e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 159100 Loss: 2.3429432985722087e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 159150 Loss: 6.627896596000937e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 159200 Loss: 5.9704575505747925e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 159250 Loss: 2.9789585823891684e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159300 Loss: 3.7660491216229275e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159350 Loss: 1.0044493137684185e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159400 Loss: 6.277995998971164e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159450 Loss: 1.7864560504676774e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159500 Loss: 6.6020402300637215e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 159550 Loss: 1.1070546861446928e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159600 Loss: 4.220572373014875e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 159650 Loss: 2.175413101213053e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159700 Loss: 3.038662725884933e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 159750 Loss: 3.966298754676245e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159800 Loss: 1.1366171747795306e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159850 Loss: 2.8094575100112706e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159900 Loss: 1.3841819054505322e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 159950 Loss: 3.928859587176703e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160000 Loss: 1.4117187674855813e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 160050 Loss: 1.779321246431209e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160100 Loss: 1.0358971849200316e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 160150 Loss: 2.191871863033157e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 160200 Loss: 2.605486224638298e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160250 Loss: 3.715475759236142e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160300 Loss: 4.090454694960499e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 160350 Loss: 4.000213084509596e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 160400 Loss: 2.2871912733535282e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 160450 Loss: 2.8285774533287622e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 160500 Loss: 1.1266450201219413e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160550 Loss: 1.8453740267432295e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160600 Loss: 2.175018198613543e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160650 Loss: 5.086310466140276e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 160700 Loss: 1.1912621630472131e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160750 Loss: 1.5538669686065987e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 160800 Loss: 3.491093957563862e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 160850 Loss: 3.853947055176832e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160900 Loss: 1.4937509149604011e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 160950 Loss: 4.560996239888482e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161000 Loss: 3.581580313039012e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161050 Loss: 2.2523403458762914e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161100 Loss: 1.960292865987867e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161150 Loss: 3.7042857002234086e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161200 Loss: 1.930516373249702e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161250 Loss: 5.278499884298071e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 161300 Loss: 3.187056427123025e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161350 Loss: 3.9881189877633005e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161400 Loss: 4.0868224459700286e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161450 Loss: 1.0203664714936167e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161500 Loss: 1.97579684027005e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161550 Loss: 3.3899621485034004e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161600 Loss: 4.463370714802295e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161650 Loss: 1.3014829164603725e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161700 Loss: 5.584654718404636e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161750 Loss: 6.522411240439396e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 161800 Loss: 1.70184994203737e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 161850 Loss: 3.454504394539981e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 161900 Loss: 1.3733553714700975e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 161950 Loss: 6.536943146784324e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 162000 Loss: 4.546324362308951e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 162050 Loss: 2.330103234271519e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 162100 Loss: 1.225583218911197e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 162150 Loss: 2.622430292831268e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 162200 Loss: 2.9851667932234704e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162250 Loss: 2.9252141757751815e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162300 Loss: 3.1013027182780206e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162350 Loss: 7.30703686713241e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 162400 Loss: 0.00010464755905559286 Accuracy: 98.72618865966797\n",
      "Iteration: 162450 Loss: 1.841276025515981e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 162500 Loss: 4.768600774696097e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162550 Loss: 1.601876283530146e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 162600 Loss: 1.2410923773131799e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 162650 Loss: 1.9085293388343416e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162700 Loss: 2.4416578526142985e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162750 Loss: 9.160479748970829e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 162800 Loss: 8.222809810831677e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 162850 Loss: 1.2122372936573811e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162900 Loss: 3.948231096728705e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 162950 Loss: 4.6097240556264296e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163000 Loss: 1.154830079030944e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163050 Loss: 1.1074255326093407e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 163100 Loss: 2.3689613954047672e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163150 Loss: 1.7094569557229988e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163200 Loss: 5.430303644970991e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163250 Loss: 4.147469371673651e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163300 Loss: 3.933892855911836e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 163350 Loss: 1.566576975164935e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163400 Loss: 6.731801022397121e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 163450 Loss: 2.178970134991687e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 163500 Loss: 7.672550418647006e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163550 Loss: 2.9149830879759975e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 163600 Loss: 1.6222382328123786e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 163650 Loss: 3.8929065340198576e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163700 Loss: 2.288507494085934e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163750 Loss: 1.8925502445199527e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163800 Loss: 1.3546545233111829e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 163850 Loss: 5.960463234089275e-09 Accuracy: 98.72618865966797\n",
      "Iteration: 163900 Loss: 4.7638564865337685e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 163950 Loss: 1.799658093659673e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164000 Loss: 3.848910637316294e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 164050 Loss: 1.2468638033169555e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 164100 Loss: 1.2447314475139137e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 164150 Loss: 1.6006337318685837e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164200 Loss: 2.0784773369086906e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 164250 Loss: 2.6484076443011872e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164300 Loss: 3.032017411896959e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164350 Loss: 1.2385604577502818e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 164400 Loss: 2.435303213133011e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164450 Loss: 4.5619584852829576e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 164500 Loss: 3.310900501674041e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164550 Loss: 3.247317727073096e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164600 Loss: 1.2309428711887449e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164650 Loss: 9.083434520107403e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 164700 Loss: 3.485926572466269e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164750 Loss: 2.0691706595243886e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164800 Loss: 1.1611734407779295e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 164850 Loss: 1.114213591790758e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 164900 Loss: 4.845461808145046e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 164950 Loss: 5.233359843259677e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 165000 Loss: 1.414099369867472e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165050 Loss: 3.2274561817757785e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165100 Loss: 4.151628672843799e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165150 Loss: 1.8571046894066967e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 165200 Loss: 2.1672918592230417e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 165250 Loss: 3.5850738640874624e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 165300 Loss: 1.496763161412673e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165350 Loss: 4.7865819396974985e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 165400 Loss: 2.1241050490061752e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165450 Loss: 2.5106355678872205e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165500 Loss: 2.0578398107318208e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 165550 Loss: 9.211306860379409e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 165600 Loss: 8.947352034738287e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165650 Loss: 1.8071110389428213e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 165700 Loss: 6.652231604675762e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 165750 Loss: 2.5635759811848402e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 165800 Loss: 1.3824832421960309e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 165850 Loss: 6.399919129762566e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 165900 Loss: 2.9227790946606547e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 165950 Loss: 1.79036633198848e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 166000 Loss: 7.110697879397776e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 166050 Loss: 1.1050167358916951e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 166100 Loss: 2.391818998148665e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 166150 Loss: 7.506532710976899e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 166200 Loss: 1.1942212040594313e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 166250 Loss: 1.1235963029321283e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 166300 Loss: 7.188377821876202e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 166350 Loss: 1.5909428839222528e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 166400 Loss: 4.844856903218897e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 166450 Loss: 9.598403266863897e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 166500 Loss: 1.6342404705937952e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 166550 Loss: 2.4907607439672574e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 166600 Loss: 1.1098508366558235e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 166650 Loss: 7.355489651672542e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 166700 Loss: 1.9344543034094386e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 166750 Loss: 2.1557474610744976e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 166800 Loss: 1.4270697647589259e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 166850 Loss: 9.360108379041776e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 166900 Loss: 1.6970014257822186e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 166950 Loss: 4.939450082019903e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167000 Loss: 2.3339705876423977e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167050 Loss: 1.435958438378293e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167100 Loss: 1.5466266631847247e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 167150 Loss: 5.384464384405874e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167200 Loss: 1.3560859770223033e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167250 Loss: 3.4529548429418355e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167300 Loss: 1.1330833331157919e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167350 Loss: 4.423667269293219e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 167400 Loss: 4.414774593897164e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 167450 Loss: 8.312053978443146e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 167500 Loss: 5.262643753667362e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 167550 Loss: 4.732524985229247e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 167600 Loss: 4.887368959316518e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 167650 Loss: 3.4187327401014045e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 167700 Loss: 2.6005829568021e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 167750 Loss: 2.595397199911531e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 167800 Loss: 8.5724759628647e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 167850 Loss: 1.579444870003499e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 167900 Loss: 1.3703489457839169e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 167950 Loss: 5.2668274292955175e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 168000 Loss: 9.654373570811003e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 168050 Loss: 9.445889190828893e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 168100 Loss: 9.892038178804796e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 168150 Loss: 4.2932751966873184e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 168200 Loss: 1.1842453204735648e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168250 Loss: 2.500437585695181e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168300 Loss: 1.7952301050172537e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 168350 Loss: 2.3162716388469562e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168400 Loss: 4.887552904619952e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 168450 Loss: 2.0436740669538267e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168500 Loss: 1.8794540665112436e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168550 Loss: 3.918165475624846e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 168600 Loss: 4.0398113924311474e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168650 Loss: 2.5906389055307955e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 168700 Loss: 3.4728491300484166e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168750 Loss: 3.362630422998336e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 168800 Loss: 2.6467780116945505e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168850 Loss: 9.634911111788824e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168900 Loss: 1.7163642041850835e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 168950 Loss: 1.940542642842047e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169000 Loss: 1.935430191224441e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169050 Loss: 6.484926302618987e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 169100 Loss: 3.594392183003947e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169150 Loss: 2.8086142265237868e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169200 Loss: 3.0863859137753025e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169250 Loss: 1.0354198821005411e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169300 Loss: 3.295099304523319e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169350 Loss: 2.682157173694577e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169400 Loss: 1.2494720067479648e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169450 Loss: 2.208729347330518e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169500 Loss: 3.531697075231932e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 169550 Loss: 2.628790753078647e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169600 Loss: 1.4839320101600606e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169650 Loss: 8.662882464705035e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 169700 Loss: 2.841172317857854e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169750 Loss: 1.355180393147748e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169800 Loss: 3.096641739830375e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169850 Loss: 3.4153516025980935e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 169900 Loss: 2.8613265385502018e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 169950 Loss: 2.8911485060234554e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 170000 Loss: 9.110214705287945e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 170050 Loss: 1.4784563973080367e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170100 Loss: 1.0983127140207216e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170150 Loss: 8.564991730963811e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 170200 Loss: 4.9565664085093886e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 170250 Loss: 3.5614282296592137e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 170300 Loss: 4.4080577936256304e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 170350 Loss: 5.68624159313913e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 170400 Loss: 6.9404418354679365e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 170450 Loss: 8.225427450270217e-08 Accuracy: 98.72618865966797\n",
      "Iteration: 170500 Loss: 5.611263986793347e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170550 Loss: 3.683967224787921e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170600 Loss: 1.3452020539261866e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170650 Loss: 2.752276304818224e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 170700 Loss: 1.4799295058764983e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170750 Loss: 1.9231709302403033e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170800 Loss: 2.3026153940008953e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170850 Loss: 3.585410013329238e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 170900 Loss: 2.160975054721348e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 170950 Loss: 1.9653840354294516e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171000 Loss: 7.962231393321417e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 171050 Loss: 3.3981079468503594e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 171100 Loss: 2.7337935534887947e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 171150 Loss: 2.9705064662266523e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171200 Loss: 1.9228453311370686e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 171250 Loss: 1.4244506928662304e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 171300 Loss: 8.936641097534448e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 171350 Loss: 2.5381143132108264e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171400 Loss: 9.719267836771905e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 171450 Loss: 2.299475227118819e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 171500 Loss: 4.754260589834303e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171550 Loss: 3.436464976402931e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 171600 Loss: 3.6812194593949243e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171650 Loss: 1.613435051694978e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171700 Loss: 1.3827333532390185e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171750 Loss: 1.571577013237402e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171800 Loss: 2.6521214749664068e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171850 Loss: 1.2905499715998303e-05 Accuracy: 98.75\n",
      "Iteration: 171900 Loss: 1.3237716302683111e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 171950 Loss: 3.7261703255353495e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 172000 Loss: 3.5890966501028743e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 172050 Loss: 1.5823792637092993e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172100 Loss: 1.3096357179165352e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 172150 Loss: 7.5874290814681444e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 172200 Loss: 2.2713225916959345e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172250 Loss: 4.618510138243437e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172300 Loss: 1.105356295738602e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 172350 Loss: 2.9441383958328515e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172400 Loss: 2.786042205116246e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172450 Loss: 2.7146912543685175e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172500 Loss: 1.673196857154835e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172550 Loss: 2.4092738385661505e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172600 Loss: 3.0382079785340466e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 172650 Loss: 1.1749128134397324e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 172700 Loss: 6.786847734474577e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 172750 Loss: 1.440482992620673e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 172800 Loss: 1.0855334949155804e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 172850 Loss: 6.317956149359816e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 172900 Loss: 2.256522520838189e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 172950 Loss: 2.1038504200987518e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 173000 Loss: 2.1937472411082126e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 173050 Loss: 1.3593570656666998e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 173100 Loss: 4.893845016340492e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 173150 Loss: 2.4466760805808008e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 173200 Loss: 5.540733764064498e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 173250 Loss: 1.6718078768462874e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 173300 Loss: 1.6063586372183636e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 173350 Loss: 4.708516280516051e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 173400 Loss: 1.4773753719055094e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 173450 Loss: 6.389447548826865e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 173500 Loss: 2.9719778467551805e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 173550 Loss: 1.6205483916564845e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 173600 Loss: 1.9430040993029252e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 173650 Loss: 1.3224565009295475e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 173700 Loss: 3.3186513519467553e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 173750 Loss: 1.68911617492995e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 173800 Loss: 8.125341992126778e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 173850 Loss: 8.8935557869263e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 173900 Loss: 2.056991979770828e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 173950 Loss: 4.488956983550452e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 174000 Loss: 5.224072992859874e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 174050 Loss: 4.863008598476881e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 174100 Loss: 1.528518259874545e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174150 Loss: 3.834712970274268e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 174200 Loss: 1.3271081115817651e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174250 Loss: 4.709905624622479e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174300 Loss: 1.426120797987096e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 174350 Loss: 4.169416206423193e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 174400 Loss: 2.0860596123384312e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174450 Loss: 1.668923346187512e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 174500 Loss: 5.246205546427518e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 174550 Loss: 2.858563129848335e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 174600 Loss: 1.4229627595341299e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174650 Loss: 3.71942987840157e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174700 Loss: 2.758384471235331e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 174750 Loss: 9.226042493537534e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 174800 Loss: 3.41853046847973e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174850 Loss: 1.0027833013737109e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 174900 Loss: 9.032312846102286e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 174950 Loss: 1.690940189291723e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175000 Loss: 7.115228800103068e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175050 Loss: 4.4208180042915046e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 175100 Loss: 2.9301905669854023e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175150 Loss: 1.740313382470049e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175200 Loss: 2.7704316380550154e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175250 Loss: 2.5831801394815557e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 175300 Loss: 2.8009935704176314e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175350 Loss: 2.489440157660283e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 175400 Loss: 9.240757208317518e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 175450 Loss: 8.562248694943264e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 175500 Loss: 1.7419239156879485e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 175550 Loss: 9.469981705478858e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 175600 Loss: 2.8933056455571204e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175650 Loss: 1.7746189769241028e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 175700 Loss: 9.193000551022124e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 175750 Loss: 4.169677140453132e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 175800 Loss: 1.203984766107169e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 175850 Loss: 1.2766884083248442e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 175900 Loss: 4.192754568066448e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 175950 Loss: 1.1523101420607418e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 176000 Loss: 5.858995791641064e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 176050 Loss: 3.692690370371565e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 176100 Loss: 8.648129551147576e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 176150 Loss: 4.084183456143364e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 176200 Loss: 3.677376980704139e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 176250 Loss: 6.7789833337883465e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 176300 Loss: 1.1559945050976239e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 176350 Loss: 9.890586625260767e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 176400 Loss: 4.257266027707374e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 176450 Loss: 1.9668639197334414e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 176500 Loss: 5.282159327180125e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 176550 Loss: 1.7520047549623996e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 176600 Loss: 2.498516369087156e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 176650 Loss: 4.464992798602907e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 176700 Loss: 8.494194844388403e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 176750 Loss: 6.352996024361346e-06 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 176800 Loss: 6.5203080339415465e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 176850 Loss: 1.4689422641822603e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 176900 Loss: 2.0597587990778266e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 176950 Loss: 1.473338443247485e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 177000 Loss: 2.8134156309533864e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 177050 Loss: 7.879611985117663e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177100 Loss: 3.227758134016767e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 177150 Loss: 2.3554584913654253e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 177200 Loss: 9.726725693326443e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177250 Loss: 1.7797173086364637e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177300 Loss: 9.280400263378397e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177350 Loss: 5.093221716379048e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 177400 Loss: 2.501704693713691e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 177450 Loss: 4.949178673996357e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 177500 Loss: 3.520979225868359e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 177550 Loss: 7.675534106965642e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177600 Loss: 4.090820311830612e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177650 Loss: 1.9062807041336782e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 177700 Loss: 7.5210805334791075e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177750 Loss: 1.5603714928147383e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 177800 Loss: 9.224473615176976e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 177850 Loss: 4.675931631936692e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 177900 Loss: 1.6378407963202335e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 177950 Loss: 7.220996485557407e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 178000 Loss: 2.6479219741304405e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 178050 Loss: 2.749689701886382e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 178100 Loss: 1.129800239141332e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 178150 Loss: 3.5558175568439765e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 178200 Loss: 4.030512718600221e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 178250 Loss: 2.004228917940054e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 178300 Loss: 1.4009969163453206e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 178350 Loss: 3.851679866784252e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 178400 Loss: 8.727437489142176e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 178450 Loss: 5.387354576669168e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 178500 Loss: 1.3736769687966444e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 178550 Loss: 1.0120841579919215e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 178600 Loss: 5.6104036048054695e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 178650 Loss: 5.4366722906706855e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 178700 Loss: 4.696699033956975e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 178750 Loss: 3.193848897353746e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 178800 Loss: 4.3800578168884385e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 178850 Loss: 1.2352288649708498e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 178900 Loss: 4.121966867387528e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 178950 Loss: 2.4218727048719302e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179000 Loss: 1.5014968994364608e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179050 Loss: 1.8523722246754915e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179100 Loss: 1.589970815984998e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179150 Loss: 6.985063009778969e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 179200 Loss: 1.5510582670685835e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179250 Loss: 8.56918723002309e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 179300 Loss: 3.855993782053702e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 179350 Loss: 4.684500709117856e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 179400 Loss: 1.6549249266972765e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179450 Loss: 2.131151268258691e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179500 Loss: 1.1785670722019859e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179550 Loss: 3.292240944574587e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179600 Loss: 1.5233438716677483e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179650 Loss: 2.3484684788854793e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179700 Loss: 2.7435813535703346e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179750 Loss: 1.3113928616803605e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 179800 Loss: 2.529382072680164e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179850 Loss: 2.3509524908149615e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179900 Loss: 1.7045729691744782e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 179950 Loss: 9.134781976172235e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 180000 Loss: 1.0242672942695208e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 180050 Loss: 2.5291154088336043e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180100 Loss: 1.6140478692250326e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180150 Loss: 2.74597459792858e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 180200 Loss: 2.211251739936415e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180250 Loss: 1.859946860349737e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180300 Loss: 2.008801129704807e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 180350 Loss: 2.0447967472136952e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180400 Loss: 3.8083708204794675e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 180450 Loss: 5.783610504295211e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 180500 Loss: 7.504107998101972e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 180550 Loss: 1.5470805010409094e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 180600 Loss: 9.587594831828028e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 180650 Loss: 2.991523979289923e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 180700 Loss: 1.2817436072509736e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180750 Loss: 4.556540443445556e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180800 Loss: 8.12198777566664e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 180850 Loss: 4.6541234041797e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 180900 Loss: 4.512910891207866e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 180950 Loss: 1.3666902304976247e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 181000 Loss: 3.4554541343823075e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 181050 Loss: 1.7320641063633957e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 181100 Loss: 2.3614884412381798e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181150 Loss: 1.2493081158027053e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181200 Loss: 1.7665520317677874e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 181250 Loss: 8.106002496788278e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 181300 Loss: 1.2967726433998905e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181350 Loss: 5.4959273256827146e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181400 Loss: 1.9640168829937465e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181450 Loss: 3.097612352576107e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181500 Loss: 2.0488732843659818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 181550 Loss: 8.521603740518913e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 181600 Loss: 1.5099290976650082e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 181650 Loss: 2.0538300304906443e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181700 Loss: 1.7127395040006377e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181750 Loss: 5.714451617677696e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 181800 Loss: 2.4970622689579614e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 181850 Loss: 2.7951238735113293e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 181900 Loss: 3.0855102522764355e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 181950 Loss: 4.126924977754243e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182000 Loss: 1.2083429282938596e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182050 Loss: 1.3130932529747952e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182100 Loss: 6.7664946072909515e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 182150 Loss: 2.5778350391192362e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 182200 Loss: 3.5279925214126706e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 182250 Loss: 1.0509132152947132e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 182300 Loss: 3.18817219522316e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182350 Loss: 1.1194273611181416e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182400 Loss: 1.8787563021760434e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 182450 Loss: 8.59818646858912e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 182500 Loss: 3.894107794621959e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182550 Loss: 2.784708522085566e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182600 Loss: 1.6698953913873993e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182650 Loss: 3.1705112633062527e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182700 Loss: 3.577691313694231e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182750 Loss: 1.2644592061406001e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 182800 Loss: 8.010723036022682e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 182850 Loss: 1.3182118891563732e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 182900 Loss: 5.161671765563369e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 182950 Loss: 1.6245188817265444e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183000 Loss: 4.83231087855529e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 183050 Loss: 7.244550943141803e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183100 Loss: 6.2886119849281386e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 183150 Loss: 8.349645213456824e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 183200 Loss: 2.0265493105853238e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 183250 Loss: 3.718241714523174e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183300 Loss: 8.43700036057271e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183350 Loss: 2.043232052528765e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183400 Loss: 8.219592018576805e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 183450 Loss: 5.020333992433734e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183500 Loss: 1.476964825997129e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 183550 Loss: 1.6885533113963902e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183600 Loss: 4.502793672145344e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183650 Loss: 1.9190385501133278e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 183700 Loss: 3.872703928209376e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 183750 Loss: 1.1899357559741475e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183800 Loss: 1.6162573956535198e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183850 Loss: 3.480513987597078e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183900 Loss: 2.3090713511919603e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 183950 Loss: 3.410931458347477e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 184000 Loss: 2.0452067474252544e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 184050 Loss: 7.19776971891406e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 184100 Loss: 4.1141611291095614e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184150 Loss: 2.1381247279350646e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184200 Loss: 1.9121205696137622e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184250 Loss: 1.3351032066566404e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 184300 Loss: 1.9149469153489918e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184350 Loss: 4.6367949835257605e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 184400 Loss: 1.3378814401221462e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184450 Loss: 1.1151992111990694e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184500 Loss: 3.516448487062007e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184550 Loss: 5.02615421282826e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 184600 Loss: 2.4833414499880746e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184650 Loss: 6.896666309330612e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 184700 Loss: 1.927645644173026e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 184750 Loss: 2.2823225663159974e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 184800 Loss: 2.366041917412076e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 184850 Loss: 3.1967917948350077e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 184900 Loss: 2.732013672357425e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 184950 Loss: 1.0758580174297094e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 185000 Loss: 3.71368951164186e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185050 Loss: 1.3419667084235698e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185100 Loss: 3.838483098661527e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185150 Loss: 2.7773161491495557e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185200 Loss: 8.145600077114068e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 185250 Loss: 3.3868898754008114e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185300 Loss: 1.4847423699393403e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185350 Loss: 2.908767419285141e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185400 Loss: 1.0242306416330393e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 185450 Loss: 1.8523358448874205e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 185500 Loss: 7.73030478740111e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 185550 Loss: 2.2869573513162322e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185600 Loss: 1.6900021364563145e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 185650 Loss: 3.852656391245546e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 185700 Loss: 2.3274233171832748e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 185750 Loss: 2.8584858227986842e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 185800 Loss: 7.3106575655401684e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 185850 Loss: 2.1457557863868715e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 185900 Loss: 4.823537165066227e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 185950 Loss: 1.4210646440915298e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186000 Loss: 5.326023165252991e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186050 Loss: 5.062960190116428e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186100 Loss: 6.209771981957601e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 186150 Loss: 1.5491828889935277e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186200 Loss: 1.430142674507806e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 186250 Loss: 5.091819912195206e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 186300 Loss: 1.1872841696458636e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 186350 Loss: 6.666151875833748e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 186400 Loss: 5.9970258007524535e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186450 Loss: 2.704789221752435e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186500 Loss: 1.875176531029865e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186550 Loss: 1.571468783367891e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 186600 Loss: 1.9031758711207658e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 186650 Loss: 5.553517439693678e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 186700 Loss: 6.520668307530286e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 186750 Loss: 2.256573861814104e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 186800 Loss: 7.339845979004167e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 186850 Loss: 2.5970153728849255e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 186900 Loss: 7.282443220901769e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 186950 Loss: 4.718480340670794e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187000 Loss: 1.5484743926208466e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 187050 Loss: 3.277497671660967e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 187100 Loss: 4.12934496125672e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187150 Loss: 1.5262265151250176e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187200 Loss: 2.4901379219954833e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 187250 Loss: 1.3365139238885604e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187300 Loss: 2.022056287387386e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 187350 Loss: 2.269456126668956e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187400 Loss: 7.50059280107962e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 187450 Loss: 1.6462343410239555e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187500 Loss: 1.4293522326624952e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187550 Loss: 1.750094634189736e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 187600 Loss: 1.2968231203558389e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 187650 Loss: 4.078780330019072e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 187700 Loss: 2.3553169739898294e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 187750 Loss: 9.955489076673985e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 187800 Loss: 1.4490765352093149e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 187850 Loss: 2.1754989575129002e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187900 Loss: 1.2245017387613188e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 187950 Loss: 1.627880919841118e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 188000 Loss: 1.2364106623863336e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 188050 Loss: 2.4791406758595258e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 188100 Loss: 1.6139600120368414e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 188150 Loss: 4.7215547965606675e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 188200 Loss: 7.583356818940956e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 188250 Loss: 5.74380692341947e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 188300 Loss: 1.4084358554100618e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 188350 Loss: 8.24182279757224e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 188400 Loss: 1.7960863260668702e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 188450 Loss: 8.187819730665069e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 188500 Loss: 2.7033680453314446e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 188550 Loss: 1.4237193681765348e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 188600 Loss: 1.1233554687350988e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 188650 Loss: 2.9824916055076756e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 188700 Loss: 1.0549536000326043e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 188750 Loss: 1.3982461268824409e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 188800 Loss: 7.256664048327366e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 188850 Loss: 3.298740557511337e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 188900 Loss: 2.467515969328815e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 188950 Loss: 2.770021092146635e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 189000 Loss: 1.4727646885148715e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 189050 Loss: 2.8230815587448888e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 189100 Loss: 2.6721534595708363e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 189150 Loss: 2.347416193515528e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 189200 Loss: 1.1577083569136448e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 189250 Loss: 1.5143938071560115e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 189300 Loss: 1.8697861378313974e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 189350 Loss: 1.5571948097203858e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 189400 Loss: 8.305031769850757e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 189450 Loss: 1.2882743249065243e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 189500 Loss: 1.5627998664058396e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 189550 Loss: 1.0928772098850459e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 189600 Loss: 1.2811604392481968e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 189650 Loss: 2.9033879400230944e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 189700 Loss: 3.6642657505581155e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 189750 Loss: 8.98355483514024e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 189800 Loss: 5.988639259157935e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 189850 Loss: 2.7598440283327363e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 189900 Loss: 8.142404112732038e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 189950 Loss: 9.153000064543448e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 190000 Loss: 7.809785529389046e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 190050 Loss: 1.3855978977517225e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 190100 Loss: 6.203961675055325e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 190150 Loss: 1.0913503501797095e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 190200 Loss: 5.532744398806244e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 190250 Loss: 2.576431870693341e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 190300 Loss: 8.699547834112309e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 190350 Loss: 7.035164344415534e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 190400 Loss: 9.089058949030004e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 190450 Loss: 9.700313057692256e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 190500 Loss: 1.0119334547198378e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 190550 Loss: 7.4472186497587245e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 190600 Loss: 1.4876538443786558e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 190650 Loss: 4.138757867622189e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 190700 Loss: 3.3329561119899154e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 190750 Loss: 1.920272370625753e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 190800 Loss: 1.4662988178315572e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 190850 Loss: 1.6478978068334982e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 190900 Loss: 2.16595458368829e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 190950 Loss: 7.474664016626775e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 191000 Loss: 2.6121435439563356e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191050 Loss: 1.2571248589665629e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191100 Loss: 3.371449565747753e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191150 Loss: 2.8965653200430097e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 191200 Loss: 5.596272330876673e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 191250 Loss: 4.573688784148544e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 191300 Loss: 1.9021912521566264e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 191350 Loss: 6.225795459613437e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 191400 Loss: 1.6877973394002765e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191450 Loss: 1.753612741595134e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191500 Loss: 1.235940453625517e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191550 Loss: 1.8102182366419584e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 191600 Loss: 1.9578086721594445e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191650 Loss: 8.011964382603765e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 191700 Loss: 6.0294642025837675e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 191750 Loss: 8.715213880350348e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 191800 Loss: 6.529770871566143e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 191850 Loss: 4.2230545659549534e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 191900 Loss: 8.736972631595563e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 191950 Loss: 1.0697786819946487e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192000 Loss: 2.2396488930098712e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 192050 Loss: 5.538895038625924e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 192100 Loss: 6.112253686296754e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 192150 Loss: 1.2252559827174991e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 192200 Loss: 7.990528501977678e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 192250 Loss: 8.526676538167521e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 192300 Loss: 2.6788551622303203e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192350 Loss: 2.974556446133647e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192400 Loss: 2.5533516236464493e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192450 Loss: 8.982665349321906e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 192500 Loss: 7.48791717342101e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 192550 Loss: 1.810765388654545e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 192600 Loss: 1.1481281035230495e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192650 Loss: 1.9636398064903915e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192700 Loss: 2.7751199013437144e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 192750 Loss: 3.7773334042867646e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 192800 Loss: 3.2082134566735476e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192850 Loss: 1.0536594345467165e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 192900 Loss: 4.59507828054484e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 192950 Loss: 6.662658961431589e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 193000 Loss: 6.503978511318564e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 193050 Loss: 5.1476613407430705e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 193100 Loss: 4.1259707359131426e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193150 Loss: 1.23599847938749e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193200 Loss: 2.6246780180372298e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193250 Loss: 8.949645234679338e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 193300 Loss: 1.798587618395686e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193350 Loss: 3.623502198024653e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 193400 Loss: 1.5247397641360294e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 193450 Loss: 1.9581524611567147e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193500 Loss: 8.96444635145599e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 193550 Loss: 7.776368875056505e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 193600 Loss: 1.9295655874884687e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193650 Loss: 1.9482526113279164e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193700 Loss: 9.360781405121088e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 193750 Loss: 2.9406028261291794e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193800 Loss: 1.4474845556833316e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193850 Loss: 1.4066214362173923e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 193900 Loss: 1.1909141903743148e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 193950 Loss: 8.143219019984826e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 194000 Loss: 3.311692853458226e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194050 Loss: 1.7117669131039293e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 194100 Loss: 1.3711289284401573e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 194150 Loss: 6.6225998125446495e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 194200 Loss: 7.589100732730003e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 194250 Loss: 6.318315718090162e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194300 Loss: 2.9286527933436446e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194350 Loss: 1.199739745061379e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194400 Loss: 1.9023016648134217e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194450 Loss: 1.1094100955233444e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 194500 Loss: 2.551661600591615e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 194550 Loss: 8.479271855321713e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 194600 Loss: 1.2881663678854238e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194650 Loss: 2.4610828404547647e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194700 Loss: 1.343785334029235e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 194750 Loss: 2.620876330183819e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 194800 Loss: 1.1316580639686435e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194850 Loss: 1.8391028788755648e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 194900 Loss: 4.695065854321001e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 194950 Loss: 5.496280664374353e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 195000 Loss: 1.214704752783291e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195050 Loss: 1.5863439330132678e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195100 Loss: 2.7232945285504684e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195150 Loss: 1.0618307896947954e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195200 Loss: 2.1843428839929402e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195250 Loss: 5.915141628065612e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 195300 Loss: 7.085059678502148e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 195350 Loss: 6.939197646715911e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 195400 Loss: 4.194315624772571e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 195450 Loss: 3.880726217175834e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 195500 Loss: 1.0588342775008641e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195550 Loss: 1.2610254088940565e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195600 Loss: 3.973591446992941e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195650 Loss: 1.1464238923508674e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 195700 Loss: 2.304224381077802e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 195750 Loss: 1.8116577848559245e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195800 Loss: 4.2883315472863615e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195850 Loss: 5.030737884226255e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195900 Loss: 3.389980338397436e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 195950 Loss: 3.456822014413774e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196000 Loss: 1.41320060720318e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196050 Loss: 3.317152732051909e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196100 Loss: 1.4375075807038229e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196150 Loss: 1.2483339560276363e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196200 Loss: 1.2626721400010865e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196250 Loss: 3.012153865711298e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196300 Loss: 3.093076520599425e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196350 Loss: 3.6505360185401514e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196400 Loss: 1.517970667919144e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196450 Loss: 9.53674117454284e-09 Accuracy: 98.73809814453125\n",
      "Iteration: 196500 Loss: 1.6903642972465605e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196550 Loss: 6.723933620378375e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196600 Loss: 1.2132691153965425e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196650 Loss: 4.049052586196922e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196700 Loss: 3.997718158643693e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196750 Loss: 2.7949794457526878e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196800 Loss: 3.908922735718079e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196850 Loss: 2.8509295589174144e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 196900 Loss: 4.3920485040871426e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 196950 Loss: 2.0424915419425815e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197000 Loss: 1.5203447219391819e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197050 Loss: 1.2283204341656528e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 197100 Loss: 6.198780511113e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 197150 Loss: 2.9356237064348534e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197200 Loss: 3.845397714030696e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 197250 Loss: 2.6690755476010963e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 197300 Loss: 3.3264313969993964e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 197350 Loss: 2.31195936066797e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197400 Loss: 2.5627652576076798e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197450 Loss: 1.3092976587358862e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197500 Loss: 1.265328683075495e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197550 Loss: 8.59984902490396e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 197600 Loss: 4.273791637388058e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 197650 Loss: 7.228829872474307e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 197700 Loss: 6.307814601314021e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 197750 Loss: 7.73128431319492e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 197800 Loss: 1.72540640051011e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197850 Loss: 3.184514935128391e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197900 Loss: 1.813304697861895e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 197950 Loss: 1.6595875422353856e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198000 Loss: 1.450295985705452e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 198050 Loss: 9.812079042603727e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 198100 Loss: 1.0624095011735335e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198150 Loss: 1.6324627722497098e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198200 Loss: 1.6194260751944967e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 198250 Loss: 1.3784829206997529e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 198300 Loss: 1.8967099094879813e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 198350 Loss: 2.0359764675959013e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 198400 Loss: 1.2480879831855418e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 198450 Loss: 2.155053152819164e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198500 Loss: 1.9230757970944978e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 198550 Loss: 3.2270811061607674e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198600 Loss: 1.3069575288682245e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 198650 Loss: 1.1080785952799488e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198700 Loss: 1.2748745575663634e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198750 Loss: 1.5589992472087033e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198800 Loss: 9.650160791352391e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 198850 Loss: 1.483939922763966e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 198900 Loss: 3.8160338590387255e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 198950 Loss: 1.0669785297068302e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199000 Loss: 3.874040430673631e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 199050 Loss: 3.6292880395194516e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199100 Loss: 1.5006583453214262e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199150 Loss: 7.371967512881383e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 199200 Loss: 7.018617179710418e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 199250 Loss: 2.2330305000650696e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 199300 Loss: 9.413403859070968e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 199350 Loss: 2.4892326109693386e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199400 Loss: 2.7971591407549568e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199450 Loss: 1.6371086530853063e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199500 Loss: 8.401924787904136e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 199550 Loss: 2.740050149441231e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 199600 Loss: 1.0610346180328634e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199650 Loss: 2.5187971459672553e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 199700 Loss: 4.4777028961107135e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 199750 Loss: 4.253716724633705e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 199800 Loss: 4.564900336845312e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 199850 Loss: 5.45379316463368e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 199900 Loss: 2.095690433634445e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 199950 Loss: 2.4403010684181936e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 200000 Loss: 9.171943929686677e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 200050 Loss: 9.111142389883753e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 200100 Loss: 1.6075417079264298e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 200150 Loss: 2.4728307835175656e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 200200 Loss: 1.0051157914858777e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 200250 Loss: 2.1076259145047516e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 200300 Loss: 2.2573391106561758e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 200350 Loss: 1.2340066859906074e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 200400 Loss: 6.742766345269047e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 200450 Loss: 5.102259365230566e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 200500 Loss: 1.0451931302668527e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 200550 Loss: 1.3554166798712686e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 200600 Loss: 7.271995400515152e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 200650 Loss: 8.58360217534937e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 200700 Loss: 1.4676732462248765e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 200750 Loss: 3.2896161428652704e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 200800 Loss: 1.6679265172570013e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 200850 Loss: 2.347122745050001e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 200900 Loss: 1.2769488421326969e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 200950 Loss: 7.788371476635803e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 201000 Loss: 8.049187272263225e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 201050 Loss: 8.938304745242931e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 201100 Loss: 3.1208481232170016e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 201150 Loss: 2.8111770006944425e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 201200 Loss: 1.3014988326176535e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 201250 Loss: 5.42535281056189e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 201300 Loss: 2.1008025214541703e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 201350 Loss: 2.034038698184304e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 201400 Loss: 6.426442269003019e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 201450 Loss: 2.5141715013887733e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 201500 Loss: 5.129088094690815e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 201550 Loss: 4.510145663516596e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 201600 Loss: 5.382879862736445e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 201650 Loss: 1.3177101209294051e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 201700 Loss: 1.0869814104808029e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 201750 Loss: 2.3751010303385556e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 201800 Loss: 2.9355636797845364e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 201850 Loss: 1.0788012332341168e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 201900 Loss: 1.6283103150271927e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 201950 Loss: 1.4519395108436584e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 202000 Loss: 3.046516030735802e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 202050 Loss: 2.4946157282101922e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202100 Loss: 2.6883035388891585e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202150 Loss: 3.2533298508496955e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202200 Loss: 1.1956164598814212e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 202250 Loss: 1.0214776011707727e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202300 Loss: 5.031410182709806e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 202350 Loss: 2.063440842903219e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202400 Loss: 7.839986210456118e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202450 Loss: 7.561265192634892e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 202500 Loss: 2.0869369109277613e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 202550 Loss: 8.136082760756835e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202600 Loss: 8.016272659006063e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 202650 Loss: 1.0519829629629385e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 202700 Loss: 4.957486453349702e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 202750 Loss: 1.3432861123874318e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 202800 Loss: 2.417461473669391e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 202850 Loss: 1.320102637691889e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 202900 Loss: 1.3232171625077171e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 202950 Loss: 8.536957466276363e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 203000 Loss: 9.35888419917319e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 203050 Loss: 1.7203548850375228e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203100 Loss: 1.3485871022567153e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203150 Loss: 2.83562476397492e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203200 Loss: 2.8470489269238897e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203250 Loss: 3.4697470255196095e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 203300 Loss: 2.9437271223287098e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203350 Loss: 2.3971169866854325e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203400 Loss: 1.4269571693148464e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203450 Loss: 1.427958795829909e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203500 Loss: 2.1425255908980034e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203550 Loss: 1.0065194146591239e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203600 Loss: 1.0543377356952988e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 203650 Loss: 1.8021870346274227e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203700 Loss: 1.0663985449355096e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203750 Loss: 2.065817898255773e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 203800 Loss: 1.521820104244398e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 203850 Loss: 1.5247984265442938e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 203900 Loss: 1.34544998218189e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 203950 Loss: 1.3401319847616833e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204000 Loss: 2.1475907487911172e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204050 Loss: 2.5082377760554664e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204100 Loss: 1.3600799320556689e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204150 Loss: 3.2652707886882126e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204200 Loss: 2.7368312657927163e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 204250 Loss: 2.358399251534138e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 204300 Loss: 7.018160249572247e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 204350 Loss: 1.9403381884330884e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204400 Loss: 1.0170284440391697e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 204450 Loss: 1.1645172889984678e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204500 Loss: 2.6777610401040874e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204550 Loss: 1.563307705509942e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 204600 Loss: 1.3551741176343057e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 204650 Loss: 1.7805376046453603e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 204700 Loss: 1.1502279448905028e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 204750 Loss: 6.777441740268841e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 204800 Loss: 7.697992259636521e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 204850 Loss: 2.211184437328484e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 204900 Loss: 3.0443739888141863e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 204950 Loss: 2.781568036880344e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 205000 Loss: 2.541445383030805e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 205050 Loss: 8.786738362687174e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 205100 Loss: 1.2282665011298377e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 205150 Loss: 2.0145495000178926e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 205200 Loss: 6.425560059142299e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 205250 Loss: 5.102410796098411e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 205300 Loss: 1.7147152902907692e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 205350 Loss: 1.4425618246605154e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 205400 Loss: 1.0199014468526002e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 205450 Loss: 7.421333975798916e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 205500 Loss: 1.266602430405328e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 205550 Loss: 5.2946083997085225e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 205600 Loss: 7.85838346928358e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 205650 Loss: 3.538663077051751e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 205700 Loss: 1.468559162276506e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 205750 Loss: 1.1396651643735822e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 205800 Loss: 1.1017834367521573e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 205850 Loss: 8.590793186158407e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 205900 Loss: 3.607793405535631e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 205950 Loss: 2.7891455829376355e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 206000 Loss: 1.4759022633370478e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 206050 Loss: 1.835079638112802e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 206100 Loss: 2.0241004676790908e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 206150 Loss: 1.557172254251782e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 206200 Loss: 1.101433667827223e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 206250 Loss: 1.5674913811380975e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 206300 Loss: 2.3046382921165787e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 206350 Loss: 3.909239967470057e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 206400 Loss: 2.085899541270919e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 206450 Loss: 2.144577410945203e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 206500 Loss: 1.3844833119946998e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 206550 Loss: 1.1548781003511976e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 206600 Loss: 2.421012368358788e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 206650 Loss: 4.6127147470542695e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 206700 Loss: 8.996425094665028e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 206750 Loss: 2.3156639144872315e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 206800 Loss: 1.900712049973663e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 206850 Loss: 5.9854214669030625e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 206900 Loss: 4.395967607706552e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 206950 Loss: 5.277379386825487e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 207000 Loss: 1.6778674762463197e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 207050 Loss: 5.055109340901254e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 207100 Loss: 7.929445928311907e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 207150 Loss: 4.4158594391774386e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 207200 Loss: 2.846133429557085e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 207250 Loss: 2.178008980990853e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 207300 Loss: 2.3531430997536518e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 207350 Loss: 7.474619906133739e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 207400 Loss: 1.3455330190481618e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 207450 Loss: 1.3005360415263567e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 207500 Loss: 3.160094638587907e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 207550 Loss: 2.545666939113289e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 207600 Loss: 3.4712255001068115e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 207650 Loss: 8.455519491690211e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 207700 Loss: 1.5209240700642113e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 207750 Loss: 1.486395103711402e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 207800 Loss: 8.503681783622596e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 207850 Loss: 8.635311132820789e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 207900 Loss: 5.704562227037968e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 207950 Loss: 4.6788012696197256e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208000 Loss: 9.677983143774327e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 208050 Loss: 1.1202464520465583e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208100 Loss: 5.4498145800607745e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 208150 Loss: 1.3303264495334588e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208200 Loss: 2.6860514481086284e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 208250 Loss: 4.447248556971317e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 208300 Loss: 3.506023494992405e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208350 Loss: 2.9617693144246005e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208400 Loss: 3.0263152439147234e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 208450 Loss: 3.429320031500538e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 208500 Loss: 6.058530925656669e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208550 Loss: 1.8798865539793042e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 208600 Loss: 1.2055836123181507e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208650 Loss: 1.0009902325691655e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 208700 Loss: 5.102097020426299e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 208750 Loss: 1.3145676348358393e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208800 Loss: 3.0200604669516906e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 208850 Loss: 9.938867151504382e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 208900 Loss: 6.627485163335223e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 208950 Loss: 2.089228291879408e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 209000 Loss: 2.601772030175198e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 209050 Loss: 3.2300042221322656e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 209100 Loss: 8.561085451219697e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 209150 Loss: 3.309060775791295e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 209200 Loss: 7.560282028862275e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 209250 Loss: 1.6671672710799612e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 209300 Loss: 4.002592049801024e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 209350 Loss: 7.31654245100799e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 209400 Loss: 3.093325403824565e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 209450 Loss: 6.4090800151461735e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 209500 Loss: 1.5639975572412368e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 209550 Loss: 2.619495899125468e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 209600 Loss: 3.181477222824469e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 209650 Loss: 2.3434869945049286e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 209700 Loss: 1.2662183507927693e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 209750 Loss: 2.388285065535456e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 209800 Loss: 1.913636879180558e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 209850 Loss: 6.085900167818181e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 209900 Loss: 2.180246610805625e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 209950 Loss: 1.612344931345433e-05 Accuracy: 98.73809814453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 210000 Loss: 1.5185242773441132e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 210050 Loss: 1.0156069038202986e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 210100 Loss: 2.1568626834778115e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 210150 Loss: 5.26616713614203e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 210200 Loss: 1.4226189705368597e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 210250 Loss: 1.745495683280751e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 210300 Loss: 1.361546583211748e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 210350 Loss: 1.1187162272108253e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 210400 Loss: 7.239502338052262e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 210450 Loss: 1.490458998887334e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 210500 Loss: 6.875312919873977e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 210550 Loss: 4.3458268919494e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 210600 Loss: 6.78294952649594e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 210650 Loss: 1.8760467355605215e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 210700 Loss: 5.909622814215254e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 210750 Loss: 3.1048679375089705e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 210800 Loss: 3.207223198842257e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 210850 Loss: 5.675249849446118e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 210900 Loss: 1.6978236089926213e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 210950 Loss: 6.9924785748298746e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 211000 Loss: 9.560081707604695e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 211050 Loss: 6.6052084548573475e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211100 Loss: 2.03533963940572e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 211150 Loss: 2.962083044621977e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211200 Loss: 7.382673356914893e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 211250 Loss: 8.800733667158056e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211300 Loss: 2.802356902975589e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 211350 Loss: 1.3378773473959882e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 211400 Loss: 1.4348840522870887e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 211450 Loss: 9.766709808900487e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211500 Loss: 2.5285069568781182e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 211550 Loss: 4.5898348616901785e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 211600 Loss: 3.939055386581458e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 211650 Loss: 7.806754183548037e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211700 Loss: 1.3886987062505796e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211750 Loss: 3.2899133657338098e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211800 Loss: 5.564012553804787e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 211850 Loss: 1.3617619515571278e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 211900 Loss: 1.776183671609033e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 211950 Loss: 1.7366284737363458e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 212000 Loss: 1.1354326488799416e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212050 Loss: 1.7907104847836308e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212100 Loss: 2.935856855401653e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 212150 Loss: 8.148527740559075e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 212200 Loss: 2.238376509922091e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212250 Loss: 1.4961667147872504e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212300 Loss: 7.188392828538781e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 212350 Loss: 3.3912124308699276e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 212400 Loss: 2.2022602934157476e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 212450 Loss: 2.889441020670347e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 212500 Loss: 1.255390452570282e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 212550 Loss: 5.335363675840199e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212600 Loss: 1.4138787264528219e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212650 Loss: 9.284796760766767e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 212700 Loss: 2.921115083154291e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212750 Loss: 2.6769344913191162e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 212800 Loss: 6.0764787122025155e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 212850 Loss: 6.230153303476982e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 212900 Loss: 1.4901094402830495e-07 Accuracy: 98.73809814453125\n",
      "Iteration: 212950 Loss: 2.652803414093796e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213000 Loss: 2.8144657335360534e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213050 Loss: 1.8006890968536027e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213100 Loss: 7.378158443316352e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 213150 Loss: 2.818227767420467e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 213200 Loss: 3.820567508228123e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213250 Loss: 2.6378961592854466e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 213300 Loss: 6.005120212648762e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 213350 Loss: 2.4151090656232554e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 213400 Loss: 1.6363319446099922e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213450 Loss: 4.60932415080606e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 213500 Loss: 1.653217441344168e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213550 Loss: 3.016177834069822e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213600 Loss: 3.810997077380307e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213650 Loss: 3.8268666685326025e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213700 Loss: 2.1127831132616848e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213750 Loss: 1.3622978258354124e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213800 Loss: 3.0134946428006515e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 213850 Loss: 3.641244620666839e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 213900 Loss: 2.7579786546994e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 213950 Loss: 1.6861133190104738e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214000 Loss: 1.8537066353019327e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214050 Loss: 1.826516199798789e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 214100 Loss: 2.3188391423900612e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214150 Loss: 7.1940221459954046e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 214200 Loss: 2.9594148145406507e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214250 Loss: 4.629610702977516e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 214300 Loss: 2.2012956833350472e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 214350 Loss: 2.682524427655153e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 214400 Loss: 1.3735348147747573e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 214450 Loss: 1.2865872122347355e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 214500 Loss: 7.045106031000614e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 214550 Loss: 1.1742463357222732e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214600 Loss: 4.942955001752125e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 214650 Loss: 7.898059266153723e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 214700 Loss: 1.813911512726918e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214750 Loss: 7.090794679243118e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214800 Loss: 7.92275295680156e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 214850 Loss: 1.067731500370428e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214900 Loss: 1.559528573125135e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 214950 Loss: 1.1256518519076053e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215000 Loss: 1.6821421013446525e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215050 Loss: 1.1423507203289773e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215100 Loss: 4.222132702125236e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 215150 Loss: 1.3902916180086322e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215200 Loss: 6.7933224272564985e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 215250 Loss: 1.5028803318273276e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 215300 Loss: 1.0739599929365795e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 215350 Loss: 3.742583794519305e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215400 Loss: 2.891288386308588e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215450 Loss: 1.3471438251144718e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215500 Loss: 4.362203981145285e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 215550 Loss: 5.73372653889237e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 215600 Loss: 1.9054370568483137e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 215650 Loss: 2.359211430302821e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215700 Loss: 1.1433814506744966e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215750 Loss: 1.565063212183304e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215800 Loss: 1.277053524972871e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 215850 Loss: 3.0684077501064166e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 215900 Loss: 4.006260496680625e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 215950 Loss: 3.603457571443869e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216000 Loss: 1.785750464478042e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 216050 Loss: 7.80762275098823e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216100 Loss: 6.686326742055826e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216150 Loss: 2.950157522718655e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216200 Loss: 2.9001839720876887e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216250 Loss: 1.130305008700816e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 216300 Loss: 1.2681065527431201e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 216350 Loss: 1.470565712224925e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 216400 Loss: 1.2232091648911592e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 216450 Loss: 3.41142299475905e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216500 Loss: 4.321061169321183e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216550 Loss: 2.7623013011179864e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 216600 Loss: 1.5071781490405556e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 216650 Loss: 2.5756684408406727e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 216700 Loss: 8.225091733038425e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216750 Loss: 5.599776159215253e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 216800 Loss: 1.1036148862331174e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 216850 Loss: 1.2117347068851814e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 216900 Loss: 2.7242051146458834e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 216950 Loss: 2.7321698325977195e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 217000 Loss: 3.0753617465961725e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217050 Loss: 3.262164682382718e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217100 Loss: 2.3449929358321242e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217150 Loss: 4.7683688109145805e-08 Accuracy: 98.73809814453125\n",
      "Iteration: 217200 Loss: 1.533395698061213e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 217250 Loss: 2.3245601710186747e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 217300 Loss: 2.0214389223838225e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 217350 Loss: 1.796322612790391e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217400 Loss: 5.712306665373035e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217450 Loss: 1.1417095265642274e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217500 Loss: 2.532246071496047e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217550 Loss: 2.6106312361662276e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217600 Loss: 1.09552638605237e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217650 Loss: 2.4877772375475615e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217700 Loss: 1.2056664672854822e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217750 Loss: 1.8269038264406845e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 217800 Loss: 6.108437810325995e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217850 Loss: 2.0998870240873657e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 217900 Loss: 6.750530246790731e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 217950 Loss: 7.412930426653475e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218000 Loss: 2.754755587375257e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 218050 Loss: 2.6072515538544394e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218100 Loss: 2.616928941279184e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218150 Loss: 7.706090400461107e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 218200 Loss: 9.045106708072126e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 218250 Loss: 1.6920997950364836e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218300 Loss: 2.719086296565365e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218350 Loss: 2.9490753149730153e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218400 Loss: 3.830934929283103e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 218450 Loss: 4.140890723647317e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 218500 Loss: 1.0499621566850692e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218550 Loss: 1.7064186977222562e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 218600 Loss: 3.882434612023644e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 218650 Loss: 1.4215920600690879e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218700 Loss: 9.257850251742639e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 218750 Loss: 1.7719652532832697e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218800 Loss: 2.1551302779698744e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218850 Loss: 1.9060796603298513e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 218900 Loss: 1.7166184989036992e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 218950 Loss: 1.0453103641339112e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219000 Loss: 1.7367859754813253e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 219050 Loss: 1.746240013744682e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219100 Loss: 7.610826287418604e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 219150 Loss: 1.1019649718946312e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 219200 Loss: 2.795693217194639e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219250 Loss: 2.7883597795153037e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219300 Loss: 1.7735312212607823e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219350 Loss: 2.7657810278469697e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219400 Loss: 1.5664203601772897e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219450 Loss: 9.56016833697504e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 219500 Loss: 2.1847441530553624e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 219550 Loss: 1.0256148925691377e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 219600 Loss: 1.9345645341672935e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 219650 Loss: 2.5584529794286937e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 219700 Loss: 1.8910814105765894e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 219750 Loss: 1.99429541680729e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 219800 Loss: 2.000667154788971e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219850 Loss: 2.5187951905536465e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219900 Loss: 3.1926210795063525e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 219950 Loss: 8.387615707761142e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 220000 Loss: 1.3369508451432921e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 220050 Loss: 2.409575790807139e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 220100 Loss: 3.917138383258134e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 220150 Loss: 1.8844028090825304e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 220200 Loss: 2.9251094019855373e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 220250 Loss: 3.338848728162702e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 220300 Loss: 1.5534344129264355e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 220350 Loss: 1.2761685866280459e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 220400 Loss: 1.4972274584579282e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 220450 Loss: 1.0898012078541797e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 220500 Loss: 2.7630292152025504e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 220550 Loss: 4.303626701585017e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 220600 Loss: 9.195658094540704e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 220650 Loss: 6.090606802899856e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 220700 Loss: 6.27220724709332e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 220750 Loss: 1.3589024092652835e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 220800 Loss: 7.3459923441987485e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 220850 Loss: 4.5691849663853645e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 220900 Loss: 1.3276073332235683e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 220950 Loss: 2.1548717995756306e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221000 Loss: 2.5114402887993492e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 221050 Loss: 5.6995930208358914e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221100 Loss: 3.5152886539435713e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 221150 Loss: 1.6179732483578846e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221200 Loss: 4.0327213355340064e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221250 Loss: 1.1576167707971763e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221300 Loss: 1.9965527826570906e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 221350 Loss: 2.7671017960528843e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221400 Loss: 6.252716957533266e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 221450 Loss: 2.0765643057529815e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 221500 Loss: 1.4183463463268708e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 221550 Loss: 8.670367606100626e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 221600 Loss: 2.2204354536370374e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 221650 Loss: 5.131063153385185e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221700 Loss: 2.2480488041765057e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221750 Loss: 2.047065572696738e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221800 Loss: 3.7976826661179075e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 221850 Loss: 2.0084771676920354e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 221900 Loss: 1.6133453755173832e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 221950 Loss: 8.053663805185352e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222000 Loss: 2.212304934801068e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 222050 Loss: 6.021957233315334e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222100 Loss: 3.3673404686851427e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222150 Loss: 1.673606675467454e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 222200 Loss: 3.5415539514360717e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222250 Loss: 2.1529547666432336e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 222300 Loss: 5.043219516664976e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222350 Loss: 5.6203822168754414e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 222400 Loss: 1.8250995708513074e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 222450 Loss: 1.908951162477024e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 222500 Loss: 2.610079536680132e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 222550 Loss: 1.0081142136186827e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 222600 Loss: 6.485541234724224e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222650 Loss: 8.220808922487777e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222700 Loss: 5.665368007612415e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 222750 Loss: 3.462007225607522e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 222800 Loss: 2.6877505661104806e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 222850 Loss: 5.437607251224108e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 222900 Loss: 1.4184493011271115e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 222950 Loss: 1.3993687389302067e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223000 Loss: 7.727398042334244e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 223050 Loss: 1.961012094398029e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223100 Loss: 1.8759670638246462e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223150 Loss: 1.0956560799968429e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223200 Loss: 2.0291656255722046e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 223250 Loss: 6.701741767756175e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 223300 Loss: 1.3204433344071731e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 223350 Loss: 1.0871254744415637e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 223400 Loss: 1.4571343854186125e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223450 Loss: 9.43391387409065e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 223500 Loss: 3.214810203644447e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223550 Loss: 1.3331969057617243e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223600 Loss: 1.0272467079630587e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223650 Loss: 3.2203159207710996e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223700 Loss: 6.835724889242556e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 223750 Loss: 2.7129342925036326e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223800 Loss: 1.6305615645251237e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223850 Loss: 3.268564796599094e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 223900 Loss: 1.4964165529818274e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 223950 Loss: 1.7042295439750887e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 224000 Loss: 1.0635074431775138e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 224050 Loss: 1.2163788596808445e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 224100 Loss: 1.2155161130067427e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 224150 Loss: 1.1014428309863433e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 224200 Loss: 2.5159066353808157e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 224250 Loss: 1.0728566621764912e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 224300 Loss: 1.4924324887033436e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 224350 Loss: 5.864846571057569e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 224400 Loss: 2.2641846953774802e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 224450 Loss: 1.1133668067486724e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 224500 Loss: 2.3773100110702217e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 224550 Loss: 2.0451376258279197e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 224600 Loss: 2.013364792219363e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 224650 Loss: 7.4338809099572245e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 224700 Loss: 2.982557452924084e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 224750 Loss: 1.731078555167187e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 224800 Loss: 9.246286936104298e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 224850 Loss: 2.6065945348818786e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 224900 Loss: 3.579507756512612e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 224950 Loss: 3.38082572852727e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225000 Loss: 2.4591105102445e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225050 Loss: 1.1905632163689006e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225100 Loss: 1.5175127373368014e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225150 Loss: 5.179510480957106e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 225200 Loss: 1.695072205620818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225250 Loss: 2.1170568288653158e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 225300 Loss: 5.54309963263222e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 225350 Loss: 2.411396781099029e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 225400 Loss: 1.3574836884799879e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225450 Loss: 2.817816312017385e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225500 Loss: 6.435197428800166e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225550 Loss: 1.8141179680242203e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225600 Loss: 1.340532980975695e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225650 Loss: 1.7186725017381832e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225700 Loss: 1.0896241292357445e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 225750 Loss: 9.57114389166236e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 225800 Loss: 8.107346729957499e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 225850 Loss: 9.126970326178707e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 225900 Loss: 1.615576729818713e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 225950 Loss: 6.82781683281064e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 226000 Loss: 1.661622809479013e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226050 Loss: 1.5532907127635553e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226100 Loss: 4.375983917270787e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226150 Loss: 5.556004452955676e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 226200 Loss: 1.717403210932389e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 226250 Loss: 6.046343969501322e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 226300 Loss: 2.8633456167881377e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226350 Loss: 1.617132693354506e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226400 Loss: 2.3684191546635702e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226450 Loss: 1.2640268323593773e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226500 Loss: 1.3389892046689056e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 226550 Loss: 3.85108887712704e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 226600 Loss: 3.1916068110149354e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226650 Loss: 7.75128592067631e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 226700 Loss: 1.6879625036381185e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 226750 Loss: 1.7837173800216988e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226800 Loss: 2.06512395379832e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226850 Loss: 1.0599973393254913e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 226900 Loss: 6.982938884902978e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 226950 Loss: 1.1475352948764339e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227000 Loss: 1.6329833670170046e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227050 Loss: 2.618519101815764e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227100 Loss: 2.503103496565018e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227150 Loss: 2.17233373405179e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 227200 Loss: 5.977604359941324e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 227250 Loss: 5.6039425544440746e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 227300 Loss: 5.63670473638922e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 227350 Loss: 1.310406150878407e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 227400 Loss: 3.8044545362936333e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 227450 Loss: 1.8691513332669274e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 227500 Loss: 1.1106106285296846e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227550 Loss: 2.1637435565935448e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 227600 Loss: 2.390815097896848e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 227650 Loss: 1.7815740648075007e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 227700 Loss: 1.8244149032398127e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 227750 Loss: 1.603014607098885e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227800 Loss: 1.3076810319034848e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 227850 Loss: 1.035157129081199e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227900 Loss: 1.3399978342931718e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 227950 Loss: 2.750460225797724e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228000 Loss: 7.663812539249193e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 228050 Loss: 1.1820953659480438e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228100 Loss: 1.2204873200971633e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228150 Loss: 1.4192716662364546e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228200 Loss: 3.472359458100982e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 228250 Loss: 8.43460475152824e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 228300 Loss: 3.177942062393413e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 228350 Loss: 7.966763405420352e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 228400 Loss: 7.148872100515291e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 228450 Loss: 1.6245840015471913e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228500 Loss: 1.994641024793964e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 228550 Loss: 7.6020965025236364e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 228600 Loss: 4.929885562887648e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 228650 Loss: 2.1503208699868992e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228700 Loss: 1.7550481061334722e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228750 Loss: 1.787238579709083e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228800 Loss: 5.428544682217762e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 228850 Loss: 4.9543799832463264e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 228900 Loss: 8.231989340856671e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 228950 Loss: 1.6899221009225585e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229000 Loss: 1.6405623682658188e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229050 Loss: 6.596890216314932e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 229100 Loss: 6.01809233558015e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 229150 Loss: 4.792158847521932e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 229200 Loss: 1.2790908840543125e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 229250 Loss: 4.199468548904406e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 229300 Loss: 4.445885861059651e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229350 Loss: 1.0050973287434317e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229400 Loss: 1.8447863112669438e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229450 Loss: 1.3149780897947494e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229500 Loss: 2.9843691663700156e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229550 Loss: 9.954923370969482e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 229600 Loss: 2.192191095673479e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229650 Loss: 5.8370683291286696e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 229700 Loss: 2.180371666327119e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 229750 Loss: 1.4908469893271104e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229800 Loss: 1.1422308489272837e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 229850 Loss: 9.092155778489541e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 229900 Loss: 7.41967915018904e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 229950 Loss: 1.7046543234755518e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 230000 Loss: 4.796556368091842e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 230050 Loss: 6.430997927964199e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 230100 Loss: 4.714756869361736e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230150 Loss: 1.8645438103703782e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230200 Loss: 1.4318532521429006e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 230250 Loss: 1.582012919243425e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 230300 Loss: 1.8690752767724916e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230350 Loss: 1.9385694031370804e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230400 Loss: 1.4554986591974739e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 230450 Loss: 2.7535194021766074e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230500 Loss: 1.5005896784714423e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230550 Loss: 5.3869112889515236e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230600 Loss: 2.63894435192924e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230650 Loss: 1.563502701174002e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230700 Loss: 1.1996324246865697e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230750 Loss: 4.0531130451881836e-08 Accuracy: 98.71428680419922\n",
      "Iteration: 230800 Loss: 1.6257126844720915e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 230850 Loss: 2.7403757485444658e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 230900 Loss: 1.2556574802147225e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 230950 Loss: 3.7380352296167985e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231000 Loss: 1.3315508113009855e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231050 Loss: 2.0913574189762585e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231100 Loss: 3.567161547834985e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231150 Loss: 2.7571659302338958e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 231200 Loss: 1.813581729948055e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 231250 Loss: 3.695483385968146e-08 Accuracy: 98.71428680419922\n",
      "Iteration: 231300 Loss: 1.5423989680130035e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231350 Loss: 1.1544709195732139e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 231400 Loss: 2.277683961438015e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231450 Loss: 3.23787062370684e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 231500 Loss: 4.3374816414143424e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 231550 Loss: 5.18801280122716e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 231600 Loss: 6.069218670745613e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 231650 Loss: 1.5062130842125043e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 231700 Loss: 2.9023680326645263e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231750 Loss: 1.0705285603762604e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231800 Loss: 1.2946659808221739e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 231850 Loss: 1.503189878349076e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 231900 Loss: 8.088512913673185e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 231950 Loss: 1.9625917047960684e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232000 Loss: 1.6703414075891487e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232050 Loss: 2.5616061520850053e-06 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 232100 Loss: 2.948607834696304e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232150 Loss: 2.4180844775401056e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232200 Loss: 2.2987604097579606e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 232250 Loss: 8.317491847265046e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 232300 Loss: 2.878621671698056e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232350 Loss: 1.5514619008172303e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232400 Loss: 1.164607692771824e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232450 Loss: 1.251453795703128e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232500 Loss: 6.201375526870834e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 232550 Loss: 8.966850145952776e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 232600 Loss: 2.414514347037766e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232650 Loss: 5.417073680291651e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 232700 Loss: 7.246630957524758e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 232750 Loss: 6.646530528087169e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232800 Loss: 3.3306794648524374e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232850 Loss: 2.1030044081271626e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 232900 Loss: 1.4010938684805296e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 232950 Loss: 2.3210652216221206e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 233000 Loss: 1.6064617739175446e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 233050 Loss: 8.575006177125033e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 233100 Loss: 3.0308110581245273e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233150 Loss: 9.527767360850703e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 233200 Loss: 7.285862920980435e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 233250 Loss: 1.4375159480550792e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233300 Loss: 2.0781366401934065e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233350 Loss: 8.395813893002924e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 233400 Loss: 1.8980423192260787e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233450 Loss: 1.584629535500426e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233500 Loss: 1.596916263224557e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233550 Loss: 1.9668053937493823e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233600 Loss: 6.400529400707455e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 233650 Loss: 2.6577894459478557e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233700 Loss: 6.3318893808173016e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 233750 Loss: 4.991685273125768e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 233800 Loss: 1.1588633242354263e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233850 Loss: 3.313717752462253e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 233900 Loss: 1.9050574337597936e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 233950 Loss: 3.339571412652731e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234000 Loss: 1.0083420420414768e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234050 Loss: 2.678784221643582e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234100 Loss: 2.4020702767302282e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234150 Loss: 1.8738863900580327e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 234200 Loss: 1.48240815178724e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 234250 Loss: 1.8205812011728995e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 234300 Loss: 9.886960469884798e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 234350 Loss: 1.1217634892091155e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234400 Loss: 1.3750134712608997e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234450 Loss: 1.3210577890276909e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234500 Loss: 1.3991361811349634e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 234550 Loss: 2.1423105863505043e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234600 Loss: 1.7234238839591853e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234650 Loss: 1.4948049056329182e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 234700 Loss: 1.9697412426467054e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 234750 Loss: 2.0972733182134107e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 234800 Loss: 1.6239335309364833e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 234850 Loss: 7.642572199983988e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 234900 Loss: 1.4942947018425912e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 234950 Loss: 3.621225005190354e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 235000 Loss: 2.4457360268570483e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235050 Loss: 1.1337088835716713e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 235100 Loss: 1.575682472321205e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 235150 Loss: 5.708765093004331e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 235200 Loss: 3.576015296857804e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235250 Loss: 9.988093552237842e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 235300 Loss: 2.569023308751639e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235350 Loss: 1.594413333805278e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235400 Loss: 9.266267625207547e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 235450 Loss: 2.235454485344235e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235500 Loss: 7.995621672307607e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 235550 Loss: 2.733960900513921e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235600 Loss: 1.4754507901670877e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235650 Loss: 5.381514347391203e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235700 Loss: 9.179903827316593e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 235750 Loss: 1.1493719284771942e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235800 Loss: 4.421863559400663e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235850 Loss: 1.514930136181647e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 235900 Loss: 2.0746416339534335e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 235950 Loss: 3.0194582905096468e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 236000 Loss: 2.9100861866027117e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 236050 Loss: 9.343722922494635e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 236100 Loss: 2.978920292662224e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 236150 Loss: 9.14331030799076e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 236200 Loss: 9.718320143292658e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 236250 Loss: 3.779850430873921e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 236300 Loss: 9.276389391743578e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 236350 Loss: 1.2418800906743854e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 236400 Loss: 2.7183534257346764e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 236450 Loss: 1.9839741071336903e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 236500 Loss: 5.074774526292458e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 236550 Loss: 1.1772676771215629e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 236600 Loss: 2.0235067495377734e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 236650 Loss: 1.1110003470093943e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 236700 Loss: 1.6007628801162355e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 236750 Loss: 2.3925727873574942e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 236800 Loss: 2.5475497750448994e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 236850 Loss: 5.058761871623574e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 236900 Loss: 9.059555850399192e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 236950 Loss: 4.720471224572975e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 237000 Loss: 1.5458925190614536e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 237050 Loss: 2.0423518435563892e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 237100 Loss: 8.751034329179674e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 237150 Loss: 3.6654300856753252e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 237200 Loss: 1.541761594126001e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 237250 Loss: 2.9266560886753723e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 237300 Loss: 5.430425062513677e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 237350 Loss: 1.6862748452695087e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 237400 Loss: 9.204837624565698e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 237450 Loss: 1.9257789972471073e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 237500 Loss: 2.823895783876651e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 237550 Loss: 1.3476570529746823e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 237600 Loss: 1.7469803424319252e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 237650 Loss: 6.51624714009813e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 237700 Loss: 3.299810487078503e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 237750 Loss: 4.3718981032725424e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 237800 Loss: 2.6142708520637825e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 237850 Loss: 1.5461619113921188e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 237900 Loss: 5.6964781833812594e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 237950 Loss: 1.5276398698915727e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 238000 Loss: 2.4859205950633623e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 238050 Loss: 4.300602540752152e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238100 Loss: 6.811862022004789e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238150 Loss: 1.3121657502779271e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 238200 Loss: 2.2538697521667928e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 238250 Loss: 5.380688889999874e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238300 Loss: 1.1415465451136697e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 238350 Loss: 5.22554319104529e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 238400 Loss: 8.511819032719359e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238450 Loss: 1.504877582192421e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 238500 Loss: 2.1527490389416926e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 238550 Loss: 4.321081632951973e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238600 Loss: 5.0296792323933914e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 238650 Loss: 5.5939935919013806e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238700 Loss: 1.779479134711437e-05 Accuracy: 98.73809814453125\n",
      "Iteration: 238750 Loss: 8.563250048609916e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238800 Loss: 7.467522664228454e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 238850 Loss: 1.2096222235413734e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 238900 Loss: 1.7472739273216575e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 238950 Loss: 1.3361896890273783e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239000 Loss: 1.1143231859023217e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239050 Loss: 5.466583388624713e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 239100 Loss: 1.3659582691616379e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239150 Loss: 3.177790131303482e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 239200 Loss: 5.2250920816732105e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 239250 Loss: 1.155404061137233e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239300 Loss: 3.0994178246146475e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 239350 Loss: 1.7329841284663416e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 239400 Loss: 1.6305564713547938e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 239450 Loss: 6.959257461858215e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 239500 Loss: 1.8612619896885008e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239550 Loss: 7.1481417762697674e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 239600 Loss: 1.789497400750406e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 239650 Loss: 2.1468956674652873e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 239700 Loss: 4.945354157825932e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 239750 Loss: 2.8088479666621424e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239800 Loss: 6.784424840589054e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 239850 Loss: 2.572049379523378e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239900 Loss: 1.4215845112630632e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 239950 Loss: 1.1868686669913586e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 240000 Loss: 1.2735463315038942e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 240050 Loss: 5.254069037619047e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 240100 Loss: 1.1109246770502068e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 240150 Loss: 1.0022358765127137e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 240200 Loss: 1.980243541765958e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 240250 Loss: 1.198460904561216e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 240300 Loss: 2.2825735868536867e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 240350 Loss: 5.452266123029403e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 240400 Loss: 4.072976662428118e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 240450 Loss: 5.754043741035275e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 240500 Loss: 1.999386768147815e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 240550 Loss: 3.1392002711072564e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 240600 Loss: 2.3604514353792183e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 240650 Loss: 8.037100997171365e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 240700 Loss: 6.190510248416103e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 240750 Loss: 9.427980330656283e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 240800 Loss: 5.35241838406364e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 240850 Loss: 9.420943570148665e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 240900 Loss: 4.5866845539421774e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 240950 Loss: 2.4544309781049378e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 241000 Loss: 9.345327271148562e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 241050 Loss: 2.6741705369204283e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 241100 Loss: 1.3258220860734582e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 241150 Loss: 4.487226397031918e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 241200 Loss: 1.1547884241736028e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 241250 Loss: 3.790813991599862e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 241300 Loss: 1.078822378985933e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 241350 Loss: 2.1758878574473783e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 241400 Loss: 5.937371952313697e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 241450 Loss: 1.8330836610402912e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 241500 Loss: 2.6975989385391586e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 241550 Loss: 3.2186275689127797e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 241600 Loss: 1.0764422313513933e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 241650 Loss: 9.685369150247425e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 241700 Loss: 1.9332497686264105e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 241750 Loss: 1.6358510038116947e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 241800 Loss: 5.150494871486444e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 241850 Loss: 9.4502020147047e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 241900 Loss: 8.624378097010776e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 241950 Loss: 1.6831879747769563e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 242000 Loss: 1.20608601719141e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242050 Loss: 3.4292891086806776e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 242100 Loss: 1.592016633367166e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242150 Loss: 5.85477164349868e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 242200 Loss: 1.443427026970312e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242250 Loss: 2.4877208488760516e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 242300 Loss: 4.796186203748221e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 242350 Loss: 2.144221252819989e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 242400 Loss: 5.038637937104795e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 242450 Loss: 1.020991476252675e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242500 Loss: 2.0074015992577188e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242550 Loss: 1.1442166396591347e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242600 Loss: 1.699190579529386e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 242650 Loss: 1.856699054769706e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 242700 Loss: 3.8827281969133765e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242750 Loss: 9.983356903831009e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 242800 Loss: 1.3981851225253195e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242850 Loss: 3.499315425870009e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242900 Loss: 1.4471892427536659e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 242950 Loss: 1.7942427803063765e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 243000 Loss: 1.2619841982086655e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 243050 Loss: 4.77163939649472e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 243100 Loss: 5.605636488326127e-06 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 243150 Loss: 7.004745839367388e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 243200 Loss: 7.877598363847937e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 243250 Loss: 5.999523637001403e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 243300 Loss: 1.4848482351226266e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 243350 Loss: 2.177789838242461e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 243400 Loss: 1.3253471479401924e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 243450 Loss: 1.5768515368108638e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 243500 Loss: 7.492849817936076e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 243550 Loss: 1.7590453353477642e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 243600 Loss: 8.191931556211784e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 243650 Loss: 1.670333404035773e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 243700 Loss: 9.412165127287153e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 243750 Loss: 1.2133989002904855e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 243800 Loss: 1.7719083189149387e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 243850 Loss: 1.0954722711176146e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 243900 Loss: 1.5375382645288482e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 243950 Loss: 6.479310832219198e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244000 Loss: 4.169259227637667e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 244050 Loss: 1.4534027286572382e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 244100 Loss: 4.188602360954974e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 244150 Loss: 8.936862286645919e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 244200 Loss: 3.9625017961952835e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 244250 Loss: 2.450506144668907e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244300 Loss: 2.117088342856732e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 244350 Loss: 2.1960842786938883e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244400 Loss: 1.1722826457116753e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244450 Loss: 7.443774848070461e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 244500 Loss: 2.5664082841103664e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 244550 Loss: 1.8473150703357533e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 244600 Loss: 2.5331024517072365e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244650 Loss: 5.316672400113021e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 244700 Loss: 1.3548949937103316e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244750 Loss: 2.797629167616833e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244800 Loss: 4.939415885019116e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244850 Loss: 1.3993076208862476e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 244900 Loss: 8.423614417552017e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 244950 Loss: 5.16371255798731e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 245000 Loss: 3.3733515465428354e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 245050 Loss: 5.076517481938936e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 245100 Loss: 1.2601793059729971e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 245150 Loss: 1.1327748325129505e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 245200 Loss: 2.0864410544163547e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 245250 Loss: 1.0470230336068198e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 245300 Loss: 2.4147237127181143e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 245350 Loss: 9.458278327656444e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 245400 Loss: 2.5524663215037435e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 245450 Loss: 1.5293562682927586e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 245500 Loss: 4.156392151344335e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 245550 Loss: 1.2489512300817296e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 245600 Loss: 5.49038995814044e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 245650 Loss: 4.5073255023453385e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 245700 Loss: 1.3470608450916188e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 245750 Loss: 4.094488303962862e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 245800 Loss: 6.72844544169493e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 245850 Loss: 4.359067588666221e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 245900 Loss: 4.066944256919669e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 245950 Loss: 1.2993971722607967e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246000 Loss: 4.769005954585737e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 246050 Loss: 1.8497257769922726e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246100 Loss: 1.8070515579893254e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246150 Loss: 3.327713784528896e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 246200 Loss: 8.275796972156968e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 246250 Loss: 1.776161411726207e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 246300 Loss: 1.0368759831180796e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 246350 Loss: 3.2642314181430265e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246400 Loss: 1.6271796994260512e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246450 Loss: 8.943753528001253e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 246500 Loss: 4.344601256889291e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246550 Loss: 2.9192964575486258e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246600 Loss: 3.846491381409578e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 246650 Loss: 1.3950450011179782e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 246700 Loss: 2.4219583792728372e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246750 Loss: 5.116865395393688e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 246800 Loss: 2.2385716874850914e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246850 Loss: 1.047796195052797e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 246900 Loss: 8.331885510415304e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 246950 Loss: 1.049329057423165e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 247000 Loss: 8.269781574199442e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 247050 Loss: 6.311237029876793e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 247100 Loss: 7.013635695329867e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 247150 Loss: 5.388794761529425e-06 Accuracy: 98.73809814453125\n",
      "Iteration: 247200 Loss: 1.5099778465810232e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 247250 Loss: 1.0352867320762016e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 247300 Loss: 2.9536337024183013e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 247350 Loss: 2.4198229766625445e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 247400 Loss: 5.59082707241032e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 247450 Loss: 2.8156400730949827e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 247500 Loss: 1.5063672435644548e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 247550 Loss: 1.0425597793073393e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 247600 Loss: 1.2112427612009924e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 247650 Loss: 3.3173419069498777e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 247700 Loss: 1.130978034780128e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 247750 Loss: 1.2131495168432593e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 247800 Loss: 7.66877201385796e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 247850 Loss: 1.7174634194816463e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 247900 Loss: 9.850885362538975e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 247950 Loss: 9.249393770005554e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 248000 Loss: 6.376183137035696e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 248050 Loss: 1.003956094791647e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 248100 Loss: 2.5720570192788728e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248150 Loss: 1.7459133232478052e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 248200 Loss: 2.2829894078313373e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248250 Loss: 1.1386415280867368e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248300 Loss: 7.140484967749217e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 248350 Loss: 5.353197411750443e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248400 Loss: 8.473834895994514e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 248450 Loss: 1.2006480574200395e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248500 Loss: 1.902929216157645e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 248550 Loss: 7.797827493050136e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 248600 Loss: 5.504305590875447e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 248650 Loss: 1.6212436548812548e-07 Accuracy: 98.71428680419922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 248700 Loss: 1.540706944069825e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248750 Loss: 2.294157638971228e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248800 Loss: 2.289874100824818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 248850 Loss: 1.1586652135520126e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 248900 Loss: 1.1166807780682575e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 248950 Loss: 7.617159099027049e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 249000 Loss: 2.915281402238179e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249050 Loss: 8.047928531595971e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 249100 Loss: 1.165146841231035e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249150 Loss: 2.66688693955075e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249200 Loss: 7.904129233793356e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 249250 Loss: 5.4533891670871526e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249300 Loss: 1.1447111319284886e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249350 Loss: 1.0858077075681649e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 249400 Loss: 1.947886084963102e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 249450 Loss: 6.697502158203861e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 249500 Loss: 5.1785074901999906e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249550 Loss: 2.0768402464454994e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 249600 Loss: 4.369854195829248e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 249650 Loss: 2.4521168597857468e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249700 Loss: 1.0735417163232341e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249750 Loss: 1.0072401892102789e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249800 Loss: 3.561778839866747e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 249850 Loss: 1.0790616215672344e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 249900 Loss: 1.1209016520297155e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 249950 Loss: 1.0080627362185623e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 250000 Loss: 7.033444035187131e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 250050 Loss: 1.2100716048735194e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 250100 Loss: 4.822614300792338e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 250150 Loss: 9.923105608322658e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 250200 Loss: 6.091439104238816e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 250250 Loss: 2.198096581196296e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 250300 Loss: 3.1564270557282725e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 250350 Loss: 6.771381777070928e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 250400 Loss: 3.072394247283228e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 250450 Loss: 1.2731350579997525e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 250500 Loss: 1.206910383189097e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 250550 Loss: 1.0906022907875013e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 250600 Loss: 5.733689704356948e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 250650 Loss: 2.2086429453338496e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 250700 Loss: 6.160904376883991e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 250750 Loss: 3.347626989125274e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 250800 Loss: 3.528498564264737e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 250850 Loss: 1.0846998520719353e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 250900 Loss: 2.8552909498102963e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 250950 Loss: 3.530786443661782e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 251000 Loss: 6.28600682830438e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251050 Loss: 1.4094412108534016e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 251100 Loss: 3.793491123360582e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251150 Loss: 1.3333267816051375e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251200 Loss: 1.8861197531805374e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251250 Loss: 2.435368878650479e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251300 Loss: 9.407130164618138e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 251350 Loss: 1.7905742424773052e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 251400 Loss: 2.095636546073365e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 251450 Loss: 7.708043995080516e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 251500 Loss: 4.953937150276033e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 251550 Loss: 1.075541240425082e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 251600 Loss: 1.2561195035232231e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251650 Loss: 1.3353562280826736e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251700 Loss: 1.2743556908390019e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251750 Loss: 6.5943222580244765e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 251800 Loss: 6.888109510327922e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 251850 Loss: 2.814248000504449e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 251900 Loss: 7.871998604969122e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 251950 Loss: 4.180174528300995e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 252000 Loss: 2.258937274746131e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252050 Loss: 3.7833578971913084e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 252100 Loss: 9.794015568331815e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 252150 Loss: 1.3210358702053782e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 252200 Loss: 2.4745149858063087e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 252250 Loss: 1.1119397640868556e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252300 Loss: 1.3118074093654286e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252350 Loss: 4.340093710197834e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 252400 Loss: 1.1627175808825996e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252450 Loss: 1.4504655155178625e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252500 Loss: 7.784175295455498e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 252550 Loss: 1.0573473900876706e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 252600 Loss: 1.6034851796575822e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252650 Loss: 2.0435381884453818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 252700 Loss: 1.2434895324986428e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 252750 Loss: 8.261980838142335e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 252800 Loss: 3.136465238640085e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 252850 Loss: 1.4703659871884156e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252900 Loss: 1.1709152204275597e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 252950 Loss: 2.1325038687791675e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 253000 Loss: 3.7976718886056915e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 253050 Loss: 1.919763963087462e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 253100 Loss: 2.643061634444166e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253150 Loss: 5.897002210986102e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 253200 Loss: 1.618710120965261e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 253250 Loss: 4.203910066280514e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 253300 Loss: 9.691410696177627e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 253350 Loss: 1.3819042578688823e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253400 Loss: 2.1157004084670916e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253450 Loss: 6.197960829013027e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 253500 Loss: 2.786303957691416e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253550 Loss: 2.9549116788984975e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 253600 Loss: 1.8888811609940603e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 253650 Loss: 6.000549092277652e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 253700 Loss: 1.0164174454985186e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253750 Loss: 3.329010360175744e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253800 Loss: 3.595064072214882e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 253850 Loss: 2.2887274099048227e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253900 Loss: 1.5116543181648012e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 253950 Loss: 3.1366984330816194e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 254000 Loss: 1.0666905836842488e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 254050 Loss: 5.159884040040197e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 254100 Loss: 1.342288487649057e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 254150 Loss: 2.9346332667046227e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 254200 Loss: 1.8547258150647394e-05 Accuracy: 98.72618865966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 254250 Loss: 1.3472778846335132e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 254300 Loss: 9.617325304134283e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 254350 Loss: 5.458411123981932e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 254400 Loss: 1.6528723790543154e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 254450 Loss: 1.1032761904061772e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 254500 Loss: 1.2949020856467541e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 254550 Loss: 7.903472578618675e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 254600 Loss: 6.836124612163985e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 254650 Loss: 8.675110620970372e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 254700 Loss: 2.2667943994747475e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 254750 Loss: 9.083351528715866e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 254800 Loss: 3.15384795612772e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 254850 Loss: 2.237195622001309e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 254900 Loss: 7.762427230773028e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 254950 Loss: 8.485680154990405e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 255000 Loss: 4.8405013330921065e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 255050 Loss: 5.7404081417189445e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 255100 Loss: 1.592679473105818e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255150 Loss: 1.6088695701910183e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255200 Loss: 1.531816815258935e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255250 Loss: 3.397441332708695e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 255300 Loss: 4.9501806643093005e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 255350 Loss: 1.54622921400005e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 255400 Loss: 1.7920652680913918e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255450 Loss: 1.8856859242077917e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255500 Loss: 5.366493041947251e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 255550 Loss: 1.5762087059556507e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 255600 Loss: 1.2990774848731235e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255650 Loss: 2.8520125852082856e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 255700 Loss: 8.286866432172246e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 255750 Loss: 1.4982237189542502e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 255800 Loss: 1.0694605407479685e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255850 Loss: 3.823052247753367e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 255900 Loss: 3.103886911048903e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 255950 Loss: 5.707252967113163e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 256000 Loss: 3.0727085686521605e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 256050 Loss: 1.7242053218069486e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 256100 Loss: 3.7440270261868136e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 256150 Loss: 6.295855200733058e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 256200 Loss: 4.7201861889334396e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 256250 Loss: 6.746492999809561e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 256300 Loss: 1.6199763194890693e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 256350 Loss: 3.4649885947146686e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 256400 Loss: 1.7456964997109026e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 256450 Loss: 1.8875471141654998e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 256500 Loss: 1.6648948076181114e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 256550 Loss: 1.054412223311374e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 256600 Loss: 9.44118994539167e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 256650 Loss: 7.346550773945637e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 256700 Loss: 9.829370355873834e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 256750 Loss: 1.1092864042439032e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 256800 Loss: 6.702371592837153e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 256850 Loss: 6.8167055360390805e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 256900 Loss: 1.39889843921992e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 256950 Loss: 2.795762338791974e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257000 Loss: 8.53570963954553e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 257050 Loss: 1.1364974852767773e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257100 Loss: 1.3144670447218232e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257150 Loss: 3.728635419975035e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 257200 Loss: 3.115020808763802e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257250 Loss: 1.341649294772651e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 257300 Loss: 2.2109006749815308e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 257350 Loss: 2.3397476979880594e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 257400 Loss: 9.617714567866642e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 257450 Loss: 1.0900123015744612e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257500 Loss: 1.717833583825268e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257550 Loss: 2.898365528380964e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 257600 Loss: 1.4972274584579282e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 257650 Loss: 4.159569652983919e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257700 Loss: 4.1364197386428714e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 257750 Loss: 1.5321624232456088e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 257800 Loss: 2.6561845515971072e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 257850 Loss: 4.914541477774037e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 257900 Loss: 1.2697893907898106e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 257950 Loss: 2.7869186851603445e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 258000 Loss: 7.0840405896888115e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 258050 Loss: 1.5077629541337956e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 258100 Loss: 4.7460398491239175e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 258150 Loss: 1.0691373972804286e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 258200 Loss: 2.9036807973170653e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 258250 Loss: 2.5500507035758346e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 258300 Loss: 1.820456236600876e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 258350 Loss: 1.0490081876923796e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 258400 Loss: 9.470979421166703e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 258450 Loss: 3.7523932405747473e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 258500 Loss: 1.616888039279729e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 258550 Loss: 2.0567886167555116e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 258600 Loss: 6.648585440416355e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 258650 Loss: 1.6033175143093104e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 258700 Loss: 2.2140042347018607e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 258750 Loss: 4.962927778251469e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 258800 Loss: 3.157608716719551e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 258850 Loss: 1.1070848813687917e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 258900 Loss: 6.475702321040444e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 258950 Loss: 2.4988614313770086e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 259000 Loss: 2.426779428787995e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 259050 Loss: 7.71572013036348e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 259100 Loss: 2.2647547666565515e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 259150 Loss: 1.0624057722452562e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 259200 Loss: 1.6934114682953805e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 259250 Loss: 2.439715899527073e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 259300 Loss: 2.1111156911501894e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 259350 Loss: 2.3228689315146767e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 259400 Loss: 7.24780147720594e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 259450 Loss: 3.0755552415939746e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 259500 Loss: 2.7055826649302617e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 259550 Loss: 1.6036479792091995e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 259600 Loss: 4.58514159618062e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 259650 Loss: 2.3668066205573268e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 259700 Loss: 3.8187408790690824e-06 Accuracy: 98.71428680419922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 259750 Loss: 8.690705726621673e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 259800 Loss: 2.196895138695254e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 259850 Loss: 1.2082285138603766e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 259900 Loss: 7.020058092166437e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 259950 Loss: 4.756996531796176e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 260000 Loss: 7.774394362058956e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 260050 Loss: 2.7925365429837257e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 260100 Loss: 1.6327143384842202e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 260150 Loss: 9.934105946740601e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 260200 Loss: 4.868159521720372e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 260250 Loss: 1.9357876226422377e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 260300 Loss: 9.3696626208839e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 260350 Loss: 2.409279295534361e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 260400 Loss: 5.20969206263544e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 260450 Loss: 2.1367804947658442e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 260500 Loss: 8.06200387160061e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 260550 Loss: 5.745778707932914e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 260600 Loss: 2.6023089958471246e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 260650 Loss: 7.709532837907318e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 260700 Loss: 1.7363585357088596e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 260750 Loss: 1.2853236512455624e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 260800 Loss: 7.419542725983774e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 260850 Loss: 6.074314114812296e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 260900 Loss: 1.472109852329595e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 260950 Loss: 1.2778875316143967e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 261000 Loss: 1.8905745946540264e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261050 Loss: 5.456452072394313e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261100 Loss: 5.868195330549497e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261150 Loss: 5.847660759172868e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261200 Loss: 2.8327149266260676e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 261250 Loss: 1.2335226529103238e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 261300 Loss: 2.2021402401151136e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 261350 Loss: 2.219580528617371e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 261400 Loss: 9.536581160318747e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 261450 Loss: 8.396137673116755e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261500 Loss: 9.72725729297963e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 261550 Loss: 6.347115231619682e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261600 Loss: 1.0300700523657724e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 261650 Loss: 4.038443876197562e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 261700 Loss: 4.355099372332916e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 261750 Loss: 5.787674581370084e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261800 Loss: 5.195224730414338e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261850 Loss: 4.807232016901253e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 261900 Loss: 1.5807187082828023e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 261950 Loss: 8.88383510755375e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262000 Loss: 7.407867997244466e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262050 Loss: 1.1689397069858387e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 262100 Loss: 6.813816526118899e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262150 Loss: 2.7582914299273398e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262200 Loss: 1.431956570741022e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 262250 Loss: 2.1652584109688178e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 262300 Loss: 3.3020623391166737e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 262350 Loss: 8.976098797575105e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 262400 Loss: 2.0230834707035683e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 262450 Loss: 3.2709110655559925e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262500 Loss: 1.0739582648966461e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 262550 Loss: 7.472495326510398e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262600 Loss: 7.561827260360587e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 262650 Loss: 7.212029231595807e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 262700 Loss: 9.931914064509328e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 262750 Loss: 6.419644705601968e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262800 Loss: 2.8943361030542292e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 262850 Loss: 1.4733955140400212e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262900 Loss: 7.712709702900611e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 262950 Loss: 1.0592844773782417e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263000 Loss: 1.6378862710553221e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 263050 Loss: 2.529728226363659e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263100 Loss: 1.675693602010142e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263150 Loss: 7.570933121314738e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 263200 Loss: 1.2585066542669665e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263250 Loss: 8.142119440890383e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 263300 Loss: 4.5855708776798565e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 263350 Loss: 6.242184099392034e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 263400 Loss: 3.154819933115505e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263450 Loss: 1.6630256141070276e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263500 Loss: 3.4732036056084326e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 263550 Loss: 3.784551836361061e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 263600 Loss: 1.282889297726797e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263650 Loss: 1.3084978490951471e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 263700 Loss: 6.908516297698952e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 263750 Loss: 5.4593409004155546e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 263800 Loss: 2.028836570389103e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263850 Loss: 1.302904183830833e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 263900 Loss: 1.8098980945069343e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 263950 Loss: 1.524753952253377e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 264000 Loss: 2.485208460711874e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264050 Loss: 5.805044565931894e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 264100 Loss: 1.9624272681539878e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264150 Loss: 6.222042884473922e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 264200 Loss: 2.239867353637237e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264250 Loss: 6.11725863564061e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 264300 Loss: 2.3488697479479015e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264350 Loss: 1.8595724213810172e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 264400 Loss: 3.0473807782982476e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264450 Loss: 3.1387506169267e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 264500 Loss: 1.4080249457038008e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 264550 Loss: 1.0889493751164991e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 264600 Loss: 1.3757271517533809e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264650 Loss: 7.206776444945717e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 264700 Loss: 1.600728319317568e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264750 Loss: 2.0076431610505097e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264800 Loss: 6.866341664135689e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 264850 Loss: 5.0385365284455474e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 264900 Loss: 1.912900188472122e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 264950 Loss: 2.520914131309837e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 265000 Loss: 1.4999967788753565e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 265050 Loss: 9.9257949841558e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 265100 Loss: 2.3478582079405896e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 265150 Loss: 1.3076746654405724e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 265200 Loss: 4.91501987198717e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 265250 Loss: 6.401531663868809e-06 Accuracy: 98.71428680419922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 265300 Loss: 8.706031621841248e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 265350 Loss: 1.1508614079502877e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 265400 Loss: 3.2731979899836006e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 265450 Loss: 7.758876563457306e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 265500 Loss: 1.0133919204236008e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 265550 Loss: 1.334701300947927e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 265600 Loss: 8.88072747784463e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 265650 Loss: 2.9095355785102583e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 265700 Loss: 1.904276359709911e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 265750 Loss: 2.139735261152964e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 265800 Loss: 2.007335024245549e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 265850 Loss: 1.4263790944823995e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 265900 Loss: 2.5365368401253363e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 265950 Loss: 9.30454370973166e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 266000 Loss: 6.4827540882106405e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 266050 Loss: 1.8056050976156257e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266100 Loss: 1.2433027222868986e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266150 Loss: 2.3473245164495893e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266200 Loss: 2.6347341190557927e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 266250 Loss: 1.40812717290828e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 266300 Loss: 8.711419468454551e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 266350 Loss: 9.102385774895083e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 266400 Loss: 9.828254405874759e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 266450 Loss: 1.3123936696501914e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266500 Loss: 5.6620334362378344e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266550 Loss: 8.15770272311056e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 266600 Loss: 9.236744517693296e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 266650 Loss: 1.1149906640639529e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266700 Loss: 5.4247047955868766e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 266750 Loss: 4.099230864085257e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 266800 Loss: 1.5438041373272426e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266850 Loss: 1.6091367797343992e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 266900 Loss: 6.6594848249224015e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 266950 Loss: 6.7582277551991865e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267000 Loss: 9.388665603182744e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267050 Loss: 9.009220775624271e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267100 Loss: 7.414647029690968e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 267150 Loss: 3.900181582139339e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267200 Loss: 4.107639597350499e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267250 Loss: 6.021934495947789e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267300 Loss: 7.216198810056085e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267350 Loss: 9.46936233958695e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267400 Loss: 4.927382633468369e-06 Accuracy: 98.72618865966797\n",
      "Iteration: 267450 Loss: 6.993116130615817e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267500 Loss: 6.667418801953318e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 267550 Loss: 1.428463110642042e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 267600 Loss: 4.53487373306416e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 267650 Loss: 2.270588265673723e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 267700 Loss: 2.762247640930582e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 267750 Loss: 1.0243074939353392e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 267800 Loss: 1.675499152042903e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 267850 Loss: 1.8749042283161543e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 267900 Loss: 1.7288874005316757e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 267950 Loss: 6.96162885560625e-07 Accuracy: 98.71428680419922\n",
      "Iteration: 268000 Loss: 2.695483635761775e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 268050 Loss: 1.0796854439831804e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 268100 Loss: 3.7154607070988277e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 268150 Loss: 1.300852727581514e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 268200 Loss: 3.230541949506005e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 268250 Loss: 8.809354881122999e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 268300 Loss: 5.268994982543518e-07 Accuracy: 98.72618865966797\n",
      "Iteration: 268350 Loss: 1.278710533370031e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 268400 Loss: 3.164626468787901e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 268450 Loss: 1.2143956155341584e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 268500 Loss: 8.344649415903405e-09 Accuracy: 98.71428680419922\n",
      "Iteration: 268550 Loss: 3.2791404009913094e-06 Accuracy: 98.71428680419922\n",
      "Iteration: 268600 Loss: 1.3643584679812193e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 268650 Loss: 2.17104097828269e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 268700 Loss: 1.0111339179275092e-05 Accuracy: 98.71428680419922\n",
      "Iteration: 268750 Loss: 1.9812938262475654e-05 Accuracy: 98.72618865966797\n",
      "Iteration: 268800 Loss: 5.12775204697391e-06 Accuracy: 98.71428680419922\n"
     ]
    }
   ],
   "source": [
    "accuracy_list =[]\n",
    "iteration_list = []\n",
    "loss_list = []\n",
    "count = 0\n",
    "# Training the model\n",
    "for e in range(epochs):\n",
    "    for i, (train_images, train_vals) in enumerate(train_loader):\n",
    "        # Variables\n",
    "        train = Variable(train_images.view(100,1,28,28)) ## Why these numbers in particular?\n",
    "        labels = Variable(train_vals)\n",
    "        \n",
    "        # Clear gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        train_out = model.forward(train)\n",
    "        \n",
    "        # loss\n",
    "        train_loss = error(train_out, train_vals)\n",
    "        \n",
    "        # gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count +=1\n",
    "        # Predict test set\n",
    "        if count%50 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for test_images, test_vals in test_loader:\n",
    "                test = Variable(test_images.view(100,1,28,28))\n",
    "                test_out = model.forward(test)\n",
    "                predicted = torch.max(test_out.data, 1)[1]\n",
    "                total += len(labels)\n",
    "                correct += (predicted==test_vals).sum()\n",
    "                \n",
    "            accuracy = correct / float(total) * 100\n",
    "            accuracy_list.append(accuracy)\n",
    "            loss_list.append(train_loss.data)\n",
    "            iteration_list.append(count)\n",
    "            print('Iteration: {} Loss: {} Accuracy: {}'.format(count, train_loss.data, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration_list, loss_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration_list, accuracy_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ann. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ghost/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type cnn_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Saving all the pytorch models\n",
    "# Saving ann model and the state dictionary\n",
    "torch.save(ann_model, '/home/ghost/Desktop/digit-recognizer/ann_model.pth') # <== ann_model saved\n",
    "torch.save(ann_model.state_dict(), '/home/ghost/Desktop/digit-recognizer/ann_state.pth') # <== state dict\n",
    "\n",
    "# Saving the cnn model and state dict\n",
    "torch.save(model, '/home/ghost/Desktop/digit-recognizer/cnn_model.pth') # <== cnn model saved\n",
    "torch.save(model.state_dict(), '/home/ghost/Desktop/digit-recognizer/cnn_state.pth')\n",
    "\n",
    "# Saving the model to be used by someone else "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Running the cnn model on the submission dataset\n",
    "# Observation: In test_loader, in the test.view / train.view function, the element in the first\n",
    "# index of view is the batch size\n",
    "submission_temp = test_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_np = submission_temp.values\n",
    "sub_torch = torch.from_numpy(submission_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_torch_re = sub_torch.reshape(-1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Variable(sub_torch.view(28000, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000, 1, 28, 28])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.max(o.data,1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 9,  ..., 3, 9, 2])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_predicted = p.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_predicted = sample_sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_predicted['Label'] = np_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_predicted.to_csv('/home/ghost/Desktop/digit-recognizer/my_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      9\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for i in test:\n",
    "    print(i.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 28, 28]' is invalid for input of size 78400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-89296004bd86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 28, 28]' is invalid for input of size 78400"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.size())\n",
    "    print(images.view(100,1,28,28).size())\n",
    "    print(images.view(-1,28,28).size())\n",
    "    print(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "temp_loader = torch.utils.data.DataLoader(train, batch_size = 10, shuffle=False)\n",
    "for images in temp_loader:\n",
    "    print(images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "for i in test:\n",
    "    print(i.size())\n",
    "    print(i[0].size())\n",
    "    print(i.view(-1, 28*28).size())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a NN model with 4 hidden layers\n",
    "class My_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 Hidden Layer NN\n",
    "        self.l1 = nn.Linear(28*28,512)\n",
    "        self.l2 = nn.Linear(512, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        self.log_softmax = functional.log_softmax\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(functional.relu(self.l1(x)))\n",
    "        x = self.dropout(functional.relu(self.l2(x)))\n",
    "        x = self.dropout(functional.relu(self.l3(x)))\n",
    "        x = self.log_softmax(self.l4(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = My_Classifier() # create our model\n",
    "criterion = nn.NLLoss() # Define loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015) # Adam optimizer, with learning rate of .0015\n",
    "\n",
    "# Setting the initial hyperparameters\n",
    "epochs = 25\n",
    "steps = 0\n",
    "print_every=50\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0 #? What is running loss?\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "model = Classifier()\n",
    "# Define our loss function\n",
    "criterion = nn.NLLLoss()\n",
    "# Define the optimier\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
    "\n",
    "epochs = 25\n",
    "steps = 0\n",
    "print_every = 50\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        steps += 1\n",
    "        # Prevent accumulation of gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        #backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            # Turn off gradients for validation\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for images, labels in test_loader:\n",
    "                    log_ps = model(images)\n",
    "                    test_loss += criterion(log_ps, labels)\n",
    "\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    # Get our top predictions\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction on the test set "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
